{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe3fb7b7",
   "metadata": {},
   "source": [
    "### Get Models and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83bc4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!wget https://raw.githubusercontent.com/YuvalRozner/WeatherNet/main/Backend/Model_Pytorch/utils/models_for_inference.zip\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c029780b",
   "metadata": {},
   "source": [
    "### Unzip Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bdfc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import zipfile\n",
    "zip_file_path = \"/content/models_for_inference.zip\"\n",
    "\n",
    "# Extract the zip file to a temporary directory\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"/content/models_for_inference\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0041713",
   "metadata": {},
   "source": [
    "### `data.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c08f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\"\"\"\n",
    "this file let you load the data of the stations from the pkl files \n",
    "and the coordinates of the stations from the json file\n",
    "\"\"\"\n",
    "\n",
    "def normalize_coordinates(x_coords, y_coords):\n",
    "    \"\"\"\n",
    "    Normalize the X and Y coordinates to the range [0, 1].\n",
    "\n",
    "    Args:\n",
    "        x_coords (numpy.ndarray): Array of X coordinates in meters.\n",
    "        y_coords (numpy.ndarray): Array of Y coordinates in meters.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Normalized X and Y coordinates as torch tensors.\n",
    "    \"\"\"\n",
    "    x_min, x_max = x_coords.min(), x_coords.max()\n",
    "    y_min, y_max = y_coords.min(), y_coords.max()\n",
    "\n",
    "    x_normalized = (x_coords - x_min) / (x_max - x_min)\n",
    "    y_normalized = (y_coords - y_min) / (y_max - y_min)\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    x_normalized = torch.tensor(x_normalized, dtype=torch.float32).unsqueeze(1)  # [num_stations, 1]\n",
    "    y_normalized = torch.tensor(y_normalized, dtype=torch.float32).unsqueeze(1)  # [num_stations, 1]\n",
    "\n",
    "    return x_normalized, y_normalized \n",
    "def drop_nan_rows_multiple_custom(df_list,custom_na =['-']):\n",
    "    \"\"\"\n",
    "    Removes rows from all DataFrames in the list where any DataFrame has NaN or custom NaN representations in any column.\n",
    "\n",
    "    Parameters:\n",
    "    df_list (List[pd.DataFrame]): List of DataFrames to process.\n",
    "    reset_indices (bool): Whether to reset the index after dropping rows. Defaults to True.\n",
    "    custom_na (List[str]): List of custom strings to be treated as NaN. Defaults to ['-'].\n",
    "\n",
    "    Returns:\n",
    "    List[pd.DataFrame]: List of cleaned DataFrames.\n",
    "    \"\"\"\n",
    "    if not df_list:\n",
    "        raise ValueError(\"The list of DataFrames is empty.\")\n",
    "\n",
    "    # Ensure all DataFrames have the same number of rows\n",
    "    num_rows = df_list[0].shape[0]\n",
    "    for df in df_list:\n",
    "        if df.shape[0] != num_rows:\n",
    "            raise ValueError(\"All DataFrames must have the same number of rows.\")\n",
    "\n",
    "    # Step 0: Replace custom NaN representations with np.nan\n",
    "    cleaned_df_list_initial = []\n",
    "    for df in df_list:\n",
    "        df_cleaned = df.replace(custom_na, np.nan)\n",
    "        cleaned_df_list_initial.append(df_cleaned)\n",
    "\n",
    "    # Step 1: Identify rows with any NaN in each DataFrame\n",
    "    nan_indices_list = [df.isnull().any(axis=1) for df in cleaned_df_list_initial]\n",
    "\n",
    "    # Step 2: Combine the indices where NaNs are present in any DataFrame\n",
    "    combined_nan = pd.Series([False] * num_rows, index=df_list[0].index)\n",
    "    for nan_mask in nan_indices_list:\n",
    "        combined_nan = combined_nan | nan_mask\n",
    "\n",
    "    # Get the indices to drop\n",
    "    indices_to_drop = combined_nan[combined_nan].index\n",
    "\n",
    "    # Step 3: Drop the identified indices from all DataFrames\n",
    "    cleaned_df_list = []\n",
    "    for df in tqdm(cleaned_df_list_initial, desc=\"Dropping NaN rows\"):\n",
    "        cleaned_df = df.drop(indices_to_drop)\n",
    "        if True:\n",
    "            cleaned_df = cleaned_df.reset_index(drop=True)\n",
    "        cleaned_df_list.append(cleaned_df)\n",
    "\n",
    "    return cleaned_df_list\n",
    "def drop_nan_rows_multiple(df_list, reset_indices=True):\n",
    "    \"\"\"\n",
    "    Removes rows from all DataFrames in the list where any DataFrame has NaN in any column.\n",
    "    \n",
    "    Parameters:\n",
    "    df_list (List[pd.DataFrame]): List of DataFrames to process.\n",
    "    reset_indices (bool): Whether to reset the index after dropping rows. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "    List[pd.DataFrame]: List of cleaned DataFrames.\n",
    "    \"\"\"\n",
    "    if not df_list:\n",
    "        raise ValueError(\"The list of DataFrames is empty.\")\n",
    "    #for df in df_list:\n",
    "    #    df.reset_index(drop=True, inplace=True)\n",
    "    # Ensure all DataFrames have the same number of rows\n",
    "    num_rows = df_list[0].shape[0]\n",
    "    for df in df_list:\n",
    "        if df.shape[0] != num_rows:\n",
    "            raise ValueError(\"All DataFrames must have the same number of rows.\")\n",
    "    \n",
    "    # Step 1: Identify rows with any NaN in each DataFrame\n",
    "    nan_indices_list = [df.isnull().any(axis=1) for df in df_list]\n",
    "    \n",
    "    # Step 2: Combine the indices where NaNs are present in any DataFrame\n",
    "    combined_nan = pd.Series([False] * num_rows, index=df_list[0].index)\n",
    "    for nan_mask in nan_indices_list:\n",
    "        combined_nan = combined_nan | nan_mask\n",
    "    \n",
    "    # Get the indices to drop\n",
    "    indices_to_drop = combined_nan[combined_nan].index\n",
    "    \n",
    "    # Step 3: Drop the identified indices from all DataFrames\n",
    "    cleaned_df_list = []\n",
    "    for df in tqdm(df_list, desc=\"Dropping NaN rows\"):\n",
    "        cleaned_df = df.drop(indices_to_drop)\n",
    "        if reset_indices:\n",
    "            cleaned_df = cleaned_df.reset_index(drop=True)\n",
    "        cleaned_df_list.append(cleaned_df)\n",
    "    \n",
    "    return cleaned_df_list\n",
    "\n",
    "# Define the normalization function\n",
    "def normalize_coordinates(x_coords, y_coords):\n",
    "    \"\"\"\n",
    "    Normalize the X and Y coordinates to the range [0, 1].\n",
    "    \"\"\"\n",
    "    x_min, x_max = x_coords.min(), x_coords.max()\n",
    "    y_min, y_max = y_coords.min(), y_coords.max()\n",
    "\n",
    "    x_normalized = (x_coords - x_min) / (x_max - x_min)\n",
    "    y_normalized = (y_coords - y_min) / (y_max - y_min)\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    x_normalized = torch.tensor(x_normalized, dtype=torch.float32).unsqueeze(1)  # [num_stations, 1]\n",
    "    y_normalized = torch.tensor(y_normalized, dtype=torch.float32).unsqueeze(1)  # [num_stations, 1]\n",
    "\n",
    "    return x_normalized, y_normalized\n",
    "\n",
    "def timeEncode(dataframes):\n",
    "    day = 24*60*60\n",
    "    year = (365.2425)*day\n",
    "\n",
    "    for df in dataframes:\n",
    "        if 'Date Time' in df.columns:\n",
    "            timestamp_s = df['Date Time'].map(pd.Timestamp.timestamp)\n",
    "            df['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    "            df['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "            df['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    "            df['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))\n",
    "            df.drop(columns=['Date Time'], inplace=True)\n",
    "\n",
    "\n",
    "def preprocessing_tensor_df(df):\n",
    "    \"\"\"\n",
    "    Apply the same preprocessing steps as during training.\n",
    "    \"\"\"\n",
    "    print(\"preproccessing data...\")\n",
    "    # Slice the DataFrame and create a copy to avoid SettingWithCopyWarning\n",
    "    df = df[5::6].copy()\n",
    "    date_time = pd.to_datetime(df.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\n",
    "\n",
    "    # Handle 'wv (m/s)'\n",
    "    wv = df['wv (m/s)']\n",
    "    bad_wv = wv == -9999.0\n",
    "    df.loc[bad_wv, 'wv (m/s)'] = 0.0  # Use .loc to modify the original DataFrame\n",
    "    wv = df.pop('wv (m/s)')\n",
    "\n",
    "    # Handle 'max. wv (m/s)'\n",
    "    max_wv = df['max. wv (m/s)']\n",
    "    bad_max_wv = max_wv == -9999.0\n",
    "    df.loc[bad_max_wv, 'max. wv (m/s)'] = 0.0  # Use .loc to modify the original DataFrame\n",
    "    max_wv = df.pop('max. wv (m/s)')\n",
    "\n",
    "    # Convert to radians.\n",
    "    wd_rad = df.pop('wd (deg)') * np.pi / 180\n",
    "\n",
    "    # Calculate wind x and y components using .loc\n",
    "    df.loc[:, 'Wx'] = wv * np.cos(wd_rad)\n",
    "    df.loc[:, 'Wy'] = wv * np.sin(wd_rad)\n",
    "    df.loc[:, 'max Wx'] = max_wv * np.cos(wd_rad)\n",
    "    df.loc[:, 'max Wy'] = max_wv * np.sin(wd_rad)\n",
    "\n",
    "    # Time-based features\n",
    "    timestamp_s = date_time.map(pd.Timestamp.timestamp)\n",
    "    day = 24 * 60 * 60\n",
    "    year = 365.2425 * day\n",
    "\n",
    "    df.loc[:, 'Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    "    df.loc[:, 'Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "    df.loc[:, 'Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    "    df.loc[:, 'Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))\n",
    "\n",
    "    return df\n",
    "\n",
    "def normalize_data(train_data, val_data, scaler_path='./scaler.pkl'):\n",
    "    \"\"\"\n",
    "    Fit a StandardScaler on the training data and transform both train and val data.\n",
    "    Save the scaler to disk for future use.\n",
    "\n",
    "    Args:\n",
    "        train_data (np.ndarray): Training data.\n",
    "        val_data (np.ndarray): Validation data.\n",
    "        scaler_path (str): Path to save the scaler.\n",
    "\n",
    "    Returns:\n",
    "        train_data_scaled (np.ndarray): Scaled training data.\n",
    "        val_data_scaled (np.ndarray): Scaled validation data.\n",
    "        scaler (StandardScaler): Fitted scaler object.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_data)\n",
    "\n",
    "    train_data_scaled = scaler.transform(train_data)\n",
    "    val_data_scaled = scaler.transform(val_data)\n",
    "\n",
    "    # Save the scaler\n",
    "    with open(scaler_path, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    print(f\"Scaler saved to {scaler_path}\")\n",
    "\n",
    "    return train_data_scaled, val_data_scaled, scaler\n",
    "\n",
    "def preprocessing_our_df(df):\n",
    "    \"\"\"\n",
    "    Apply the same preprocessing steps as during training.\n",
    "    \"\"\"\n",
    "    print(\"preproccessing data...\")\n",
    "    df = df[5::6].copy()\n",
    "    # drop nan\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def return_and_save_scaler_normalize_data(train_data, val_data, scaler_path='./scaler.pkl'):\n",
    "   \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_data)\n",
    "    \n",
    "    # Save the scaler\n",
    "    with open(scaler_path, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    print(f\"Scaler saved to {scaler_path}\")\n",
    "    \n",
    "    return scaler\n",
    "\n",
    "def normalize_data_independent(train_data, val_data, scaler_dir='./scalers'):\n",
    "    \"\"\"\n",
    "    Fit a StandardScaler per station on the training data and transform both train and val data.\n",
    "    Save each scaler to disk for future use.\n",
    "    \n",
    "    Args:\n",
    "        train_data (np.ndarray): Training data of shape (T_train, num_stations, num_features).\n",
    "        val_data (np.ndarray): Validation data of shape (T_val, num_stations, num_features).\n",
    "        scaler_dir (str): Directory path to save the scalers.\n",
    "        \n",
    "    Returns:\n",
    "        train_data_scaled (np.ndarray): Scaled training data of shape (T_train, num_stations, num_features).\n",
    "        val_data_scaled (np.ndarray): Scaled validation data of shape (T_val, num_stations, num_features).\n",
    "        scalers (list of StandardScaler): List containing a scaler for each station.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(scaler_dir):\n",
    "        os.makedirs(scaler_dir)\n",
    "    \n",
    "    T_train, num_stations, num_features = train_data.shape\n",
    "    T_val = val_data.shape[0]\n",
    "    \n",
    "    # Initialize arrays to hold scaled data\n",
    "    train_data_scaled = np.zeros_like(train_data)\n",
    "    val_data_scaled = np.zeros_like(val_data)\n",
    "    \n",
    "    scalers = []\n",
    "    \n",
    "    for station_idx in range(num_stations):\n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        # Extract training data for the current station\n",
    "        train_station_data = train_data[:, station_idx, :]  # Shape: (T_train, num_features)\n",
    "        \n",
    "        # Fit the scaler on training data\n",
    "        scaler.fit(train_station_data)\n",
    "        scalers.append(scaler)\n",
    "        \n",
    "        # Transform training and validation data for the current station\n",
    "        train_data_scaled[:, station_idx, :] = scaler.transform(train_station_data)\n",
    "        val_data_scaled[:, station_idx, :] = scaler.transform(val_data[:, station_idx, :])\n",
    "        \n",
    "        # Save the scaler for the current station\n",
    "        scaler_path = os.path.join(scaler_dir, f'scaler_station_{station_idx}.pkl')\n",
    "        with open(scaler_path, 'wb') as f:\n",
    "            pickle.dump(scaler, f)\n",
    "        print(f\"Scaler for Station {station_idx} saved to {scaler_path}\")\n",
    "    \n",
    "    return train_data_scaled, val_data_scaled, scalers\n",
    "\n",
    "def normalize_data_collective(train_data, val_data, scaler_path='./scaler.pkl'):\n",
    "    \"\"\"\n",
    "    Fit a single StandardScaler across all stations and features.\n",
    "    \n",
    "    Args:\n",
    "        train_data (np.ndarray): Training data of shape (T_train, num_stations, num_features).\n",
    "        val_data (np.ndarray): Validation data of shape (T_val, num_stations, num_features).\n",
    "        scaler_path (str): Path to save the scaler.\n",
    "        \n",
    "    Returns:\n",
    "        train_scaled (np.ndarray), val_scaled (np.ndarray), scaler (StandardScaler)\n",
    "    \"\"\"\n",
    "    T_train, num_stations, num_features = train_data.shape\n",
    "    T_val = val_data.shape[0]\n",
    "    \n",
    "    # Reshape to (T_train*num_stations, num_features)\n",
    "    train_reshaped = train_data.reshape(-1, num_features)\n",
    "    val_reshaped = val_data.reshape(-1, num_features)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_reshaped)\n",
    "    \n",
    "    train_scaled = scaler.transform(train_reshaped).reshape(train_data.shape)\n",
    "    val_scaled = scaler.transform(val_reshaped).reshape(val_data.shape)\n",
    "    \n",
    "    # Save the scaler\n",
    "    with open(scaler_path, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    print(f\"Scaler saved to {scaler_path}\")\n",
    "    \n",
    "    return train_scaled, val_scaled, scaler\n",
    "\n",
    "def load_pkl_file(station_name):\n",
    "    current_path = os.path.dirname(__file__)\n",
    "    file_path = f\"{current_path}\\\\..\\\\..\\\\..\\\\data\\\\{station_name}.pkl\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "        print(f\"data succsesfuly loaded from {file_path}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load file:\\n{e}\")\n",
    "        return None\n",
    "\n",
    "def openJsonFile():\n",
    "    current_path = os.path.dirname(__file__)\n",
    "    file_path = f\"{current_path}\\\\..\\\\..\\\\data code files\\\\stations_details_updated.json\"\n",
    "    with open(file_path) as file:\n",
    "        stations = json.load(file)\n",
    "    return stations\n",
    "\n",
    "def loadCoordinatesNewIsraelData(stations_details, station_name):\n",
    "    for station_id, station_details in stations_details.items():\n",
    "        if station_details[\"name\"] == station_name:\n",
    "            return station_details[\"coordinates_in_a_new_israe\"][\"east\"], station_details[\"coordinates_in_a_new_israe\"][\"north\"]\n",
    "\n",
    "def loadData(station_names):\n",
    "    stations_data = {}\n",
    "    stations_details = openJsonFile()\n",
    "    for station in station_names:\n",
    "        stations_csv = load_pkl_file(station)\n",
    "        station_coordinates = loadCoordinatesNewIsraelData(stations_details, station)\n",
    "        stations_data[station] = stations_csv, station_coordinates\n",
    "    return stations_data\n",
    "\"\"\"\n",
    "# example of use for this file\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the data\n",
    "    stations_data = loadData([\"Afeq\",\"Harashim\"])\n",
    "    if \"Afeq\" in stations_data:\n",
    "        print(\"Data of Afeq:\")\n",
    "        print(stations_data[\"Afeq\"][0].head())\n",
    "\n",
    "        print(\"Coordinate of Afeq:\")\n",
    "        print(stations_data[\"Afeq\"][1])\n",
    "\n",
    "        print(\"First coordinate of Afeq:\")\n",
    "        print(stations_data[\"Afeq\"][1][0])\n",
    "\n",
    "        print(\"Second coordinate of Afeq:\")\n",
    "        print(stations_data[\"Afeq\"][1][1])\n",
    "    else:\n",
    "        print(\"Afeq data not found\")\n",
    "\n",
    "    print(\"yey\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fb1413",
   "metadata": {},
   "source": [
    "### `constantsParams.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a8efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEGINING_OF_YEAR = \"01010000\"\n",
    "ENDING_OF_YEAR = \"12312350\"\n",
    "START_YEAR = 2005\n",
    "END_YEAR = 2024\n",
    "\n",
    "DATA_DIRECTORY = \"data/\"\n",
    "\n",
    "STATIONS_LIST = {\n",
    "    \"Newe Yaar\": \"186\",\n",
    "    \"Tavor Kadoorie\": \"13\",\n",
    "    \"Yavneel\": \"11\",\n",
    "    \"En Hashofet\": \"67\",\n",
    "    \"Eden Farm\": \"206\",\n",
    "    \"Eshhar\": \"205\",\n",
    "    \"Afula Nir Haemeq\": \"16\"\n",
    "}\n",
    "\n",
    "columns = [\n",
    "    \"Date Time\", \"BP (hPa)\", \"DiffR (w/m^2)\", \"Grad (w/m^2)\", \"NIP (w/m^2)\", \"RH (%)\",\n",
    "    \"TD (degC)\", \"TDmax (degC)\", \"TDmin (degC)\", \"WD (deg)\", \"WDmax (deg)\",\n",
    "    \"WS (m/s)\", \"Ws1mm (m/s)\", \"Ws10mm (m/s)\", \"WSmax (m/s)\", \"STDwd (deg)\"\n",
    "]\n",
    "\n",
    "COLUMN_PAIRS = [\n",
    "    (\"date\", \"Date Time\"),\n",
    "    (\"BP\", \"BP (hPa)\"),\n",
    "    (\"DiffR\", \"DiffR (w/m^2)\"),\n",
    "    (\"Grad\", \"Grad (w/m^2)\"),\n",
    "    (\"NIP\", \"NIP (w/m^2)\"),\n",
    "    (\"RH\", \"RH (%)\"),\n",
    "    (\"TD\", \"TD (degC)\"),\n",
    "    (\"TDmax\", \"TDmax (degC)\"),\n",
    "    (\"TDmin\", \"TDmin (degC)\"),\n",
    "    (\"WD\", \"WD (deg)\"),\n",
    "    (\"WDmax\", \"WDmax (deg)\"),\n",
    "    (\"WS\", \"WS (m/s)\"),\n",
    "    (\"WS1mm\", \"Ws1mm (m/s)\"),\n",
    "    (\"Ws10mm\", \"Ws10mm (m/s)\"),\n",
    "    (\"WSmax\", \"WSmax (m/s)\"),\n",
    "    (\"STDwd\", \"STDwd (deg)\")\n",
    "]\n",
    "\n",
    "COLUMNS_TO_REMOVE = ['date_for_sort', 'BP (hPa)', 'Time', 'Grad (w/m^2)', 'DiffR (w/m^2)', 'NIP (w/m^2)', 'Ws10mm (m/s)', 'Ws1mm (m/s)']\n",
    "\n",
    "VALUES_TO_FILL = ['TD (degC)', 'TDmin (degC)', 'TDmax (degC)', 'RH (%)']\n",
    "\n",
    "NA_VALUES = ['None', 'null', '-', '', ' ', 'NaN', 'nan', 'NAN']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1798531",
   "metadata": {},
   "source": [
    "### `import_and_process_data.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141cfee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd# type: ignore\n",
    "import requests# type: ignore\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np# type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "from tqdm import tqdm # type: ignore\n",
    "\n",
    "\n",
    "##    function used for the first time to get the data from the IMS ##\n",
    "######################################################################################################################\n",
    "def fetch_weather_data(station_id, start_date, end_date):\n",
    "    url = f\"https://ims.gov.il/he/envista_station_all_data_time_range/{station_id}/BP%26DiffR%26Grad%26NIP%26RH%26TD%26TDmax%26TDmin%26TW%26WD%26WDmax%26WS%26WS1mm%26Ws10mm%26Ws10maxEnd%26WSmax%26STDwd%26Rain/{start_date}/{end_date}/1/S\"\n",
    "    response = requests.get(url)\n",
    "    data = json.loads(response.content)\n",
    "    return data\n",
    "\n",
    "def fetch_data_for_station(station_id, start_year, end_year):\n",
    "    all_data = []\n",
    "    for year in tqdm(range(start_year, end_year + 1), desc=\"Fetching data by year\"):\n",
    "        today_fore0 = f\"{year}\" + BEGINING_OF_YEAR\n",
    "        today_fore23 = f\"{year}\" + ENDING_OF_YEAR\n",
    "        data = fetch_weather_data(station_id, today_fore0, today_fore23)\n",
    "        process_data(data)\n",
    "        # Convert the data to a DataFrame and append to the list\n",
    "        df = pd.DataFrame(data['data']['records'])\n",
    "        all_data.append(df)\n",
    "    # Concatenate all DataFrames\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "def fetch_data_for_station_manual_time_range(station_id, start_year, end_year, start_date, end_date):\n",
    "    all_data = []\n",
    "    for year in tqdm(range(start_year, end_year + 1), desc=\"Fetching data by year\"):\n",
    "        today_fore0 = f\"{year}\" + start_date\n",
    "        today_fore23 = f\"{year}\" + end_date\n",
    "        data = fetch_weather_data(station_id, today_fore0, today_fore23)\n",
    "        process_data(data)\n",
    "        # Convert the data to a DataFrame and append to the list\n",
    "        df = pd.DataFrame(data['data']['records'])\n",
    "        all_data.append(df)\n",
    "    # Concatenate all DataFrames\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "def get_station_data(station_id, start_year, end_year):\n",
    "    # Get all data for the station\n",
    "    combined_df = fetch_data_for_station(station_id, start_year, end_year)\n",
    "    # Convert the DataFrame back to the dictionary format expected by process_data\n",
    "    data = {'data': {'records': combined_df.to_dict(orient='records')}}\n",
    "    # Process the data\n",
    "    process_data(data)\n",
    "    return data\n",
    "\n",
    "def get_station_data_manual_time_range(station_id, start_year, end_year, start_date, end_date):\n",
    "    # Get all data for the station\n",
    "    combined_df = fetch_data_for_station_manual_time_range(station_id, start_year, end_year, start_date, end_date)\n",
    "    # Convert the DataFrame back to the dictionary format expected by process_data\n",
    "    data = {'data': {'records': combined_df.to_dict(orient='records')}}\n",
    "    # Process the data\n",
    "    process_data(data)\n",
    "    return data\n",
    "\n",
    "def remove_unwanted_keys(data):\n",
    "    # Remove 'sid', 'sname', and 'date_for_sort' from each record in data\n",
    "    for record in data['data']['records']:\n",
    "        # if 'date_for_sort' in record:\n",
    "        #     del record['date_for_sort']\n",
    "        if 'sid' in record:\n",
    "            del record['sid']\n",
    "        if 'TW' in record:\n",
    "            del record['TW']\n",
    "        if 'sname' in record:\n",
    "            del record['sname']\n",
    "\n",
    "def replace_column_names(data):\n",
    "    # Replace the names of the columns by the pairs in COLUMN_PAIRS\n",
    "    for record in data['data']['records']:\n",
    "        for new_name, old_name in COLUMN_PAIRS:\n",
    "            if new_name in record:\n",
    "                record[old_name] = record.pop(new_name)\n",
    "\n",
    "def process_data(data):\n",
    "    remove_unwanted_keys(data)\n",
    "    replace_column_names(data)\n",
    "\n",
    "def get_data_of_stations_from_ims_by_constants_params(StationsList, startYear, endYear):\n",
    "    dataframes = {}\n",
    "    for station_name, station_id in StationsList.items():\n",
    "      print(f\"\\n Downloading data for {station_name}\")\n",
    "      try:\n",
    "          data = get_station_data(station_id, START_YEAR, END_YEAR)\n",
    "          df = pd.DataFrame(data['data']['records'])\n",
    "          dataframes[station_name] = df\n",
    "      except IndexError as e:\n",
    "          print(f\"Error processing data for {station_name}: {e}\")\n",
    "    return dataframes\n",
    "\n",
    "def get_data_of_stations_from_ims_manual_time_range(StationsList, startYear, endYear, startDate, endDate):\n",
    "    dataframes = {}\n",
    "    for station_name, station_id in StationsList.items():\n",
    "      print(f\"\\n Downloading data for {station_name}\")\n",
    "      try:\n",
    "          data = get_station_data_manual_time_range(station_id, startYear, endYear, startDate, endDate)\n",
    "          df = pd.DataFrame(data['data']['records'])\n",
    "          dataframes[station_name] = df\n",
    "      except IndexError as e:\n",
    "          print(f\"Error processing data for {station_name}: {e}\")\n",
    "    return dataframes\n",
    "######################################################################################################################\n",
    "\n",
    "\n",
    "##    functions used for loading and saving the data to pickles   ##\n",
    "######################################################################################################################\n",
    "def save_dataframes_to_pickles(dataframes, DATA_DIRECTORY):\n",
    "  for df_name, df in dataframes.items():\n",
    "      file_path = os.path.join(DATA_DIRECTORY, f\"{df_name}.pkl\")\n",
    "      df.to_pickle(file_path)\n",
    "      print(f\"Saved {df_name} to {file_path}\")\n",
    "\n",
    "def load_dataframes_from_pickles(DATA_DIRECTORY):\n",
    "    data_files = [f for f in os.listdir(DATA_DIRECTORY) if f.endswith('.pkl')]\n",
    "\n",
    "    dataframes = {}\n",
    "    for file in tqdm(data_files, desc=\"Loading Pickle files of year data\"):\n",
    "        file_path = os.path.join(DATA_DIRECTORY, file)\n",
    "        df_name = os.path.splitext(file)[0]\n",
    "        dataframes[df_name] = pd.read_pickle(file_path)\n",
    "    \n",
    "    return dataframes\n",
    "######################################################################################################################\n",
    "\n",
    "\n",
    "##    function used for displaying ##\n",
    "######################################################################################################################\n",
    "def display_dataframes_heads(dataframes):\n",
    "    for df_name, df in dataframes.items():\n",
    "        print(f\"Heads of {df_name}:\")\n",
    "        print(df.head())\n",
    "        print(\"\\n\")\n",
    "\n",
    "def display_wind_before_vectorize(dataframes):\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    # Assuming 'dataframes' is a dictionary of DataFrames and we take the first one\n",
    "    first_df_name = list(dataframes.keys())[0]\n",
    "    first_df = dataframes[first_df_name]\n",
    "\n",
    "    first_df['WD (deg)'] = first_df['WD (deg)'].replace(NA_VALUES, np.nan).infer_objects(copy=False)\n",
    "    first_df['WS (m/s)'] = first_df['WS (m/s)'].replace(NA_VALUES, np.nan).infer_objects(copy=False)\n",
    "    first_df['WDmax (deg)'] = first_df['WDmax (deg)'].replace(NA_VALUES, np.nan).infer_objects(copy=False)\n",
    "    first_df['WSmax (m/s)'] = first_df['WSmax (m/s)'].replace(NA_VALUES, np.nan).infer_objects(copy=False)\n",
    "\n",
    "    # Convert columns to numeric, forcing errors to NaN\n",
    "    first_df['WD (deg)'] = pd.to_numeric(first_df['WD (deg)'], errors='coerce')\n",
    "    first_df['WS (m/s)'] = pd.to_numeric(first_df['WS (m/s)'], errors='coerce')\n",
    "    first_df['WDmax (deg)'] = pd.to_numeric(first_df['WDmax (deg)'], errors='coerce')\n",
    "    first_df['WSmax (m/s)'] = pd.to_numeric(first_df['WSmax (m/s)'], errors='coerce')\n",
    "\n",
    "    # Create subplots\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Mask to filter out NaN values for wind\n",
    "    mask_wind = first_df['WD (deg)'].notna() & first_df['WS (m/s)'].notna()\n",
    "\n",
    "    # Create the 2D histogram plot for wind\n",
    "    hist_wind = ax[0].hist2d(\n",
    "        first_df.loc[mask_wind, 'WD (deg)'],\n",
    "        first_df.loc[mask_wind, 'WS (m/s)'],\n",
    "        bins=(50, 50),\n",
    "        vmax=400\n",
    "    )\n",
    "    fig.colorbar(hist_wind[3], ax=ax[0])\n",
    "    ax[0].set_xlabel('Wind Direction [deg]')\n",
    "    ax[0].set_ylabel('Wind Velocity [m/s]')\n",
    "    ax[0].set_title(f'2D Histogram of Wind for {first_df_name}')\n",
    "\n",
    "    # Mask to filter out NaN values for gust\n",
    "    mask_gust = first_df['WDmax (deg)'].notna() & first_df['WSmax (m/s)'].notna()\n",
    "\n",
    "    # Create the 2D histogram plot for gust\n",
    "    hist_gust = ax[1].hist2d(\n",
    "        first_df.loc[mask_gust, 'WDmax (deg)'],\n",
    "        first_df.loc[mask_gust, 'WSmax (m/s)'],\n",
    "        bins=(50, 50),\n",
    "        vmax=400\n",
    "    )\n",
    "    fig.colorbar(hist_gust[3], ax=ax[1])\n",
    "    ax[1].set_xlabel('Gust Direction [deg]')\n",
    "    ax[1].set_ylabel('Gust Velocity [m/s]')\n",
    "    ax[1].set_title(f'2D Histogram of Gust for {first_df_name}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def display_wind_after_vectorize(dataframes):\n",
    "  # Plot 2D histogram plots of the wind ('Wind_x', 'Wind_y') and gust ('Gust_x', 'Gust_y') for the first dataframe\n",
    "  first_df_name = list(dataframes.keys())[0]\n",
    "  first_df = dataframes[first_df_name]\n",
    "\n",
    "  fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "  # Mask to filter out NaN values for wind\n",
    "  mask_wind = first_df['Wind_x'].notna() & first_df['Wind_y'].notna()\n",
    "\n",
    "  # Create the 2D histogram plot for wind\n",
    "  hist_wind = ax[0].hist2d(first_df.loc[mask_wind, 'Wind_x'], first_df.loc[mask_wind, 'Wind_y'], bins=(50, 50), vmax=400)\n",
    "  fig.colorbar(hist_wind[3], ax=ax[0])\n",
    "  ax[0].set_xlabel('Wind X Component')\n",
    "  ax[0].set_ylabel('Wind Y Component')\n",
    "  ax[0].set_title(f'2D Histogram of Wind Components for {first_df_name}')\n",
    "\n",
    "  # Mask to filter out NaN values for gust\n",
    "  mask_gust = first_df['Gust_x'].notna() & first_df['Gust_y'].notna()\n",
    "\n",
    "  # Create the 2D histogram plot for gust\n",
    "  hist_gust = ax[1].hist2d(first_df.loc[mask_gust, 'Gust_x'], first_df.loc[mask_gust, 'Gust_y'], bins=(50, 50), vmax=400)\n",
    "  fig.colorbar(hist_gust[3], ax=ax[1])\n",
    "  ax[1].set_xlabel('Gust X Component')\n",
    "  ax[1].set_ylabel('Gust Y Component')\n",
    "  ax[1].set_title(f'2D Histogram of Gust Components for {first_df_name}')\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "def print_length_of_dataframes(dataframes):\n",
    "   print(\"\\n  Length of dataframes:\")\n",
    "   for df_name, df in dataframes.items():\n",
    "    print(\"Length of dataframe {}: {}\".format(df_name, len(df)))\n",
    "######################################################################################################################\n",
    "\n",
    "\n",
    "##    function used for syncing the dataframes ##\n",
    "######################################################################################################################\n",
    "def sort_dataframes(dataframes):\n",
    "  # Sort each dataframe by the column 'date_for_sort'\n",
    "  for df_name, df in dataframes.items():\n",
    "    dataframes[df_name] = df.sort_values(by='date_for_sort')\n",
    "\n",
    "def slice_dataframes_beginning(dataframes, begin_date):\n",
    "   for df_name, df in dataframes.items():\n",
    "    index_to_keep = df[df['Date Time'] == begin_date].index\n",
    "    if not index_to_keep.empty:\n",
    "      index_to_keep = index_to_keep[0]\n",
    "      print(f\"Index to keep from dataframe {df_name}: {index_to_keep}\")\n",
    "      dataframes[df_name] = df.loc[index_to_keep:].reset_index(drop=True)\n",
    "    else:\n",
    "      print(f\"{begin_date} not found in dataframe {df_name}\")\n",
    "\n",
    "def delete_rows_not_existing_in_all_dataframes(dataframes):\n",
    "  # Find common 'Date Time' keys present in all dataframes\n",
    "  common_keys = set.intersection(*(set(df['Date Time']) for df in dataframes.values()))\n",
    "  # Initialize a dictionary to store the number of deleted rows\n",
    "  deleted_rows = {}\n",
    "  # Initialize a variable to store the latest deleted date across all dataframes\n",
    "  latest_deleted_date = None\n",
    "  # Remove rows not in common_keys and count deletions\n",
    "  for df_name, df in dataframes.items():\n",
    "    initial_count = len(df)\n",
    "    df_filtered = df[df['Date Time'].isin(common_keys)].reset_index(drop=True)\n",
    "    deleted = initial_count - len(df_filtered)\n",
    "    dataframes[df_name] = df_filtered\n",
    "    deleted_rows[df_name] = deleted\n",
    "    if deleted > 0:\n",
    "      max_deleted_date = df[~df['Date Time'].isin(common_keys)]['Date Time'].max()\n",
    "      if latest_deleted_date is None or max_deleted_date > latest_deleted_date:\n",
    "        latest_deleted_date = max_deleted_date\n",
    "\n",
    "  # Return the number of rows deleted from each dataframe and the latest deleted date\n",
    "  return deleted_rows, latest_deleted_date\n",
    "######################################################################################################################\n",
    "\n",
    "\n",
    "##    function used for preprocessing the dataframes ##\n",
    "######################################################################################################################\n",
    "def remove_unecessery_columns(dataframes, columns_to_remove):\n",
    "  for df_name, df in dataframes.items():\n",
    "    dataframes[df_name] = df.drop(columns=[col for col in columns_to_remove if col in df.columns])\n",
    "\n",
    "def format_the_time_column(dataframes):\n",
    "  for df_name, df in dataframes.items():\n",
    "    if 'Date Time' in df.columns:\n",
    "      df['Date Time'] = pd.to_datetime(df.pop('Date Time'), format=\"%d/%m/%Y %H:%M\")\n",
    "      df['Year'] = df['Date Time'].dt.year\n",
    "\n",
    "def take_round_hours(dataframes):\n",
    "  for df_name, df in dataframes.items():\n",
    "    if 'Date Time' in df.columns:\n",
    "      df = df[df['Date Time'].dt.minute == 0]\n",
    "      dataframes[df_name] = df\n",
    "\n",
    "def fill_1_missing_values(dataframes, values_to_fill, should_print=False):\n",
    "  for df_name, df in dataframes.items():\n",
    "    for value in values_to_fill:\n",
    "      if value in df.columns:\n",
    "        df[value] = df[value].replace(NA_VALUES, np.nan)\n",
    "        \n",
    "        # Fill NaN values wrapped with two non-NaN values:\n",
    "        nan_wrapped_count = 0\n",
    "        value_values = df[value].values\n",
    "        for i in range(1, len(value_values) - 1):\n",
    "          if pd.isna(value_values[i]) and not pd.isna(value_values[i - 1]) and not pd.isna(value_values[i + 1]):\n",
    "            try:\n",
    "              value_values[i] = (float(value_values[i - 1]) + float(value_values[i + 1])) / 2\n",
    "              nan_wrapped_count += 1\n",
    "            except ValueError as e:\n",
    "              print(f\"ValueError encountered in {df_name} at index {i} for column {value}: {e}\")\n",
    "        if should_print:\n",
    "          print(f\"Number of NaN values wrapped with two non-NaN values and filled in {df_name} station for column {value}: {nan_wrapped_count} which is {nan_wrapped_count / len(df) * 100}% of the data\")\n",
    "\n",
    "def fill_2_missing_values(dataframes, values_to_fill, should_print=False):\n",
    "  for value in values_to_fill:\n",
    "    for df_name, df in dataframes.items():\n",
    "      if value in df.columns:\n",
    "        df[value] = df[value].replace(NA_VALUES, np.nan)\n",
    "        \n",
    "        # Fill two consecutive NaN values wrapped with two non-NaN values:\n",
    "        nan_wrapped_count = 0\n",
    "        value_values = df[value].values\n",
    "        i = 2  # Start from index 2 to ensure i-2 is valid\n",
    "        while i < len(value_values) - 2:\n",
    "          if (pd.isna(value_values[i]) and pd.isna(value_values[i+1]) and\n",
    "            not pd.isna(value_values[i - 2]) and not pd.isna(value_values[i - 1]) and\n",
    "            not pd.isna(value_values[i + 2]) and not pd.isna(value_values[i + 3])):\n",
    "            \n",
    "            val1 = float(value_values[i - 2])\n",
    "            val2 = float(value_values[i - 1])\n",
    "            val3 = float(value_values[i + 2])\n",
    "            val4 = float(value_values[i + 3])\n",
    "            \n",
    "            # Determine trends\n",
    "            trend_before = val2 < val1\n",
    "            trend_after = val4 < val3\n",
    "            if trend_before == trend_after:\n",
    "              try:\n",
    "                diff = val3 - val2\n",
    "                value_values[i] = val2 + diff / 3\n",
    "                value_values[i + 1] = val2 + diff * 2 / 3\n",
    "                nan_wrapped_count += 2\n",
    "                i += 2  # Skip the next index as it's already processed\n",
    "                continue\n",
    "              except ValueError as e:\n",
    "                print(f\"ValueError encountered in {df_name} at indices {i} and {i+1} for column {value}: {e}\")\n",
    "          i += 1\n",
    "        if should_print:\n",
    "          print(f\"Number of NaN values wrapped with two non-NaN values and filled in {df_name} station for column {value}: {nan_wrapped_count} which is {nan_wrapped_count / len(df) * 100:.2f}% of the data\")\n",
    "    \n",
    "def fill_3_missing_values(dataframes, values_to_fill, should_print=False):\n",
    "  for value in values_to_fill:\n",
    "    for df_name, df in dataframes.items():\n",
    "      if value in df.columns:\n",
    "        df[value] = df[value].replace(NA_VALUES, np.nan)\n",
    "        \n",
    "        # Fill three consecutive NaN values wrapped with two non-NaN values:\n",
    "        nan_wrapped_count = 0\n",
    "        value_values = df[value].values\n",
    "        i = 2  # Start from index 2 to ensure i-2 is valid\n",
    "        while i < len(value_values) - 4:\n",
    "          if (pd.isna(value_values[i]) and pd.isna(value_values[i+1]) and pd.isna(value_values[i+2]) and\n",
    "            not pd.isna(value_values[i - 2]) and not pd.isna(value_values[i - 1]) and\n",
    "            not pd.isna(value_values[i + 3]) and not pd.isna(value_values[i + 4])):\n",
    "            \n",
    "            val1 = float(value_values[i - 2])\n",
    "            val2 = float(value_values[i - 1])\n",
    "            val3 = float(value_values[i + 3])\n",
    "            val4 = float(value_values[i + 4])\n",
    "            \n",
    "            # Determine trends\n",
    "            trend_before = val2 < val1\n",
    "            trend_after = val4 < val3\n",
    "            if trend_before == trend_after:\n",
    "              try:\n",
    "                diff = val3 - val2\n",
    "                value_values[i] = val2 + diff / 4\n",
    "                value_values[i + 1] = val2 + (diff * 2) / 4\n",
    "                value_values[i + 2] = val2 + (diff * 3) / 4\n",
    "                nan_wrapped_count += 3\n",
    "                i += 3  # Skip the next indices as they're already processed\n",
    "                continue\n",
    "              except ValueError as e:\n",
    "                print(f\"ValueError encountered in {df_name} at indices {i}, {i+1}, and {i+2} for column {value}: {e}\")\n",
    "          i += 1\n",
    "        if should_print:\n",
    "          print(f\"Number of NaN values wrapped with three non-NaN values and filled in {df_name} station for column {value}: {nan_wrapped_count} which is {nan_wrapped_count / len(df) * 100:.2f}% of the data\")\n",
    "\n",
    "def replace_time_with_cyclic_representation(dataframes):\n",
    "  day = 24*60*60\n",
    "  year = (365.2425)*day\n",
    "\n",
    "  for df_name, df in dataframes.items():\n",
    "    if 'Date Time' in df.columns:\n",
    "      timestamp_s = df['Date Time'].map(pd.Timestamp.timestamp)\n",
    "      df['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n",
    "      df['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n",
    "      df['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n",
    "      df['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))\n",
    "      dataframes[df_name] = df.drop(columns=['Date Time'])\n",
    "\n",
    "def vectorize_wind(dataframes):\n",
    "  print(\"vectorizing wind.\")\n",
    "  for df_name, df in dataframes.items():\n",
    "    try:\n",
    "      wind_speed = pd.to_numeric(df.pop('WS (m/s)'), errors='coerce')\n",
    "      wind_direction_rad = pd.to_numeric(df.pop('WD (deg)'), errors='coerce') * np.pi / 180\n",
    "      if wind_speed is not None and wind_direction_rad is not None:\n",
    "        mask_wind = wind_speed.notna() & wind_direction_rad.notna()\n",
    "        df['Wind_x'] = wind_speed * np.cos(wind_direction_rad)\n",
    "        df['Wind_y'] = wind_speed * np.sin(wind_direction_rad)\n",
    "        df.loc[~mask_wind, ['Wind_x', 'Wind_y']] = np.nan\n",
    "\n",
    "      gust_speed = pd.to_numeric(df.pop('WSmax (m/s)'), errors='coerce')\n",
    "      gust_direction_rad = pd.to_numeric(df.pop('WDmax (deg)'), errors='coerce') * np.pi / 180\n",
    "      if gust_speed is not None and gust_direction_rad is not None:\n",
    "        mask_gust = gust_speed.notna() & gust_direction_rad.notna()\n",
    "        df['Gust_x'] = gust_speed * np.cos(gust_direction_rad)\n",
    "        df['Gust_y'] = gust_speed * np.sin(gust_direction_rad)\n",
    "        df.loc[~mask_gust, ['Gust_x', 'Gust_y']] = np.nan\n",
    "    except KeyError as e:\n",
    "      print(f\"KeyError encountered in {df_name}: {e}\")\n",
    "    except TypeError as e:\n",
    "      print(f\"TypeError encountered in {df_name}: {e}\")\n",
    "\n",
    "def drop_nan_rows_multiple(dataframes, reset_indices=True):\n",
    "    \"\"\"\n",
    "    Removes rows from all DataFrames in the list where any DataFrame has NaN in any column.\n",
    "    \n",
    "    Parameters:\n",
    "    df_list (List[pd.DataFrame]): List of DataFrames to process.\n",
    "    reset_indices (bool): Whether to reset the index after dropping rows. Defaults to True.\n",
    "    \n",
    "    Returns:\n",
    "    Tuple[List[pd.DataFrame], Optional[pd.Timestamp]]: List of cleaned DataFrames and the latest 'Date Time' of removed rows.\n",
    "    \"\"\"\n",
    "    if not dataframes:\n",
    "        raise ValueError(\"The list of DataFrames is empty.\")\n",
    "    \n",
    "    # Step 1: Identify rows with any NaN in each DataFrame\n",
    "    nan_indices_list = [df.isnull().any(axis=1) for df in dataframes.values()]\n",
    "    \n",
    "    # Step 2: Combine the indices where NaNs are present in any DataFrame\n",
    "    combined_nan = pd.Series([False] * len(dataframes[list(dataframes.keys())[0]]), index=dataframes[list(dataframes.keys())[0]].index)\n",
    "    for nan_mask in nan_indices_list:\n",
    "        combined_nan = combined_nan | nan_mask\n",
    "    \n",
    "    # Get the indices to drop\n",
    "    indices_to_drop = combined_nan[combined_nan].index\n",
    "    print(\" number of rows with NaNs to drop: \", len(indices_to_drop))\n",
    "    \n",
    "    # Step 3: Find the latest 'Date Time' of the rows to be removed\n",
    "    latest_removed_date = None\n",
    "    for df in dataframes.values():\n",
    "        if 'Date Time' in df.columns:\n",
    "            removed_dates = df.loc[indices_to_drop, 'Date Time']\n",
    "            if not removed_dates.empty:\n",
    "                max_date = pd.to_datetime(removed_dates, errors='coerce').max()\n",
    "                if latest_removed_date is None or (max_date is not pd.NaT and max_date > latest_removed_date):\n",
    "                    latest_removed_date = max_date\n",
    "    \n",
    "    # Step 4: Drop the identified indices from all DataFrames\n",
    "    cleaned_df_list = []\n",
    "    for df in dataframes.values():\n",
    "        cleaned_df = df.drop(indices_to_drop)\n",
    "        if reset_indices:\n",
    "            cleaned_df = cleaned_df.reset_index(drop=True)\n",
    "        cleaned_df_list.append(cleaned_df)\n",
    "    \n",
    "    return cleaned_df_list, latest_removed_date\n",
    "######################################################################################################################\n",
    "\n",
    "def get_prccessed_latest_data_by_hour_and_station(stations_list, hours_back, begin_forecast_time=datetime.now()):\n",
    "  success = True\n",
    "  end_datetime = begin_forecast_time.replace(minute=0, second=0, microsecond=0)\n",
    "  start_datetime = end_datetime - pd.Timedelta(days=7) # 7 days back\n",
    "  startYear, endYear = start_datetime.year, end_datetime.year\n",
    "  startDate, endDate = start_datetime.strftime(\"%m%d%H%M\"), end_datetime.strftime(\"%m%d%H%M\")\n",
    "  print(f\"startDate: {startDate}, endDate: {endDate}\")\n",
    "\n",
    "  dataframes = get_data_of_stations_from_ims_manual_time_range(stations_list, startYear, endYear, startDate, endDate)\n",
    "  if len(dataframes) == 0:\n",
    "    success = False\n",
    "  \n",
    "  sort_dataframes(dataframes)\n",
    "  latest_deleted_date = delete_rows_not_existing_in_all_dataframes(dataframes)\n",
    "  if latest_deleted_date and isinstance(latest_deleted_date, datetime) and latest_deleted_date > end_datetime:\n",
    "    success = False\n",
    "  remove_unecessery_columns(dataframes, COLUMNS_TO_REMOVE)\n",
    "  format_the_time_column(dataframes)\n",
    "  fill_1_missing_values(dataframes, VALUES_TO_FILL)\n",
    "  fill_2_missing_values(dataframes, VALUES_TO_FILL)\n",
    "  fill_3_missing_values(dataframes, VALUES_TO_FILL)\n",
    "  take_round_hours(dataframes)\n",
    "  fill_1_missing_values(dataframes, VALUES_TO_FILL)\n",
    "  fill_2_missing_values(dataframes, VALUES_TO_FILL)\n",
    "  last_datetime = pd.to_datetime(dataframes[list(dataframes.keys())[0]]['Date Time'].iloc[-1], format=\"%d/%m/%Y %H:%M\")\n",
    "  last_hour = last_datetime.strftime(\"%H:%M\")\n",
    "  last_date = last_datetime.strftime(\"%Y-%m-%d\")\n",
    "  print(f\"last hour: {last_hour}, last hour date: {last_date}\")\n",
    "\n",
    "  replace_time_with_cyclic_representation(dataframes)\n",
    "  vectorize_wind(dataframes)\n",
    "  latest_deleted_date = drop_nan_rows_multiple(dataframes)\n",
    "  if latest_deleted_date and isinstance(latest_deleted_date, datetime) and latest_deleted_date > end_datetime:\n",
    "    success = False\n",
    "    \n",
    "  for df_name, df in dataframes.items(): # return only the last hours_back hours\n",
    "      if len(df) > hours_back:\n",
    "          dataframes[df_name] = df.iloc[-hours_back:].reset_index(drop=True)\n",
    "  return dataframes, last_hour, last_date, success\n",
    "\n",
    "## how to use this function:\n",
    "    # stations_list = STATIONS_LIST\n",
    "    # hours_back = 72\n",
    "    # dataframes, last_hour, last_date, success = get_prccessed_latest_data_by_hour_and_station(stations_list, hours_back)\n",
    "    # print(f\"len of df: {len(dataframes)}\")\n",
    "    # print(f\"len of df[0]: {len(dataframes[list(dataframes.keys())[0]])}\")\n",
    "    # print(f\"len of df[1]: {len(dataframes[list(dataframes.keys())[1]])}\")\n",
    "    # print(f\"len of df[2]: {len(dataframes[list(dataframes.keys())[2]])}\")\n",
    "    # print(f\"Last hour: {last_hour}\")\n",
    "    # print(f\"Last hour date: {last_date}\")\n",
    "    # print(f\"success: {success}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "  #menue: \n",
    "  ## load from:\n",
    "  get_data_from_ims = True # (including the first process)\n",
    "  load_data_from_directory = not get_data_from_ims and True\n",
    "  ## save to:\n",
    "  save_data_to_pickles_in_the_end = True\n",
    "  ## sync:\n",
    "  sync = True #master switch\n",
    "  should_sort_dataframes = sync and True\n",
    "  should_slice_dataframes_beginning = sync and True\n",
    "  should_delete_rows_not_existing_in_all_dataframes = sync and True\n",
    "  ## preprocessing:\n",
    "  preprocess = True\n",
    "  should_remove_unecessery_columns = preprocess and True\n",
    "  should_format_the_time_column = preprocess and True\n",
    "  data_imputation = preprocess and True\n",
    "  should_fill_data_1_missing_value = data_imputation and True\n",
    "  should_fill_data_2_missing_values = data_imputation and True\n",
    "  should_fill_data_3_missing_values = data_imputation and True\n",
    "  should_take_round_hours = preprocess and True\n",
    "  should_replace_time_with_cyclic_representation = preprocess and True #leave it false\n",
    "  should_vectorize_wind = preprocess and True\n",
    "  should_drop_nan_rows = preprocess and True\n",
    "    ## display:\n",
    "  should_display_heads_of_dataframes = False\n",
    "  should_print_length_of_dataframes = True\n",
    "  should_display_wind_before_vectorize = False\n",
    "  should_display_wind_after_vectorize = should_vectorize_wind and False\n",
    "\n",
    "\n",
    "  if get_data_from_ims:\n",
    "    dataframes = get_data_of_stations_from_ims_by_constants_params(STATIONS_LIST, START_YEAR, END_YEAR)\n",
    "\n",
    "  if load_data_from_directory:\n",
    "    dataframes = load_dataframes_from_pickles(DATA_DIRECTORY)\n",
    "\n",
    "  if should_sort_dataframes:\n",
    "    sort_dataframes(dataframes)\n",
    "\n",
    "  if should_slice_dataframes_beginning:\n",
    "    slice_dataframes_beginning(dataframes, '01/04/2005 00:00')\n",
    "  \n",
    "  if should_print_length_of_dataframes:\n",
    "    print_length_of_dataframes(dataframes)\n",
    "\n",
    "  if should_delete_rows_not_existing_in_all_dataframes:\n",
    "    delete_rows_not_existing_in_all_dataframes(dataframes)\n",
    "\n",
    "  if sync and should_print_length_of_dataframes:\n",
    "    print_length_of_dataframes(dataframes)\n",
    "\n",
    "  if should_remove_unecessery_columns:\n",
    "    remove_unecessery_columns(dataframes, COLUMNS_TO_REMOVE)\n",
    "\n",
    "  if should_format_the_time_column:\n",
    "    format_the_time_column(dataframes)\n",
    "\n",
    "  if data_imputation: # imputation before rounding hours\n",
    "    if should_fill_data_1_missing_value:\n",
    "      fill_1_missing_values(dataframes, VALUES_TO_FILL, should_print=True)\n",
    "    if should_fill_data_2_missing_values:\n",
    "      fill_2_missing_values(dataframes, VALUES_TO_FILL, should_print=True)\n",
    "    if should_fill_data_3_missing_values:\n",
    "      fill_3_missing_values(dataframes, VALUES_TO_FILL, should_print=True)\n",
    "\n",
    "  if should_take_round_hours:\n",
    "    take_round_hours(dataframes)\n",
    "\n",
    "  if data_imputation: # imputation after rounding hours\n",
    "    if should_fill_data_1_missing_value:\n",
    "      fill_1_missing_values(dataframes, VALUES_TO_FILL, should_print=True)\n",
    "    if should_fill_data_2_missing_values:\n",
    "      fill_2_missing_values(dataframes, VALUES_TO_FILL, should_print=True)\n",
    "\n",
    "  if should_replace_time_with_cyclic_representation:\n",
    "    replace_time_with_cyclic_representation(dataframes)\n",
    "\n",
    "  if should_display_wind_before_vectorize:\n",
    "    display_wind_before_vectorize(dataframes)\n",
    "\n",
    "  if should_vectorize_wind:\n",
    "    vectorize_wind(dataframes)\n",
    "\n",
    "  if should_display_wind_after_vectorize:\n",
    "    display_wind_after_vectorize(dataframes)\n",
    "  \n",
    "  if should_drop_nan_rows:\n",
    "    drop_nan_rows_multiple(dataframes)\n",
    "\n",
    "  if should_display_heads_of_dataframes:\n",
    "    display_dataframes_heads(dataframes)\n",
    "\n",
    "  if save_data_to_pickles_in_the_end:\n",
    "    save_dataframes_to_pickles(dataframes, DATA_DIRECTORY)\n",
    "\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5253202c",
   "metadata": {},
   "source": [
    "### `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84e27b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class StationCNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_features=15,\n",
    "                 output_per_feature=3,\n",
    "                 kernel_size=3,\n",
    "                 use_batch_norm=False,\n",
    "                 use_residual=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_features (int): Number of input features per station.\n",
    "            output_per_feature (int): Number of output channels per feature.\n",
    "            kernel_size (int): Size of the convolutional kernel.\n",
    "            use_batch_norm (bool): Whether to use Batch Normalization.\n",
    "            use_residual (bool): Whether to use residual connections.\n",
    "        \"\"\"\n",
    "        super(StationCNN, self).__init__()\n",
    "        self.output_per_feature = output_per_feature\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.use_residual = use_residual\n",
    "\n",
    "        # Total out_channels = input_features * output_per_feature\n",
    "        self.out_channels = input_features * output_per_feature\n",
    "\n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=input_features,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=kernel_size // 2,\n",
    "            groups=input_features  # Depthwise convolution\n",
    "        )\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # Optional Batch Normalization\n",
    "        if self.use_batch_norm:\n",
    "            self.bn1 = nn.BatchNorm1d(self.out_channels)\n",
    "\n",
    "        # Second convolutional layer (optional for deeper CNN)\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=self.out_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=kernel_size // 2,\n",
    "            groups=input_features  # Maintain feature independence\n",
    "        )\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Optional Batch Normalization\n",
    "        if self.use_batch_norm:\n",
    "            self.bn2 = nn.BatchNorm1d(self.out_channels)\n",
    "\n",
    "        # Optional Residual Connection\n",
    "        if self.use_residual:\n",
    "            self.residual_conv = nn.Conv1d(\n",
    "                in_channels=input_features,\n",
    "                out_channels=self.out_channels,\n",
    "                kernel_size=1,\n",
    "                groups=input_features  # Depthwise 1x1 convolution\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, input_features, time_steps]\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape [batch_size, output_per_feature, time_steps, input_features]\n",
    "        \"\"\"\n",
    "        b, f, t = x.shape  # [batch_size, input_features, time_steps]\n",
    "\n",
    "        # First convolution\n",
    "        out = self.conv1(x)  # [batch_size, input_features * output_per_feature, time_steps]\n",
    "        if self.use_batch_norm:\n",
    "            out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        # Second convolution\n",
    "        out = self.conv2(out)  # [batch_size, input_features * output_per_feature, time_steps]\n",
    "        if self.use_batch_norm:\n",
    "            out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        # Optional Residual Connection\n",
    "        if self.use_residual:\n",
    "            residual = self.residual_conv(x)\n",
    "            out = out + residual  # Ensures a new tensor is produced\n",
    "            out = self.relu2(out)\n",
    "\n",
    "        # Reshape to [batch_size, output_per_feature, input_features, time_steps]\n",
    "        out = out.view(b, self.output_per_feature, f, t)\n",
    "        # Permute to [batch_size, output_per_feature, time_steps, input_features]\n",
    "        out = out.permute(0, 1, 3, 2)  # [batch_size, output_per_feature, time_steps, features]\n",
    "\n",
    "        return out  # [batch_size, output_per_feature, time_steps, features]\n",
    "\n",
    "class CoordinatePositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(CoordinatePositionalEncoding, self).__init__()\n",
    "        # Assuming two coordinates: X and Y\n",
    "        self.lat_linear = nn.Linear(1, d_model // 2)\n",
    "        self.lon_linear = nn.Linear(1, d_model // 2)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, lat, lon):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            lat (torch.Tensor): [num_stations, 1] - Normalized X coordinates\n",
    "            lon (torch.Tensor): [num_stations, 1] - Normalized Y coordinates\n",
    "        Returns:\n",
    "            torch.Tensor: [num_stations, d_model]\n",
    "        \"\"\"\n",
    "        lat_enc = self.lat_linear(lat)  # [num_stations, d_model//2]\n",
    "        lon_enc = self.lon_linear(lon)  # [num_stations, d_model//2]\n",
    "        spatial_emb = self.activation(torch.cat([lat_enc, lon_enc], dim=1))  # [num_stations, d_model]\n",
    "        return spatial_emb\n",
    "\n",
    "class TemporalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(TemporalPositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): [batch_size, num_stations, time_steps, d_model]\n",
    "        Returns:\n",
    "            torch.Tensor: [batch_size, num_stations, time_steps, d_model]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(2), :].unsqueeze(1)  # [batch, num_stations, time_steps, d_model]\n",
    "        return x\n",
    "\n",
    "class TargetedWeatherPredictionModel(nn.Module):\n",
    "    def __init__(self, num_stations, time_steps, feature_dim, kernel_size,\n",
    "                 d_model, nhead, num_layers, target_station_idx, label_width=1,\n",
    "                 output_per_feature=3, use_batch_norm=False, use_residual=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_stations (int): Number of stations.\n",
    "            time_steps (int): Number of time steps in the sliding window.\n",
    "            feature_dim (int): Number of features per station.\n",
    "            kernel_size (int): Size of the CNN kernel.\n",
    "            d_model (int): Dimension of the model (for Transformer).\n",
    "            nhead (int): Number of attention heads in the Transformer.\n",
    "            num_layers (int): Number of Transformer encoder layers.\n",
    "            target_station_idx (int): Index of the target station.\n",
    "            label_width (int): Number of prediction steps.\n",
    "            output_per_feature (int): Number of output channels per feature in CNN.\n",
    "            use_batch_norm (bool): Whether to use Batch Normalization in CNNs.\n",
    "            use_residual (bool): Whether to use residual connections in CNNs.\n",
    "        \"\"\"\n",
    "        super(TargetedWeatherPredictionModel, self).__init__()\n",
    "        self.num_stations = num_stations\n",
    "        self.time_steps = time_steps\n",
    "        self.target_station_idx = target_station_idx\n",
    "        self.label_width = label_width\n",
    "        self.output_per_feature = output_per_feature\n",
    "\n",
    "        # Initialize separate CNNs for each station\n",
    "        self.station_cnns = nn.ModuleList([\n",
    "            StationCNN(\n",
    "                input_features=feature_dim,\n",
    "                output_per_feature=output_per_feature,\n",
    "                kernel_size=kernel_size,\n",
    "                use_batch_norm=use_batch_norm,\n",
    "                use_residual=use_residual\n",
    "            )\n",
    "            for _ in range(num_stations)\n",
    "        ])\n",
    "\n",
    "        # Coordinate Positional Encoding\n",
    "        self.coord_pos_encoding = CoordinatePositionalEncoding(d_model=d_model)\n",
    "\n",
    "        # Linear layer to map CNN features to d_model\n",
    "        # New feature_dim after CNN: output_per_feature * original feature_dim\n",
    "        self.feature_mapping = nn.Linear(feature_dim * output_per_feature, d_model)\n",
    "\n",
    "        # Temporal Positional Encoding\n",
    "        self.temporal_pos_encoding = TemporalPositionalEncoding(d_model=d_model, max_len=time_steps)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Final prediction layer\n",
    "        self.fc_out = nn.Linear(d_model, label_width)  # Output label_width predictions\n",
    "\n",
    "    def forward(self, x, lat, lon):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): [batch_size, num_stations, time_steps, feature_dim]\n",
    "            lat (torch.Tensor): [num_stations, 1] - Normalized X coordinates\n",
    "            lon (torch.Tensor): [num_stations, 1] - Normalized Y coordinates\n",
    "        Returns:\n",
    "            torch.Tensor: [batch_size, label_width]\n",
    "        \"\"\"\n",
    "        batch_size, num_stations, time_steps, feature_dim = x.size()\n",
    "\n",
    "        # Extract temporal features for each station\n",
    "        # Initialize a list to collect CNN outputs\n",
    "        temporal_features = []\n",
    "        for i in range(num_stations):\n",
    "            station_data = x[:, i, :, :]  # [batch_size, time_steps, feature_dim]\n",
    "            station_data = station_data.permute(0, 2, 1)  # [batch_size, feature_dim, time_steps]\n",
    "            cnn_out = self.station_cnns[i](station_data)  # [batch_size, output_per_feature, time_steps, feature_dim]\n",
    "            temporal_features.append(cnn_out)\n",
    "\n",
    "        # Stack temporal features: [batch_size, num_stations, output_per_feature, time_steps, feature_dim]\n",
    "        temporal_features = torch.stack(temporal_features, dim=1)  # [batch, num_stations, output_per_feature, time_steps, features]\n",
    "\n",
    "        # Reshape to combine output_per_feature and features dimensions\n",
    "        # New shape: [batch_size, num_stations, time_steps, output_per_feature * feature_dim]\n",
    "        temporal_features = temporal_features.view(batch_size, num_stations, self.output_per_feature, time_steps, feature_dim)\n",
    "        temporal_features = temporal_features.permute(0, 1, 3, 2, 4)  # [batch, num_stations, time_steps, output_per_feature, features]\n",
    "        temporal_features = temporal_features.contiguous().view(batch_size, num_stations, time_steps, self.output_per_feature * feature_dim)  # [batch, num_stations, time_steps, output_per_feature * features]\n",
    "\n",
    "        # Spatial positional encoding using coordinates\n",
    "        spatial_emb = self.coord_pos_encoding(lat, lon)  # [num_stations, d_model]\n",
    "        spatial_emb = spatial_emb.unsqueeze(0).unsqueeze(2)  # [1, num_stations, 1, d_model]\n",
    "\n",
    "        # Map temporal features to d_model\n",
    "        temporal_features = self.feature_mapping(temporal_features)  # [batch_size, num_stations, time_steps, d_model]\n",
    "\n",
    "        # Apply temporal positional encoding\n",
    "        temporal_features = self.temporal_pos_encoding(temporal_features)  # [batch, num_stations, time_steps, d_model]\n",
    "\n",
    "        # Combine temporal and spatial features\n",
    "        combined_features = temporal_features + spatial_emb  # [batch, num_stations, time_steps, d_model]\n",
    "\n",
    "        # Reshape for Transformer: [batch_size, num_stations * time_steps, d_model]\n",
    "        combined_features = combined_features.view(batch_size, num_stations * time_steps, -1)\n",
    "\n",
    "        # Transpose for Transformer: [sequence_length, batch_size, d_model]\n",
    "        combined_features = combined_features.permute(1, 0, 2)  # [num_stations * time_steps, batch_size, d_model]\n",
    "\n",
    "        # Transformer expects [sequence_length, batch_size, d_model]\n",
    "        transformer_out = self.transformer_encoder(combined_features)  # [sequence_length, batch_size, d_model]\n",
    "\n",
    "        # Reshape back: [batch_size, num_stations, time_steps, d_model]\n",
    "        transformer_out = transformer_out.permute(1, 0, 2)  # [batch_size, sequence_length, d_model]\n",
    "        transformer_out = transformer_out.view(batch_size, num_stations, time_steps, -1)  # [batch_size, num_stations, time_steps, d_model]\n",
    "\n",
    "        # Select target station's features: [batch_size, time_steps, d_model]\n",
    "        target_features = transformer_out[:, self.target_station_idx, :, :]  # [batch_size, time_steps, d_model]\n",
    "\n",
    "        # Instead of mean pooling, retain temporal information or use other aggregation\n",
    "        # Here, we'll take the last time step's features for simplicity\n",
    "        last_time_step_features = target_features[:, -1, :]  # [batch_size, d_model]\n",
    "\n",
    "        # Final prediction\n",
    "        prediction = self.fc_out(last_time_step_features)  # [batch_size, label_width]\n",
    "\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8c52dd",
   "metadata": {},
   "source": [
    "### `parameters.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f928b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Determine device\n",
    "\n",
    "# must define these 3 variables below!!!\n",
    "###########################################################################################################################################################\n",
    "\n",
    "# for training - where the output will be saved\n",
    "#base_path = 'C:\\\\Users\\\\dorsh\\\\Documents\\\\GitHub\\\\WeatherNet\\\\backend\\\\Model_Pytorch\\\\AdvancedModel\\\\models' # in general we put the folder of the path that contains the parameters.py file\n",
    "colab_path = 'C:\\\\Users\\\\dorsh\\\\Documents\\\\GitHub\\\\WeatherNet\\\\backend\\\\Model_Pytorch\\\\AdvancedModel\\\\models' # in general we put the folder of the path that contains the parameters.py file\n",
    "colab_path = r'/content/models_for_inference'\n",
    "#'C:\\\\Users\\\\dorsh\\\\Documents\\\\GitHub\\\\WeatherNet\\\\backend\\\\Model_Pytorch\\\\AdvancedModel\\\\models'\n",
    "name_of_the_model_to_save_train = r'1_12'\n",
    "\n",
    "# for inference\n",
    "models_paths_dir_names_for_inference = ['1_12','12_24','24_36','36_60'] # for instance for alot of models we want to inference: ['model_1','model_2' ... ] for one : ['model_1']\n",
    "\n",
    "###########################################################################################################################################################\n",
    "base_path = colab_path\n",
    "output_path = os.path.join(base_path, name_of_the_model_to_save_train)\n",
    "checkpoints_path = os.path.join(output_path, 'checkpoints')\n",
    "scalers_path = os.path.join(output_path, 'scalers')\n",
    "inference_output_path = os.path.join(output_path, 'inference_output')\n",
    "\n",
    "STATIONS_COORDINATES = {\n",
    "    'Tavor Kadoorie':           (238440, 734540), #station id: 13\n",
    "    'Newe Yaar':                (217010, 734820), #station id: 186\n",
    "    'Yavneel':                  (248110, 733730), #station id: 11\n",
    "    'En Hashofet':              (209310, 723170), #station id: 67\n",
    "    'Eden Farm':                (246190, 708240), #station id: 206\n",
    "    'Eshhar':                   (228530, 754390), #station id: 205\n",
    "    'Afula Nir Haemeq':         (226260, 722410)  #station id: 16\n",
    "}\n",
    "\n",
    "STATIONS_COORDINATES_COLAB = {\n",
    "    f'/content/drive/MyDrive/final data/Tavor Kadoorie.pkl':     (238440, 734540),\n",
    "    f'/content/drive/MyDrive/final data/Newe Yaar.pkl':          (217010, 734820),\n",
    "    f'/content/drive/MyDrive/final data/Yavneel.pkl':            (248110, 733730),\n",
    "    f'/content/drive/MyDrive/final data/En Hashofet.pkl':        (209310, 723170),\n",
    "    f'/content/drive/MyDrive/final data/Eden Farm.pkl':          (246190, 708240),\n",
    "    f'/content/drive/MyDrive/final data/Eshhar.pkl':             (228530, 754390),\n",
    "    f'/content/drive/MyDrive/final data/Afula Nir Haemeq.pkl':   (226260, 722410)\n",
    "}\n",
    "\n",
    "STATIONS_LIST = {\n",
    "    \"Tavor Kadoorie\":   \"13\",\n",
    "    \"Newe Yaar\":        \"186\",\n",
    "    \"Yavneel\":          \"11\",\n",
    "    \"En Hashofet\":      \"67\",\n",
    "    \"Eden Farm\":        \"206\",\n",
    "    \"Eshhar\":           \"205\",\n",
    "    \"Afula Nir Haemeq\": \"16\"\n",
    "}\n",
    "\n",
    "PARAMS = {\n",
    "    'paths_in_colab': [\n",
    "        f'/content/drive/MyDrive/final data/Tavor Kadoorie.pkl',\n",
    "        f'/content/drive/MyDrive/final data/Newe Yaar.pkl',\n",
    "        f'/content/drive/MyDrive/final data/Yavneel.pkl',\n",
    "        f'/content/drive/MyDrive/final data/En Hashofet.pkl',\n",
    "        f'/content/drive/MyDrive/final data/Eden Farm.pkl',\n",
    "        f'/content/drive/MyDrive/final data/Eshhar.pkl',\n",
    "        f'/content/drive/MyDrive/final data/Afula Nir Haemeq.pkl'],\n",
    "    'fileNames':        ['Tavor Kadoorie', 'Newe Yaar', 'Yavneel', 'En Hashofet', 'Eden Farm', 'Eshhar', 'Afula Nir Haemeq'],\n",
    "    'target_station':   'Tavor Kadoorie',\n",
    "    'target_station_desplay_name':   'Tavor Kadoorie',\n",
    "    'target_station_id': 0,\n",
    "    'device' :           device,\n",
    "    'in_channels' :      15, # how many features we have\n",
    "    'output_path':       output_path,\n",
    "    'checkpoints_path':  checkpoints_path,\n",
    "    'scalers_path':      scalers_path,\n",
    "    'inference_output_path': inference_output_path\n",
    "}\n",
    "\n",
    "WINDOW_PARAMS = {\n",
    "    'input_width' :     72, # window input size\n",
    "    'label_width' :     12, # how many hours to predict to the future\n",
    "    'shift' :           1,\n",
    "    'label_columns' :   ['TD (degC)'],\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "WINDOW_PARAMS = {\n",
    "    'input_width' :     72, # window input size\n",
    "    'label_width' :     12, # how many hours to predict to the future\n",
    "    'shift' :           1,\n",
    "    'label_columns' :   ['TD (degC)'],\n",
    "}\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "WINDOW_PARAMS = {\n",
    "    'input_width' :     72, # window input size\n",
    "    'label_width' :     12, # how many hours to predict to the future\n",
    "    'shift' :           13,\n",
    "    'label_columns' :   ['TD (degC)'],\n",
    "}\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "WINDOW_PARAMS = {\n",
    "    'input_width' :     72, # window input size\n",
    "    'label_width' :     12, # how many hours to predict to the future\n",
    "    'shift' :           25,\n",
    "    'label_columns' :   ['TD (degC)'],\n",
    "}\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "WINDOW_PARAMS = {\n",
    "    'input_width' :     72, # window input size\n",
    "    'label_width' :     24, # how many hours to predict to the future\n",
    "    'shift' :           37,\n",
    "    'label_columns' :   ['TD (degC)'],\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "TRAIN_PARAMS = {\n",
    "    'epochs' :          50,\n",
    "    'batch_size':       32,\n",
    "    'lr':               1e-5,                                   # 1e-3, 1e-4, 1e-5\n",
    "    'checkpoint_dir' :  PARAMS['checkpoints_path'],\n",
    "    'resume':           False,\n",
    "    'device':           PARAMS['device'],\n",
    "    'early_stopping_patience':10,                               # how many epochs to wait before stopping the training\n",
    "    'scheduler_patience':3,                                     # how many epochs to wait before reducing the learning rate\n",
    "    'scheduler_factor':  0.5,                                   # the factor to reduce the learning rate\n",
    "    'min_lr':            1e-7,\n",
    "    'logger_path':       PARAMS['output_path']\n",
    "}\n",
    "\n",
    "ADVANCED_MODEL_PARAMS = {\n",
    "    'num_stations':         len(PARAMS['fileNames']),\n",
    "    'time_steps':           WINDOW_PARAMS['input_width'],\n",
    "    'feature_dim':          PARAMS['in_channels'],\n",
    "    'kernel_size':          3,  # cnn filter size                       4, 5, 6, 7\n",
    "    'd_model':              64, # input for transformer size            64, 128\n",
    "    'nhead':                8,  # number of heads in the transformer    8, 16\n",
    "    'num_layers':           4,  # number of layers in the transformer - 6 - 12\n",
    "    'target_station_idx':   PARAMS['target_station_id'],\n",
    "    'label_width':          WINDOW_PARAMS['label_width'],\n",
    "    'output_per_feature':   3,                                          # 4 ,5\n",
    "    'use_batch_norm':       False,\n",
    "    'use_residual':         True\n",
    "}\n",
    "\n",
    "models_paths_dir_names_full_paths = [os.path.join(base_path, model_folder_name) for model_folder_name in models_paths_dir_names_for_inference]\n",
    "\n",
    "INFERENCE_PARAMS = {\n",
    "    'params_path':             [os.path.join(folder, 'parameters.py') for folder in models_paths_dir_names_full_paths],\n",
    "    'weights_paths':           [os.path.join(folder, 'checkpoints', 'best_checkpoint.pth') for folder in models_paths_dir_names_full_paths],\n",
    "    'scaler_folder_path':      PARAMS['scalers_path'],\n",
    "    'inference_output_path_per_model':  models_paths_dir_names_full_paths, # for saving the output of the inference in the model folder for each model\n",
    "    'inference_output_path':  os.path.join(base_path, 'inference_output'), # for saving the output of the inference of all models in one folder (later analyze.py will use it)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c723654a",
   "metadata": {},
   "source": [
    "### `inference.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad754547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference.py\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import importlib.util\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pytz\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_params(params_path):\n",
    "    # Convert path to absolute if it's relative\n",
    "    params_path = Path(params_path).resolve()\n",
    "\n",
    "    # Create a module name dynamically (avoid conflicts)\n",
    "    module_name = \"params_module\"\n",
    "\n",
    "    # Load the module\n",
    "    spec = importlib.util.spec_from_file_location(module_name, params_path)\n",
    "    params_module = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[module_name] = params_module\n",
    "    spec.loader.exec_module(params_module)\n",
    "\n",
    "    return params_module  # Now you can access its attributes\n",
    "\n",
    "\n",
    "def flatten_data(predictions, actuals):\n",
    "    flat_predictions = [temp for window in predictions for temp in window]\n",
    "    flat_actuals = [temp for window in actuals for temp in window]\n",
    "\n",
    "    data = pd.DataFrame({\n",
    "        'Predicted': flat_predictions,\n",
    "        'Actual': flat_actuals\n",
    "    })\n",
    "\n",
    "    data['Error'] = data['Predicted'] - data['Actual']\n",
    "    return data\n",
    "\n",
    "def generate_forecast_json(city_name, date_str, starting_hour, temperatures, output_file):\n",
    "    \"\"\"\n",
    "    Generates a JSON file containing forecast data.\n",
    "\n",
    "    Parameters:\n",
    "    - city_name (str): Name of the city.\n",
    "    - date_str (str): Starting date in \"YYYY-MM-DD\" format.\n",
    "    - starting_hour (int): Starting hour in 24-hour format (0-23).\n",
    "    - temperatures (np.ndarray): NumPy array of temperature readings.\n",
    "    - output_file (str): Path to the output JSON file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate inputs\n",
    "    if not isinstance(city_name, str):\n",
    "        raise TypeError(\"city_name must be a string.\")\n",
    "    \n",
    "    try:\n",
    "        current_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"date_str must be in 'YYYY-MM-DD' format.\")\n",
    "    \n",
    "    if not (0 <= starting_hour <= 23):\n",
    "        raise ValueError(\"starting_hour must be between 0 and 23.\")\n",
    "    \n",
    "    if not isinstance(temperatures, np.ndarray):\n",
    "        raise TypeError(\"temperatures must be a NumPy array.\")\n",
    "    \n",
    "    # Initialize forecast_data dictionary\n",
    "    forecast_data = {}\n",
    "    \n",
    "    current_hour = starting_hour\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        # Format the current date\n",
    "        date_key = current_date.strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "        # Initialize the date entry if it doesn't exist\n",
    "        if date_key not in forecast_data:\n",
    "            forecast_data[date_key] = {\"hourly\": {}}\n",
    "        \n",
    "        # Format the current time\n",
    "        time_str = \"{:02d}:00\".format(current_hour)\n",
    "        \n",
    "        # Assign the temperature, formatted to one decimal place\n",
    "        forecast_data[date_key][\"hourly\"][time_str] = {\n",
    "            \"temperature\": \"{:.1f}\".format(temp)\n",
    "        }\n",
    "        \n",
    "        # Increment the hour\n",
    "        current_hour += 1\n",
    "        \n",
    "        # If hour exceeds 23, reset to 0 and move to the next day\n",
    "        if current_hour > 23:\n",
    "            current_hour = 0\n",
    "            current_date += timedelta(days=1)\n",
    "    \n",
    "    # Construct the final JSON structure\n",
    "    data = {\n",
    "        \"data\": {\n",
    "            \"title\": city_name,\n",
    "            \"forecast_data\": forecast_data\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Write the data to a JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    \n",
    "    print(f\"Forecast data successfully written to {output_file}\")\n",
    "\n",
    "\n",
    "def load_scalers(scaler_dir='./output/scalers'):\n",
    "    \"\"\"\n",
    "    Load the previously saved scalers for each station.\n",
    "    Assumes scalers are saved as 'scaler_station0.pkl', 'scaler_station1.pkl', etc.\n",
    "    \"\"\"\n",
    "    scalers = []\n",
    "    num_stations = ADVANCED_MODEL_PARAMS['num_stations']\n",
    "    for i in range(num_stations):\n",
    "        scaler_path = os.path.join(scaler_dir, f'scaler_station_{i}.pkl')\n",
    "        if not os.path.exists(scaler_path):\n",
    "            raise FileNotFoundError(f\"Scaler file not found at {scaler_path}\")\n",
    "        with open(scaler_path, 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "            scalers.append(scaler)\n",
    "        print(f\"Scaler for Station {i} loaded from {scaler_path}\")\n",
    "    return scalers\n",
    "\n",
    "\n",
    "def load_model_for_inference(checkpoint_path, model_params, device='cpu'):\n",
    "    \"\"\"\n",
    "    Create a model with the same architecture,\n",
    "    load checkpoint, and return it in eval mode.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint file not found at {checkpoint_path}\")\n",
    "\n",
    "    model = TargetedWeatherPredictionModel(**model_params)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {checkpoint_path}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_window_multi_station_return_only_input_window(data_np, window_size, scalers, idx=0):\n",
    "    total_window_size = window_size\n",
    "    if idx + total_window_size > len(data_np):\n",
    "        raise ValueError(f\"Index {idx} with window size {total_window_size} exceeds data length {len(data_np)}.\")\n",
    "\n",
    "    # Extract input window: shape (window_size, num_stations, num_features)\n",
    "    window = data_np[idx:idx + window_size, :, :]  # (window_size, num_stations, num_features)\n",
    "\n",
    "    # Apply individual scalers to each station's data\n",
    "    scaled_window = []\n",
    "    num_stations = window.shape[1]\n",
    "    for station_idx in range(num_stations):\n",
    "        station_data = window[:, station_idx, :]  # (window_size, num_features)\n",
    "        scaler = scalers[station_idx]\n",
    "        station_data_scaled = scaler.transform(station_data)  # (window_size, num_features)\n",
    "        scaled_window.append(station_data_scaled)\n",
    "\n",
    "    # Stack scaled data: shape (window_size, num_stations, num_features)\n",
    "    scaled_window = np.stack(scaled_window, axis=1)\n",
    "\n",
    "    # Convert to torch.Tensor and reshape to (1, num_stations, time_steps, feature_dim)\n",
    "    window_tensor = torch.tensor(scaled_window, dtype=torch.float32).unsqueeze(0)\n",
    "    # batch_size, num_stations, time_steps, feature_dim = x.size()\n",
    "\n",
    "    window_tensor = window_tensor.permute(0, 2, 1, 3)\n",
    "\n",
    "    return window_tensor\n",
    "\n",
    "\n",
    "def load_window_multi_station(data_np, window_size, shift, label_width, scalers, target_column_index, idx=0):\n",
    "   \n",
    "    total_window_size = window_size + shift - 1 + label_width\n",
    "    if idx + total_window_size > len(data_np):\n",
    "        raise ValueError(f\"Index {idx} with window size {total_window_size} exceeds data length {len(data_np)}.\")\n",
    "\n",
    "    # Extract input window: shape (window_size, num_stations, num_features)\n",
    "    window = data_np[idx:idx + window_size, :, :]  # (window_size, num_stations, num_features)\n",
    "\n",
    "    # Extract target: shape (label_width, )\n",
    "    target_start = idx + window_size + shift - 1\n",
    "    target_end = target_start + label_width\n",
    "    actual_target = data_np[target_start:target_end, ADVANCED_MODEL_PARAMS['target_station_idx'], target_column_index]\n",
    "    # actual_target_mean = actual_target.mean()  # Aggregate if label_width >1\n",
    "\n",
    "    # Apply individual scalers to each station's data\n",
    "    scaled_window = []\n",
    "    num_stations = window.shape[1]\n",
    "    for station_idx in range(num_stations):\n",
    "        station_data = window[:, station_idx, :]  # (window_size, num_features)\n",
    "        scaler = scalers[station_idx]\n",
    "        station_data_scaled = scaler.transform(station_data)  # (window_size, num_features)\n",
    "        scaled_window.append(station_data_scaled)\n",
    "    \n",
    "    # Stack scaled data: shape (window_size, num_stations, num_features)\n",
    "    scaled_window = np.stack(scaled_window, axis=1)\n",
    "\n",
    "    # Convert to torch.Tensor and reshape to (1, num_stations, time_steps, feature_dim)\n",
    "    window_tensor = torch.tensor(scaled_window, dtype=torch.float32).unsqueeze(0)\n",
    "    # batch_size, num_stations, time_steps, feature_dim = x.size()\n",
    "\n",
    "    window_tensor = window_tensor.permute(0, 2, 1, 3)\n",
    "\n",
    "    return window_tensor, actual_target\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, input_window, lat, lon, device='cpu'):\n",
    "    \"\"\"\n",
    "    Perform prediction using the multi-station model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Trained TargetedWeatherPredictionModel.\n",
    "        input_window (torch.Tensor): Input data of shape (1, num_stations, time_steps, feature_dim).\n",
    "        lat (torch.Tensor): Normalized latitude coordinates of shape (num_stations, 1).\n",
    "        lon (torch.Tensor): Normalized longitude coordinates of shape (num_stations, 1).\n",
    "        device (str): Device to perform inference on.\n",
    "\n",
    "    Returns:\n",
    "        float: Predicted target value in original scale (e.g., Temperature in °C).\n",
    "    \"\"\"\n",
    "    input_tensor = input_window.to(device)\n",
    "    lat = lat.to(device)\n",
    "    lon = lon.to(device)\n",
    "\n",
    "    # Model prediction (scaled)\n",
    "    output_scaled = model(input_tensor, lat, lon)\n",
    "\n",
    "    # Convert to numpy\n",
    "    output_scaled_np = output_scaled.squeeze(-1).cpu().numpy().reshape(-1, 1)  # Scalar\n",
    "    return output_scaled_np\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    1. define INFERENCE_PARAMS in your file - all the INFERENCE_PARAMS are mandatory in addition to that all the parameters that are in section 2\n",
    "\n",
    "    2. there is an assumption that yours parameters file (not what define in the infarance parameters nor what in the folders)\n",
    "        has the same values in these parameters:\n",
    "        PARAMS['paths_in_colab'] - list of the stations names, if you have different names it wont make sense \n",
    "        PARAMS['target_station_id'] - the index of the target station in the list of stations\n",
    "        ADVANCED_MODEL_PARAMS['target_station_idx'] - the index of the target station in the list of stations\n",
    "        WINDOW_PARAMS['label_columns'] - the label column\n",
    "        PARAMS['device']\n",
    "    \"\"\"\n",
    "    inference_mode = 'live'  # Options: 'live', 'analyze'\n",
    "    analyze_stop_at = 0  # Number of predictions to analyze\n",
    "    local_tz = pytz.timezone('Asia/Jerusalem')\n",
    "    delta = timedelta(hours=0)\n",
    "    start_time_to_predict =  datetime.now(local_tz) - delta\n",
    "    print(f\"start_time_to_predict: {start_time_to_predict}\")\n",
    "    print(f\"delta: {delta}\")\n",
    "    verbos_get_prccessed_latest_data_by_hour_and_station = False\n",
    "\n",
    "\n",
    "    parameters_files = [] # load parameters files\n",
    "    for path in INFERENCE_PARAMS['params_path']:\n",
    "        parameters_files.append(load_params(path))\n",
    "\n",
    "    east = []\n",
    "    north = []\n",
    "    filenames = PARAMS['paths_in_colab'] \n",
    "    for filename in filenames: # get the coordinates of the stations\n",
    "        east.append(STATIONS_COORDINATES_COLAB[filename][0])\n",
    "        north.append(STATIONS_COORDINATES_COLAB[filename][1])\n",
    "\n",
    "    east = np.array(east)\n",
    "    north = np.array(north)\n",
    "    east_normalized, north_normalized = normalize_coordinates(east, north)\n",
    "    \n",
    "    scalers = load_scalers(scaler_dir=INFERENCE_PARAMS['scaler_folder_path'])\n",
    "    # model_params = ADVANCED_MODEL_PARAMS.copy()\n",
    "\n",
    "    \n",
    "    model_params = []\n",
    "    for params_file in parameters_files:\n",
    "        model_params.append(params_file.ADVANCED_MODEL_PARAMS)\n",
    "\n",
    "    models = []\n",
    "    for i, weights_path in enumerate(INFERENCE_PARAMS['weights_paths']):\n",
    "        model = load_model_for_inference(weights_path, model_params[i], device=PARAMS['device'])\n",
    "        models.append(model)\n",
    "\n",
    "    window_params = []\n",
    "    for params_file in parameters_files:\n",
    "        window_params.append(params_file.WINDOW_PARAMS)\n",
    "    \n",
    "    max_input_width = max([window_param['input_width'] for window_param in window_params])\n",
    "\n",
    "    device = PARAMS['device']\n",
    "    target_station_idx = PARAMS['target_station_id']\n",
    "\n",
    "    if inference_mode == 'live':        \n",
    "        dataframes, last_hour, last_date, success = get_prccessed_latest_data_by_hour_and_station(STATIONS_LIST, max_input_width)\n",
    "        last_hour = int(last_hour.split(':')[0])\n",
    "        if verbos_get_prccessed_latest_data_by_hour_and_station:\n",
    "            print(f\"len of df: {len(dataframes)}\")\n",
    "            print(f\"len of df[0]: {len(dataframes[list(dataframes.keys())[0]])}\")\n",
    "            print(f\"len of df[1]: {len(dataframes[list(dataframes.keys())[1]])}\")\n",
    "            print(f\"Last hour: {last_hour}\")\n",
    "            print(f\"Last hour date: {last_date}\")\n",
    "            print(f\"success: {success}\")\n",
    "        # datafreames is a dictionary with the station name as key and the dataframe as value - convering it into a list of dataframes\n",
    "        dataframes_list = [dataframes[station] for station in STATIONS_LIST]\n",
    "        \n",
    "        representative_df = dataframes_list[0]\n",
    "        column_indices = {name: i for i, name in enumerate(representative_df.columns)}\n",
    "        label_columns = [column_indices[WINDOW_PARAMS['label_columns'][0]]]\n",
    "        target_col_index = label_columns[0]\n",
    "\n",
    "        list_of_values = [df.values for df in dataframes_list]\n",
    "        combined_window = np.stack(list_of_values, axis=1) \n",
    "    \n",
    "        target_scaler = scalers[ADVANCED_MODEL_PARAMS['target_station_idx']]\n",
    "        predictions_of_models = []\n",
    "        for i, model in enumerate(models):\n",
    "            input_width = parameters_files[i].WINDOW_PARAMS['input_width']\n",
    "            input_window = load_window_multi_station_return_only_input_window(\n",
    "                data_np=    combined_window[-input_width:],\n",
    "                window_size=input_width,\n",
    "                scalers=    scalers\n",
    "            )\n",
    "            y_pred_scaled = predict(model, input_window, east_normalized, north_normalized,  device=device)\n",
    "            dummy = np.zeros((y_pred_scaled.shape[0], target_scaler.mean_.shape[0]))\n",
    "            dummy[:, target_col_index] = y_pred_scaled[:, 0]\n",
    "            y_pred_original = target_scaler.inverse_transform(dummy)[:, target_col_index]\n",
    "            predictions_of_models.append(y_pred_original)\n",
    "\n",
    "        generate_forecast_json(PARAMS['target_station_desplay_name'], last_date, last_hour+1, np.concatenate(predictions_of_models), \"forecast.json\")\n",
    "\n",
    "    elif inference_mode == 'analyze':\n",
    "        dfs = []\n",
    "        for filename in filenames:\n",
    "            df = pd.read_pickle(filename)\n",
    "            dfs.append(df)\n",
    "\n",
    "        print(\"Original size of data:\")\n",
    "        for i, df in enumerate(dfs):\n",
    "            print(f\"Station {i}: {df.shape}\")\n",
    "\n",
    "        list_of_values = [df.values for df in dfs]\n",
    "\n",
    "        # Train/Validation Split per Station\n",
    "        train_size = int(0.8 * len(list_of_values[0]))\n",
    "        list_of_train_data = []\n",
    "        list_of_val_data = []\n",
    "        for values in list_of_values:\n",
    "            train_data = values[:train_size]\n",
    "            val_data = values[train_size:]\n",
    "            list_of_train_data.append(train_data)\n",
    "            list_of_val_data.append(val_data)\n",
    "\n",
    "        # Combine Data into 3D Arrays\n",
    "        combined_train_data = np.stack(list_of_train_data, axis=1)  # (T_train, num_stations, num_features)\n",
    "        combined_val_data = np.stack(list_of_val_data, axis=1)  # (T_val, num_stations, num_features)\n",
    "\n",
    "        # Ensure consistent column indexing\n",
    "        representative_df = dfs[0]\n",
    "        column_indices = {name: i for i, name in enumerate(representative_df.columns)}\n",
    "        label_columns = [column_indices[WINDOW_PARAMS['label_columns'][0]]]\n",
    "\n",
    "        # Define target station index\n",
    "        target_col_index = label_columns[0]\n",
    "\n",
    "        for i, model in enumerate(models):\n",
    "            input_width = parameters_files[i].WINDOW_PARAMS['input_width']\n",
    "            shift = parameters_files[i].WINDOW_PARAMS['shift']\n",
    "            label_width = parameters_files[i].WINDOW_PARAMS['label_width']\n",
    "\n",
    "            total_window_size = input_width + shift - 1 + label_width\n",
    "            end = len(combined_val_data) - total_window_size if analyze_stop_at == 0 else min(analyze_stop_at, len(combined_val_data) - total_window_size)\n",
    "            \n",
    "            predictions = []\n",
    "            actual_temps = []\n",
    "            for j in tqdm(range(0, end), desc=\"Predicting\"):\n",
    "                try:\n",
    "                    input_window, actual_temp = load_window_multi_station(\n",
    "                        data_np=combined_val_data,\n",
    "                        window_size=    input_width,\n",
    "                        shift=          shift,\n",
    "                        label_width=    label_width,\n",
    "                        scalers=        scalers,\n",
    "                        target_column_index=target_col_index,\n",
    "                        idx=    j\n",
    "                    )\n",
    "                    y_pred_scaled = predict(model, input_window, east_normalized, north_normalized,  device=device)\n",
    "                    # Inverse transform\n",
    "                    target_scaler = scalers[ADVANCED_MODEL_PARAMS['target_station_idx']]\n",
    "                    dummy = np.zeros((y_pred_scaled.shape[0], target_scaler.mean_.shape[0]))\n",
    "                    dummy[:, target_col_index] = y_pred_scaled[:, 0]\n",
    "                    y_pred_original = target_scaler.inverse_transform(dummy)[:, target_col_index]\n",
    "\n",
    "                    if len(y_pred_original) != len(actual_temp):\n",
    "                        continue\n",
    "                    predictions.append(y_pred_original)\n",
    "                    actual_temps.append(actual_temp)\n",
    "                except ValueError as ve:\n",
    "                    print(f\"Skipping index {j}: {ve}\")\n",
    "                    continue\n",
    "            \n",
    "            output_dir_for_all = os.path.join(os.path.dirname(__file__), INFERENCE_PARAMS['inference_output_path'])\n",
    "            os.makedirs(output_dir_for_all, exist_ok=True)\n",
    "            output_dir_per_folder = os.path.join(os.path.dirname(__file__), INFERENCE_PARAMS['inference_output_path_per_model'][i])\n",
    "            os.makedirs(output_dir_per_folder, exist_ok=True)\n",
    "            predictions_actuals_df = flatten_data(predictions, actual_temps)\n",
    "            predictions_actuals_df['input_width'] = input_width\n",
    "            predictions_actuals_df['label_width'] = label_width\n",
    "            predictions_actuals_df.to_csv(os.path.join(output_dir_per_folder, f'{i}_predictions_{i}.csv'), index=False)\n",
    "            predictions_actuals_df.to_csv(os.path.join(output_dir_for_all, f'{i}_predictions_{i}.csv'), index=False)\n",
    "    else:\n",
    "        print(f\"Invalid inference_mode : {inference_mode}.\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
