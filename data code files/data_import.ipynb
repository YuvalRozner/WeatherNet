{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pips and includes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEGINING_OF_YEAR = \"01010000\"\n",
    "ENDING_OF_YEAR = \"12312350\"\n",
    "START_YEAR = 2000\n",
    "END_YEAR = 2023\n",
    "\n",
    "DATA_DIRECTORY = \"../data/\"\n",
    "\n",
    "columns = [\n",
    "    \"Date Time\", \"BP (hPa)\", \"DiffR (w/m^2)\", \"Grad (w/m^2)\", \"NIP (w/m^2)\", \"RH (%)\",\n",
    "    \"TD (degC)\", \"TDmax (degC)\", \"TDmin (degC)\", \"WD (deg)\", \"WDmax (deg)\",\n",
    "    \"WS (m/s)\", \"Ws1mm (m/s)\", \"Ws10mm (m/s)\", \"WSmax (m/s)\", \"STDwd (deg)\"\n",
    "]\n",
    "\n",
    "column_pairs = [\n",
    "    (\"date\", \"Date Time\"),\n",
    "    (\"BP\", \"BP (hPa)\"),\n",
    "    (\"DiffR\", \"DiffR (w/m^2)\"),\n",
    "    (\"Grad\", \"Grad (w/m^2)\"),\n",
    "    (\"NIP\", \"NIP (w/m^2)\"),\n",
    "    (\"RH\", \"RH (%)\"),\n",
    "    (\"TD\", \"TD (degC)\"),\n",
    "    (\"TDmax\", \"TDmax (degC)\"),\n",
    "    (\"TDmin\", \"TDmin (degC)\"),\n",
    "    (\"WD\", \"WD (deg)\"),\n",
    "    (\"WDmax\", \"WDmax (deg)\"),\n",
    "    (\"WS\", \"WS (m/s)\"),\n",
    "    (\"WS1mm\", \"Ws1mm (m/s)\"),\n",
    "    (\"Ws10mm\", \"Ws10mm (m/s)\"),\n",
    "    (\"WSmax\", \"WSmax (m/s)\"),\n",
    "    (\"STDwd\", \"STDwd (deg)\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utills functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_weather_data(station_id, start_date, end_date):\n",
    "    url = f\"https://ims.gov.il/he/envista_station_all_data_time_range/{station_id}/BP%26DiffR%26Grad%26NIP%26RH%26TD%26TDmax%26TDmin%26TW%26WD%26WDmax%26WS%26WS1mm%26Ws10mm%26Ws10maxEnd%26WSmax%26STDwd%26Rain/{start_date}/{end_date}/1/S\"\n",
    "    response = requests.get(url)\n",
    "    data = json.loads(response.content)\n",
    "    return data\n",
    "\n",
    "def remove_unwanted_keys(data):\n",
    "    # Remove 'sid', 'sname', and 'date_for_sort' from each record in data\n",
    "    for record in data['data']['records']:\n",
    "        if 'date_for_sort' in record:\n",
    "            del record['date_for_sort']\n",
    "        if 'sid' in record:\n",
    "            del record['sid']\n",
    "        if 'TW' in record:\n",
    "            del record['TW']\n",
    "        if 'sname' in record:\n",
    "            del record['sname']\n",
    "\n",
    "def replace_column_names(data):\n",
    "    # Replace the names of the columns by the pairs in column_pairs\n",
    "    for record in data['data']['records']:\n",
    "        for new_name, old_name in column_pairs:\n",
    "            if new_name in record:\n",
    "                record[old_name] = record.pop(new_name)\n",
    "\n",
    "def process_data(data):\n",
    "    remove_unwanted_keys(data)\n",
    "    replace_column_names(data)\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    import csv\n",
    "\n",
    "    # Extract the column names from the first record\n",
    "    column_names = data['data']['records'][0].keys()\n",
    "\n",
    "    # Open the file in write mode\n",
    "    with open(DATA_DIRECTORY+filename, mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=column_names)\n",
    "\n",
    "        # Write the header\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Write the data\n",
    "        for record in data['data']['records']:\n",
    "            writer.writerow(record)\n",
    "\n",
    "def fetch_data_for_station(station_id, start_year, end_year):\n",
    "    all_data = []\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        today_fore0 = f\"{year}\" + BEGINING_OF_YEAR\n",
    "        today_fore23 = f\"{year}\" + ENDING_OF_YEAR\n",
    "\n",
    "        data = fetch_weather_data(station_id, today_fore0, today_fore23)\n",
    "        process_data(data)\n",
    "        \n",
    "        # Convert the data to a DataFrame and append to the list\n",
    "        df = pd.DataFrame(data['data']['records'])\n",
    "        all_data.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "def get_and_save_station_data(station_name, station_id, start_year, end_year):\n",
    "    # Get all data for the station\n",
    "    combined_df = fetch_data_for_station(station_id, start_year, end_year)\n",
    "    \n",
    "    # Convert the DataFrame back to the dictionary format expected by process_data\n",
    "    data = {'data': {'records': combined_df.to_dict(orient='records')}}\n",
    "    \n",
    "    # Process the data\n",
    "    process_data(data)\n",
    "    \n",
    "    # Save the data to a CSV file\n",
    "    filename = f\"{station_name}_data_{start_year}_{end_year}.csv\"\n",
    "    save_to_csv(data, filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_data_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSTART_YEAR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEND_YEAR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(filename):\n\u001b[1;32m---> 11\u001b[0m     \u001b[43mget_and_save_station_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstation_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSTART_YEAR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEND_YEAR\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 67\u001b[0m, in \u001b[0;36mget_and_save_station_data\u001b[1;34m(station_name, station_id, start_year, end_year)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_and_save_station_data\u001b[39m(station_name, station_id, start_year, end_year):\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Get all data for the station\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m     combined_df \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_data_for_station\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstation_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_year\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_year\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# Convert the DataFrame back to the dictionary format expected by process_data\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m: combined_df\u001b[38;5;241m.\u001b[39mto_dict(orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m)}}\n",
      "Cell \u001b[1;32mIn[3], line 54\u001b[0m, in \u001b[0;36mfetch_data_for_station\u001b[1;34m(station_id, start_year, end_year)\u001b[0m\n\u001b[0;32m     51\u001b[0m today_fore0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m BEGINING_OF_YEAR\n\u001b[0;32m     52\u001b[0m today_fore23 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m ENDING_OF_YEAR\n\u001b[1;32m---> 54\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_weather_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstation_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoday_fore0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoday_fore23\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m process_data(data)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Convert the data to a DataFrame and append to the list\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m, in \u001b[0;36mfetch_weather_data\u001b[1;34m(station_id, start_date, end_date)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch_weather_data\u001b[39m(station_id, start_date, end_date):\n\u001b[0;32m      2\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://ims.gov.il/he/envista_station_all_data_time_range/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstation_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/BP%26DiffR%26Grad%26NIP%26RH%26TD%26TDmax%26TDmin%26TW%26WD%26WDmax%26WS%26WS1mm%26Ws10mm%26Ws10maxEnd%26WSmax%26STDwd%26Rain/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/1/S\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\gr062\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gr062\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gr062\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\gr062\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:747\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    744\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 747\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32mc:\\Users\\gr062\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 899\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gr062\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\gr062\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\response.py:931\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m--> 931\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\gr062\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\response.py:1074\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1073\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1074\u001b[0m chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1075\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[0;32m   1076\u001b[0m     chunk, decode_content\u001b[38;5;241m=\u001b[39mdecode_content, flush_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1077\u001b[0m )\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoded:\n",
      "File \u001b[1;32mc:\\Users\\gr062\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\response.py:1016\u001b[0m, in \u001b[0;36mHTTPResponse._handle_chunk\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left:\n\u001b[1;32m-> 1016\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_safe_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m-\u001b[39m amt\n\u001b[0;32m   1018\u001b[0m     returned_chunk \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[1;32mc:\\Users\\gr062\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:630\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_safe_read\u001b[39m(\u001b[38;5;28mself\u001b[39m, amt):\n\u001b[0;32m    624\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read the number of bytes requested.\u001b[39;00m\n\u001b[0;32m    625\u001b[0m \n\u001b[0;32m    626\u001b[0m \u001b[38;5;124;03m    This function should be used when <amt> bytes \"should\" be present for\u001b[39;00m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;124;03m    reading. If the bytes are truly not available (due to EOF), then the\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;124;03m    IncompleteRead exception can be used to detect the problem.\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m<\u001b[39m amt:\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(data))\n",
      "File \u001b[1;32mc:\\Users\\gr062\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gr062\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\gr062\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load stations_ids.json\n",
    "with open('stations_ids.json', 'r', encoding='utf-8') as f:\n",
    "    stations_ids = json.load(f)\n",
    "\n",
    "# Iterate over all stations and use the function get_and_save_station_data\n",
    "for station_name, station_id in stations_ids.items():\n",
    "    filename = f\"{station_name}_data_{START_YEAR}_{END_YEAR}.csv\"\n",
    "    if not os.path.exists(filename):\n",
    "        get_and_save_station_data(station_name, station_id, START_YEAR, END_YEAR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting out the stations ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Kefar Giladi': '241', 'Dafna': '499', 'Kefar Blum': '202', 'Merom Golan Picman': '10', 'Rosh Haniqra': '106', 'Elon': '73', 'Ayyelet Hashahar': '353', 'Shave Ziyyon': '343', 'Zefat Har Kenaan': '62', 'Harashim': '269', 'Ammiad': '123', 'Gamla': '227', 'Eshhar': '205', 'Kefar Nahum': '233', 'Bet Zayda': '6', 'Deir Hanna': '99', 'Afeq': '78', 'Avne Etan': '2', 'Haifa Refineries': '41', 'Haifa Technion': '43', 'Tiberias': '502', 'Haifa University': '42', 'Nazareth': '666', 'Newe Yaar': '186', 'Tavor Kadoorie': '13', 'Zemah': '8', 'Yavneel': '11', 'Massada': '355', 'En Karmel': '44', 'En Hashofet': '67', 'Afula Nir Haemeq': '16', 'Zikhron Yaaqov': '45', 'Tel Yosef': '380', 'Maale Gilboa': '224', 'Hadera Port': '46', 'Eden Farm': '206', 'Sede Eliyyahu': '366', 'En Hahoresh': '107', 'Qarne Shomeron': '20', 'Itamar': '90', 'Hakfar Hayarok': '275', 'Ariel': '21', 'Tel Aviv Coast': '178', 'Bet Dagan': '54', 'Gilgal': '30', 'Har Harasha': '24', 'Ashdod Port': '124', 'Nahshon': '259', 'Qevuzat Yavne': '74', 'Bet Haarava': '228', 'Hafez Hayyim': '121', 'Zova': '188', 'Jerusalem Centre': '23', 'Maale Adummim': '218', 'Jerusalem Givat Ram': '22', 'Nizzan': '274', 'Beit Jimal': '75', 'Netiv Halamed He': '25', 'Rosh Zurim': '77', 'Negba': '82', 'Ashqelon Port': '208', 'Gat': '236', 'Metzoke Dragot': '210', 'Dorot': '79', 'Lahav': '350', 'Shani': '28', 'Besor Farm': '58', 'Beer Sheva University': '411', 'Arad': '29', 'Nevatim': '349', 'Zomet Hanegev': '112', 'Sedom': '65', 'Ashalim': '381', 'Sede Boqer': '98', 'Ezuz': '338', 'Avdat': '271', 'Hazeva': '33', 'Mizpe Ramon': '379', 'Paran': '207', 'Neot Smadar': '232', 'Yotvata': '36', 'Elat': '64'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the station map\n",
    "station_map = {}\n",
    "\n",
    "# Load stations.json\n",
    "with open('stations.json', 'r', encoding='utf-8') as f:\n",
    "    stations_data = json.load(f)\n",
    "\n",
    "# Iterate through all categories and areas to populate station_map\n",
    "for category, areas in stations_data['data']['area_stations'].items():\n",
    "    for area, stations in areas.items():\n",
    "        for station in stations.values():\n",
    "            station_name = station.get('name')\n",
    "            sid = station.get('envista_id')\n",
    "            if station_name and sid:\n",
    "                station_map[station_name] = sid\n",
    "\n",
    "print(station_map)\n",
    "\n",
    "with open('stations_ids.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(station_map, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calc the missing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_missing_percentage(csv_path):\n",
    "    df = pd.read_csv(csv_path, na_values=['None', 'null', '-', '', ' ', 'NaN', 'nan', 'NAN'], low_memory=False)\n",
    "    missing_percentages = df.isnull().mean() * 100\n",
    "    return missing_percentages\n",
    "\n",
    "missing_data = {}\n",
    "\n",
    "for filename in os.listdir(DATA_DIRECTORY):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(DATA_DIRECTORY, filename)\n",
    "        missing_data[filename] = calculate_missing_percentage(file_path)\n",
    "\n",
    "missing_data_df = pd.DataFrame(missing_data).transpose()\n",
    "# Replace NaN values with 100%\n",
    "missing_data_df.fillna(100, inplace=True)\n",
    "# Add a column to missing_data_df with the average percentage for each station\n",
    "missing_data_df['Average Percentage'] = missing_data_df.mean(axis=1)\n",
    "# Calculate the average percentage for each row excluding specific columns\n",
    "columns_to_exclude = ['BP (hPa)', 'Date Time', 'DiffR (w/m^2)', 'Grad (w/m^2)', 'NIP']\n",
    "columns_to_include = [col for col in missing_data_df.columns if col not in columns_to_exclude]\n",
    "\n",
    "# sort the dataframe by the average percentage excluding specific columns\n",
    "missing_data_df = missing_data_df.sort_values(by='Average Percentage Excluding Specific Columns')\n",
    "\n",
    "missing_data_df['Average Percentage Excluding Specific Columns'] = missing_data_df[columns_to_include].mean(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "missing_data_df = missing_data_df.round(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BP (hPa)</th>\n",
       "      <th>Date Time</th>\n",
       "      <th>DiffR (w/m^2)</th>\n",
       "      <th>Grad (w/m^2)</th>\n",
       "      <th>NIP (w/m^2)</th>\n",
       "      <th>RH (%)</th>\n",
       "      <th>Rain</th>\n",
       "      <th>STDwd (deg)</th>\n",
       "      <th>TD (degC)</th>\n",
       "      <th>TDmax (degC)</th>\n",
       "      <th>TDmin (degC)</th>\n",
       "      <th>Time</th>\n",
       "      <th>WD (deg)</th>\n",
       "      <th>WDmax (deg)</th>\n",
       "      <th>WS (m/s)</th>\n",
       "      <th>WSmax (m/s)</th>\n",
       "      <th>Ws10mm (m/s)</th>\n",
       "      <th>Ws1mm (m/s)</th>\n",
       "      <th>Average Percentage</th>\n",
       "      <th>Average Percentage Excluding Specific Columns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Afeq_data_2000_2023.csv</th>\n",
       "      <td>37.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>12.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>13.1</td>\n",
       "      <td>12.2</td>\n",
       "      <td>12.2</td>\n",
       "      <td>11.6</td>\n",
       "      <td>11.6</td>\n",
       "      <td>20.9</td>\n",
       "      <td>11.6</td>\n",
       "      <td>25.8</td>\n",
       "      <td>16.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ammiad_data_2000_2023.csv</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>8.0</td>\n",
       "      <td>45.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>45.1</td>\n",
       "      <td>45.2</td>\n",
       "      <td>45.2</td>\n",
       "      <td>45.3</td>\n",
       "      <td>45.3</td>\n",
       "      <td>45.2</td>\n",
       "      <td>45.2</td>\n",
       "      <td>43.1</td>\n",
       "      <td>34.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avne Etan_data_2000_2023.csv</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>23.1</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ayyelet Hashahar_data_2000_2023.csv</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>66.8</td>\n",
       "      <td>64.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bet Zayda_data_2000_2023.csv</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>4.4</td>\n",
       "      <td>4.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>6.1</td>\n",
       "      <td>6.1</td>\n",
       "      <td>24.3</td>\n",
       "      <td>10.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dafna_data_2000_2023.csv</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.3</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Deir Hanna_data_2000_2023.csv</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>35.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.9</td>\n",
       "      <td>35.9</td>\n",
       "      <td>35.9</td>\n",
       "      <td>35.9</td>\n",
       "      <td>36.2</td>\n",
       "      <td>36.6</td>\n",
       "      <td>36.6</td>\n",
       "      <td>36.6</td>\n",
       "      <td>38.8</td>\n",
       "      <td>29.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Elon_data_2000_2023.csv</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>23.5</td>\n",
       "      <td>9.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Eshhar_data_2000_2023.csv</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>22.4</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gamla_data_2000_2023.csv</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>74.7</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>21.7</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Haifa Refineries_data_2000_2023.csv</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>67.2</td>\n",
       "      <td>65.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Haifa Technion_data_2000_2023.csv</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>2.7</td>\n",
       "      <td>23.6</td>\n",
       "      <td>12.3</td>\n",
       "      <td>29.9</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>15.3</td>\n",
       "      <td>25.5</td>\n",
       "      <td>25.6</td>\n",
       "      <td>25.5</td>\n",
       "      <td>25.8</td>\n",
       "      <td>25.5</td>\n",
       "      <td>26.6</td>\n",
       "      <td>20.6</td>\n",
       "      <td>18.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Harashim_data_2000_2023.csv</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.9</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>15.6</td>\n",
       "      <td>16.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>15.6</td>\n",
       "      <td>15.6</td>\n",
       "      <td>15.6</td>\n",
       "      <td>15.6</td>\n",
       "      <td>29.5</td>\n",
       "      <td>17.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kefar Blum_data_2000_2023.csv</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>22.3</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kefar Giladi_data_2000_2023.csv</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>23.0</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kefar Nahum_data_2000_2023.csv</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.1</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Merom Golan Picman_data_2000_2023.csv</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>17.8</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rosh Haniqra_data_2000_2023.csv</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>20.3</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>25.6</td>\n",
       "      <td>12.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shave Ziyyon_data_2000_2023.csv</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>22.6</td>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zefat Har Kenaan_data_2000_2023.csv</th>\n",
       "      <td>28.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>11.4</td>\n",
       "      <td>11.2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.7</td>\n",
       "      <td>13.1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.7</td>\n",
       "      <td>13.4</td>\n",
       "      <td>13.4</td>\n",
       "      <td>24.9</td>\n",
       "      <td>16.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       BP (hPa)  Date Time  DiffR (w/m^2)  \\\n",
       "Afeq_data_2000_2023.csv                    37.8        0.0          100.0   \n",
       "Ammiad_data_2000_2023.csv                 100.0        0.0          100.0   \n",
       "Avne Etan_data_2000_2023.csv              100.0        0.0          100.0   \n",
       "Ayyelet Hashahar_data_2000_2023.csv       100.0        0.0          100.0   \n",
       "Bet Zayda_data_2000_2023.csv              100.0        0.0          100.0   \n",
       "Dafna_data_2000_2023.csv                  100.0        0.0          100.0   \n",
       "Deir Hanna_data_2000_2023.csv             100.0        0.0          100.0   \n",
       "Elon_data_2000_2023.csv                   100.0        0.0          100.0   \n",
       "Eshhar_data_2000_2023.csv                 100.0        0.0          100.0   \n",
       "Gamla_data_2000_2023.csv                  100.0        0.0          100.0   \n",
       "Haifa Refineries_data_2000_2023.csv       100.0        0.0          100.0   \n",
       "Haifa Technion_data_2000_2023.csv         100.0        0.0            6.2   \n",
       "Harashim_data_2000_2023.csv               100.0        0.0          100.0   \n",
       "Kefar Blum_data_2000_2023.csv             100.0        0.0          100.0   \n",
       "Kefar Giladi_data_2000_2023.csv           100.0        0.0          100.0   \n",
       "Kefar Nahum_data_2000_2023.csv            100.0        0.0          100.0   \n",
       "Merom Golan Picman_data_2000_2023.csv     100.0        0.0          100.0   \n",
       "Rosh Haniqra_data_2000_2023.csv           100.0        0.0          100.0   \n",
       "Shave Ziyyon_data_2000_2023.csv           100.0        0.0          100.0   \n",
       "Zefat Har Kenaan_data_2000_2023.csv        28.5        0.0          100.0   \n",
       "\n",
       "                                       Grad (w/m^2)  NIP (w/m^2)  RH (%)  \\\n",
       "Afeq_data_2000_2023.csv                       100.0        100.0    11.0   \n",
       "Ammiad_data_2000_2023.csv                     100.0        100.0     1.3   \n",
       "Avne Etan_data_2000_2023.csv                  100.0        100.0     3.0   \n",
       "Ayyelet Hashahar_data_2000_2023.csv           100.0        100.0     0.4   \n",
       "Bet Zayda_data_2000_2023.csv                  100.0        100.0     5.6   \n",
       "Dafna_data_2000_2023.csv                      100.0        100.0     0.1   \n",
       "Deir Hanna_data_2000_2023.csv                 100.0        100.0     2.0   \n",
       "Elon_data_2000_2023.csv                       100.0        100.0    10.1   \n",
       "Eshhar_data_2000_2023.csv                     100.0        100.0     0.5   \n",
       "Gamla_data_2000_2023.csv                       74.7        100.0     1.0   \n",
       "Haifa Refineries_data_2000_2023.csv           100.0        100.0     6.7   \n",
       "Haifa Technion_data_2000_2023.csv               2.7         23.6    12.3   \n",
       "Harashim_data_2000_2023.csv                   100.0        100.0     3.7   \n",
       "Kefar Blum_data_2000_2023.csv                 100.0        100.0     1.0   \n",
       "Kefar Giladi_data_2000_2023.csv               100.0        100.0     0.4   \n",
       "Kefar Nahum_data_2000_2023.csv                100.0        100.0     1.7   \n",
       "Merom Golan Picman_data_2000_2023.csv           4.4        100.0     6.5   \n",
       "Rosh Haniqra_data_2000_2023.csv               100.0        100.0     9.2   \n",
       "Shave Ziyyon_data_2000_2023.csv               100.0        100.0     2.7   \n",
       "Zefat Har Kenaan_data_2000_2023.csv           100.0        100.0    11.4   \n",
       "\n",
       "                                       Rain  STDwd (deg)  TD (degC)  \\\n",
       "Afeq_data_2000_2023.csv                 4.3         12.1        1.9   \n",
       "Ammiad_data_2000_2023.csv               8.0         45.2        1.3   \n",
       "Avne Etan_data_2000_2023.csv            0.3          0.6        0.1   \n",
       "Ayyelet Hashahar_data_2000_2023.csv     1.1        100.0        0.1   \n",
       "Bet Zayda_data_2000_2023.csv            2.0          0.5        0.4   \n",
       "Dafna_data_2000_2023.csv                0.0          0.0        0.1   \n",
       "Deir Hanna_data_2000_2023.csv           1.1         35.9        1.9   \n",
       "Elon_data_2000_2023.csv                 7.9          0.4        0.4   \n",
       "Eshhar_data_2000_2023.csv               0.7          0.1        0.3   \n",
       "Gamla_data_2000_2023.csv                0.4          0.3        0.1   \n",
       "Haifa Refineries_data_2000_2023.csv     3.2        100.0        0.2   \n",
       "Haifa Technion_data_2000_2023.csv      29.9         25.6        0.4   \n",
       "Harashim_data_2000_2023.csv             1.0         15.9        0.3   \n",
       "Kefar Blum_data_2000_2023.csv           0.0          0.1        0.2   \n",
       "Kefar Giladi_data_2000_2023.csv         2.4          1.7        0.1   \n",
       "Kefar Nahum_data_2000_2023.csv          0.0          0.0        2.2   \n",
       "Merom Golan Picman_data_2000_2023.csv   3.0          0.7        0.0   \n",
       "Rosh Haniqra_data_2000_2023.csv        20.3          4.4        0.9   \n",
       "Shave Ziyyon_data_2000_2023.csv         0.5          0.4        0.3   \n",
       "Zefat Har Kenaan_data_2000_2023.csv    11.2          8.0        1.6   \n",
       "\n",
       "                                       TDmax (degC)  TDmin (degC)   Time  \\\n",
       "Afeq_data_2000_2023.csv                         1.9           1.9   13.1   \n",
       "Ammiad_data_2000_2023.csv                       1.6           1.6   45.1   \n",
       "Avne Etan_data_2000_2023.csv                    0.1           0.1    2.3   \n",
       "Ayyelet Hashahar_data_2000_2023.csv             0.1           0.1  100.0   \n",
       "Bet Zayda_data_2000_2023.csv                    0.4           0.4    4.4   \n",
       "Dafna_data_2000_2023.csv                        0.1           0.1    0.0   \n",
       "Deir Hanna_data_2000_2023.csv                   1.9           1.9   35.9   \n",
       "Elon_data_2000_2023.csv                         0.4           0.4    0.3   \n",
       "Eshhar_data_2000_2023.csv                       0.3           0.3    0.0   \n",
       "Gamla_data_2000_2023.csv                        0.1           0.1    5.7   \n",
       "Haifa Refineries_data_2000_2023.csv             0.2           0.2  100.0   \n",
       "Haifa Technion_data_2000_2023.csv               0.4           0.4   15.3   \n",
       "Harashim_data_2000_2023.csv                     0.3           0.3   15.6   \n",
       "Kefar Blum_data_2000_2023.csv                   0.2           0.2    0.0   \n",
       "Kefar Giladi_data_2000_2023.csv                 0.1           0.1    0.1   \n",
       "Kefar Nahum_data_2000_2023.csv                  2.1           2.2    3.6   \n",
       "Merom Golan Picman_data_2000_2023.csv           0.1           0.1    0.4   \n",
       "Rosh Haniqra_data_2000_2023.csv                 0.9           0.9    1.8   \n",
       "Shave Ziyyon_data_2000_2023.csv                 0.3           0.3    0.2   \n",
       "Zefat Har Kenaan_data_2000_2023.csv             4.6           4.7   13.1   \n",
       "\n",
       "                                       WD (deg)  WDmax (deg)  WS (m/s)  \\\n",
       "Afeq_data_2000_2023.csv                    12.2         12.2      11.6   \n",
       "Ammiad_data_2000_2023.csv                  45.2         45.2      45.3   \n",
       "Avne Etan_data_2000_2023.csv                0.9          0.9       0.6   \n",
       "Ayyelet Hashahar_data_2000_2023.csv       100.0        100.0     100.0   \n",
       "Bet Zayda_data_2000_2023.csv                4.8          4.8       0.9   \n",
       "Dafna_data_2000_2023.csv                    0.0          0.0       0.0   \n",
       "Deir Hanna_data_2000_2023.csv              35.9         35.9      36.2   \n",
       "Elon_data_2000_2023.csv                     0.4          0.5       0.4   \n",
       "Eshhar_data_2000_2023.csv                   0.2          0.2       0.2   \n",
       "Gamla_data_2000_2023.csv                    0.5          0.5       0.5   \n",
       "Haifa Refineries_data_2000_2023.csv       100.0        100.0     100.0   \n",
       "Haifa Technion_data_2000_2023.csv          25.5         25.6      25.5   \n",
       "Harashim_data_2000_2023.csv                16.0         15.6      15.6   \n",
       "Kefar Blum_data_2000_2023.csv               0.2          0.2       0.1   \n",
       "Kefar Giladi_data_2000_2023.csv             1.8          1.8       1.6   \n",
       "Kefar Nahum_data_2000_2023.csv              0.0          0.0       0.0   \n",
       "Merom Golan Picman_data_2000_2023.csv       1.1          1.2       0.5   \n",
       "Rosh Haniqra_data_2000_2023.csv             4.5          4.5       3.3   \n",
       "Shave Ziyyon_data_2000_2023.csv             0.4          0.4       0.3   \n",
       "Zefat Har Kenaan_data_2000_2023.csv         8.0         13.4       8.0   \n",
       "\n",
       "                                       WSmax (m/s)  Ws10mm (m/s)  Ws1mm (m/s)  \\\n",
       "Afeq_data_2000_2023.csv                       11.6          20.9         11.6   \n",
       "Ammiad_data_2000_2023.csv                     45.3          45.2         45.2   \n",
       "Avne Etan_data_2000_2023.csv                   0.7           3.1          3.1   \n",
       "Ayyelet Hashahar_data_2000_2023.csv          100.0         100.0        100.0   \n",
       "Bet Zayda_data_2000_2023.csv                   0.9           6.1          6.1   \n",
       "Dafna_data_2000_2023.csv                       0.0           0.0          0.0   \n",
       "Deir Hanna_data_2000_2023.csv                 36.6          36.6         36.6   \n",
       "Elon_data_2000_2023.csv                        0.4           0.4          0.4   \n",
       "Eshhar_data_2000_2023.csv                      0.2           0.1          0.2   \n",
       "Gamla_data_2000_2023.csv                       0.5           5.9          0.2   \n",
       "Haifa Refineries_data_2000_2023.csv          100.0         100.0        100.0   \n",
       "Haifa Technion_data_2000_2023.csv             25.8          25.5         26.6   \n",
       "Harashim_data_2000_2023.csv                   15.6          15.6         15.6   \n",
       "Kefar Blum_data_2000_2023.csv                  0.1           0.1          0.1   \n",
       "Kefar Giladi_data_2000_2023.csv                1.6           1.4          1.4   \n",
       "Kefar Nahum_data_2000_2023.csv                 0.0           3.6          0.0   \n",
       "Merom Golan Picman_data_2000_2023.csv          0.5           0.6          0.5   \n",
       "Rosh Haniqra_data_2000_2023.csv                3.3           3.3          3.3   \n",
       "Shave Ziyyon_data_2000_2023.csv                0.3           0.3          0.3   \n",
       "Zefat Har Kenaan_data_2000_2023.csv            8.7          13.4         13.4   \n",
       "\n",
       "                                       Average Percentage  \\\n",
       "Afeq_data_2000_2023.csv                              25.8   \n",
       "Ammiad_data_2000_2023.csv                            43.1   \n",
       "Avne Etan_data_2000_2023.csv                         23.1   \n",
       "Ayyelet Hashahar_data_2000_2023.csv                  66.8   \n",
       "Bet Zayda_data_2000_2023.csv                         24.3   \n",
       "Dafna_data_2000_2023.csv                             22.3   \n",
       "Deir Hanna_data_2000_2023.csv                        38.8   \n",
       "Elon_data_2000_2023.csv                              23.5   \n",
       "Eshhar_data_2000_2023.csv                            22.4   \n",
       "Gamla_data_2000_2023.csv                             21.7   \n",
       "Haifa Refineries_data_2000_2023.csv                  67.2   \n",
       "Haifa Technion_data_2000_2023.csv                    20.6   \n",
       "Harashim_data_2000_2023.csv                          29.5   \n",
       "Kefar Blum_data_2000_2023.csv                        22.3   \n",
       "Kefar Giladi_data_2000_2023.csv                      23.0   \n",
       "Kefar Nahum_data_2000_2023.csv                       23.1   \n",
       "Merom Golan Picman_data_2000_2023.csv                17.8   \n",
       "Rosh Haniqra_data_2000_2023.csv                      25.6   \n",
       "Shave Ziyyon_data_2000_2023.csv                      22.6   \n",
       "Zefat Har Kenaan_data_2000_2023.csv                  24.9   \n",
       "\n",
       "                                       Average Percentage Excluding Specific Columns  \n",
       "Afeq_data_2000_2023.csv                                                         16.8  \n",
       "Ammiad_data_2000_2023.csv                                                       34.6  \n",
       "Avne Etan_data_2000_2023.csv                                                     9.2  \n",
       "Ayyelet Hashahar_data_2000_2023.csv                                             64.6  \n",
       "Bet Zayda_data_2000_2023.csv                                                    10.8  \n",
       "Dafna_data_2000_2023.csv                                                         8.2  \n",
       "Deir Hanna_data_2000_2023.csv                                                   29.1  \n",
       "Elon_data_2000_2023.csv                                                          9.7  \n",
       "Eshhar_data_2000_2023.csv                                                        8.4  \n",
       "Gamla_data_2000_2023.csv                                                         9.1  \n",
       "Haifa Refineries_data_2000_2023.csv                                             65.2  \n",
       "Haifa Technion_data_2000_2023.csv                                               18.9  \n",
       "Harashim_data_2000_2023.csv                                                     17.4  \n",
       "Kefar Blum_data_2000_2023.csv                                                    8.3  \n",
       "Kefar Giladi_data_2000_2023.csv                                                  9.2  \n",
       "Kefar Nahum_data_2000_2023.csv                                                   9.2  \n",
       "Merom Golan Picman_data_2000_2023.csv                                            8.9  \n",
       "Rosh Haniqra_data_2000_2023.csv                                                 12.4  \n",
       "Shave Ziyyon_data_2000_2023.csv                                                  8.6  \n",
       "Zefat Har Kenaan_data_2000_2023.csv                                             16.3  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data code files/stations.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Read the stations.json file\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata code files/stations.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      5\u001b[0m     stations_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Count the number of stations\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data code files/stations.json'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Read the stations.json file\n",
    "with open('stations.json', 'r', encoding='utf-8') as f:\n",
    "    stations_data = json.load(f)\n",
    "\n",
    "# Count the number of stations\n",
    "number_of_stations = len(stations_data[\"data\"][\"area_stations\"][\"auto\"])\n",
    "\n",
    "# Print the number of stations\n",
    "print(f'The number of stations in stations.json is: {number_of_stations}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
