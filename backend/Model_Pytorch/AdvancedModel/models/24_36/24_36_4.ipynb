{"cells":[{"cell_type":"markdown","id":"48a75690","metadata":{"id":"48a75690"},"source":["### Colab Mount"]},{"cell_type":"code","execution_count":2,"id":"b24f85e5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b24f85e5","executionInfo":{"status":"ok","timestamp":1738430987013,"user_tz":-120,"elapsed":1755,"user":{"displayName":"יובל רוזנר","userId":"07921919419559702173"}},"outputId":"2f79a314-46eb-456f-b3c4-9bb425b313e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["\n","from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"markdown","id":"d42d954f","metadata":{"id":"d42d954f"},"source":["### `data.py`"]},{"cell_type":"code","execution_count":3,"id":"fff4dae9","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":88},"id":"fff4dae9","executionInfo":{"status":"ok","timestamp":1738430991959,"user_tz":-120,"elapsed":4949,"user":{"displayName":"יובל רוזנר","userId":"07921919419559702173"}},"outputId":"6006f2c2-f30c-4702-bc37-034386968060"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# example of use for this file\\nif __name__ == \"__main__\":\\n    # Load the data\\n    stations_data = loadData([\"Afeq\",\"Harashim\"])\\n    if \"Afeq\" in stations_data:\\n        print(\"Data of Afeq:\")\\n        print(stations_data[\"Afeq\"][0].head())\\n\\n        print(\"Coordinate of Afeq:\")\\n        print(stations_data[\"Afeq\"][1])\\n\\n        print(\"First coordinate of Afeq:\")\\n        print(stations_data[\"Afeq\"][1][0])\\n\\n        print(\"Second coordinate of Afeq:\")\\n        print(stations_data[\"Afeq\"][1][1])\\n    else:\\n        print(\"Afeq data not found\")\\n\\n    print(\"yey\")\\n\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["import pickle\n","import pandas as pd\n","import os\n","import json\n","from sklearn.preprocessing import StandardScaler\n","import pickle\n","import numpy as np\n","import torch\n","from tqdm import tqdm\n","\"\"\"\n","this file let you load the data of the stations from the pkl files\n","and the coordinates of the stations from the json file\n","\"\"\"\n","\n","def normalize_coordinates(x_coords, y_coords):\n","    \"\"\"\n","    Normalize the X and Y coordinates to the range [0, 1].\n","\n","    Args:\n","        x_coords (numpy.ndarray): Array of X coordinates in meters.\n","        y_coords (numpy.ndarray): Array of Y coordinates in meters.\n","\n","    Returns:\n","        tuple: Normalized X and Y coordinates as torch tensors.\n","    \"\"\"\n","    x_min, x_max = x_coords.min(), x_coords.max()\n","    y_min, y_max = y_coords.min(), y_coords.max()\n","\n","    x_normalized = (x_coords - x_min) / (x_max - x_min)\n","    y_normalized = (y_coords - y_min) / (y_max - y_min)\n","\n","    # Convert to torch tensors\n","    x_normalized = torch.tensor(x_normalized, dtype=torch.float32).unsqueeze(1)  # [num_stations, 1]\n","    y_normalized = torch.tensor(y_normalized, dtype=torch.float32).unsqueeze(1)  # [num_stations, 1]\n","\n","    return x_normalized, y_normalized\n","def drop_nan_rows_multiple_custom(df_list,custom_na =['-']):\n","    \"\"\"\n","    Removes rows from all DataFrames in the list where any DataFrame has NaN or custom NaN representations in any column.\n","\n","    Parameters:\n","    df_list (List[pd.DataFrame]): List of DataFrames to process.\n","    reset_indices (bool): Whether to reset the index after dropping rows. Defaults to True.\n","    custom_na (List[str]): List of custom strings to be treated as NaN. Defaults to ['-'].\n","\n","    Returns:\n","    List[pd.DataFrame]: List of cleaned DataFrames.\n","    \"\"\"\n","    if not df_list:\n","        raise ValueError(\"The list of DataFrames is empty.\")\n","\n","    # Ensure all DataFrames have the same number of rows\n","    num_rows = df_list[0].shape[0]\n","    for df in df_list:\n","        if df.shape[0] != num_rows:\n","            raise ValueError(\"All DataFrames must have the same number of rows.\")\n","\n","    # Step 0: Replace custom NaN representations with np.nan\n","    cleaned_df_list_initial = []\n","    for df in df_list:\n","        df_cleaned = df.replace(custom_na, np.nan)\n","        cleaned_df_list_initial.append(df_cleaned)\n","\n","    # Step 1: Identify rows with any NaN in each DataFrame\n","    nan_indices_list = [df.isnull().any(axis=1) for df in cleaned_df_list_initial]\n","\n","    # Step 2: Combine the indices where NaNs are present in any DataFrame\n","    combined_nan = pd.Series([False] * num_rows, index=df_list[0].index)\n","    for nan_mask in nan_indices_list:\n","        combined_nan = combined_nan | nan_mask\n","\n","    # Get the indices to drop\n","    indices_to_drop = combined_nan[combined_nan].index\n","\n","    # Step 3: Drop the identified indices from all DataFrames\n","    cleaned_df_list = []\n","    for df in tqdm(cleaned_df_list_initial, desc=\"Dropping NaN rows\"):\n","        cleaned_df = df.drop(indices_to_drop)\n","        if True:\n","            cleaned_df = cleaned_df.reset_index(drop=True)\n","        cleaned_df_list.append(cleaned_df)\n","\n","    return cleaned_df_list\n","def drop_nan_rows_multiple(df_list, reset_indices=True):\n","    \"\"\"\n","    Removes rows from all DataFrames in the list where any DataFrame has NaN in any column.\n","\n","    Parameters:\n","    df_list (List[pd.DataFrame]): List of DataFrames to process.\n","    reset_indices (bool): Whether to reset the index after dropping rows. Defaults to True.\n","\n","    Returns:\n","    List[pd.DataFrame]: List of cleaned DataFrames.\n","    \"\"\"\n","    if not df_list:\n","        raise ValueError(\"The list of DataFrames is empty.\")\n","    #for df in df_list:\n","    #    df.reset_index(drop=True, inplace=True)\n","    # Ensure all DataFrames have the same number of rows\n","    num_rows = df_list[0].shape[0]\n","    for df in df_list:\n","        if df.shape[0] != num_rows:\n","            raise ValueError(\"All DataFrames must have the same number of rows.\")\n","\n","    # Step 1: Identify rows with any NaN in each DataFrame\n","    nan_indices_list = [df.isnull().any(axis=1) for df in df_list]\n","\n","    # Step 2: Combine the indices where NaNs are present in any DataFrame\n","    combined_nan = pd.Series([False] * num_rows, index=df_list[0].index)\n","    for nan_mask in nan_indices_list:\n","        combined_nan = combined_nan | nan_mask\n","\n","    # Get the indices to drop\n","    indices_to_drop = combined_nan[combined_nan].index\n","\n","    # Step 3: Drop the identified indices from all DataFrames\n","    cleaned_df_list = []\n","    for df in tqdm(df_list, desc=\"Dropping NaN rows\"):\n","        cleaned_df = df.drop(indices_to_drop)\n","        if reset_indices:\n","            cleaned_df = cleaned_df.reset_index(drop=True)\n","        cleaned_df_list.append(cleaned_df)\n","\n","    return cleaned_df_list\n","\n","# Define the normalization function\n","def normalize_coordinates(x_coords, y_coords):\n","    \"\"\"\n","    Normalize the X and Y coordinates to the range [0, 1].\n","    \"\"\"\n","    x_min, x_max = x_coords.min(), x_coords.max()\n","    y_min, y_max = y_coords.min(), y_coords.max()\n","\n","    x_normalized = (x_coords - x_min) / (x_max - x_min)\n","    y_normalized = (y_coords - y_min) / (y_max - y_min)\n","\n","    # Convert to torch tensors\n","    x_normalized = torch.tensor(x_normalized, dtype=torch.float32).unsqueeze(1)  # [num_stations, 1]\n","    y_normalized = torch.tensor(y_normalized, dtype=torch.float32).unsqueeze(1)  # [num_stations, 1]\n","\n","    return x_normalized, y_normalized\n","\n","def timeEncode(dataframes):\n","    day = 24*60*60\n","    year = (365.2425)*day\n","\n","    for df in dataframes:\n","        if 'Date Time' in df.columns:\n","            timestamp_s = df['Date Time'].map(pd.Timestamp.timestamp)\n","            df['Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n","            df['Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n","            df['Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n","            df['Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))\n","            df.drop(columns=['Date Time'], inplace=True)\n","\n","\n","def preprocessing_tensor_df(df):\n","    \"\"\"\n","    Apply the same preprocessing steps as during training.\n","    \"\"\"\n","    print(\"preproccessing data...\")\n","    # Slice the DataFrame and create a copy to avoid SettingWithCopyWarning\n","    df = df[5::6].copy()\n","    date_time = pd.to_datetime(df.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\n","\n","    # Handle 'wv (m/s)'\n","    wv = df['wv (m/s)']\n","    bad_wv = wv == -9999.0\n","    df.loc[bad_wv, 'wv (m/s)'] = 0.0  # Use .loc to modify the original DataFrame\n","    wv = df.pop('wv (m/s)')\n","\n","    # Handle 'max. wv (m/s)'\n","    max_wv = df['max. wv (m/s)']\n","    bad_max_wv = max_wv == -9999.0\n","    df.loc[bad_max_wv, 'max. wv (m/s)'] = 0.0  # Use .loc to modify the original DataFrame\n","    max_wv = df.pop('max. wv (m/s)')\n","\n","    # Convert to radians.\n","    wd_rad = df.pop('wd (deg)') * np.pi / 180\n","\n","    # Calculate wind x and y components using .loc\n","    df.loc[:, 'Wx'] = wv * np.cos(wd_rad)\n","    df.loc[:, 'Wy'] = wv * np.sin(wd_rad)\n","    df.loc[:, 'max Wx'] = max_wv * np.cos(wd_rad)\n","    df.loc[:, 'max Wy'] = max_wv * np.sin(wd_rad)\n","\n","    # Time-based features\n","    timestamp_s = date_time.map(pd.Timestamp.timestamp)\n","    day = 24 * 60 * 60\n","    year = 365.2425 * day\n","\n","    df.loc[:, 'Day sin'] = np.sin(timestamp_s * (2 * np.pi / day))\n","    df.loc[:, 'Day cos'] = np.cos(timestamp_s * (2 * np.pi / day))\n","    df.loc[:, 'Year sin'] = np.sin(timestamp_s * (2 * np.pi / year))\n","    df.loc[:, 'Year cos'] = np.cos(timestamp_s * (2 * np.pi / year))\n","\n","    return df\n","\n","def normalize_data(train_data, val_data, scaler_path='./scaler.pkl'):\n","    \"\"\"\n","    Fit a StandardScaler on the training data and transform both train and val data.\n","    Save the scaler to disk for future use.\n","\n","    Args:\n","        train_data (np.ndarray): Training data.\n","        val_data (np.ndarray): Validation data.\n","        scaler_path (str): Path to save the scaler.\n","\n","    Returns:\n","        train_data_scaled (np.ndarray): Scaled training data.\n","        val_data_scaled (np.ndarray): Scaled validation data.\n","        scaler (StandardScaler): Fitted scaler object.\n","    \"\"\"\n","    scaler = StandardScaler()\n","    scaler.fit(train_data)\n","\n","    train_data_scaled = scaler.transform(train_data)\n","    val_data_scaled = scaler.transform(val_data)\n","\n","    # Save the scaler\n","    with open(scaler_path, 'wb') as f:\n","        pickle.dump(scaler, f)\n","\n","    print(f\"Scaler saved to {scaler_path}\")\n","\n","    return train_data_scaled, val_data_scaled, scaler\n","\n","def preprocessing_our_df(df):\n","    \"\"\"\n","    Apply the same preprocessing steps as during training.\n","    \"\"\"\n","    print(\"preproccessing data...\")\n","    df = df[5::6].copy()\n","    # drop nan\n","    df = df.dropna()\n","    return df\n","\n","def return_and_save_scaler_normalize_data(train_data, val_data, scaler_path='./scaler.pkl'):\n","\n","    scaler = StandardScaler()\n","    scaler.fit(train_data)\n","\n","    # Save the scaler\n","    with open(scaler_path, 'wb') as f:\n","        pickle.dump(scaler, f)\n","\n","    print(f\"Scaler saved to {scaler_path}\")\n","\n","    return scaler\n","\n","def normalize_data_independent(train_data, val_data, scaler_dir='./scalers'):\n","    \"\"\"\n","    Fit a StandardScaler per station on the training data and transform both train and val data.\n","    Save each scaler to disk for future use.\n","\n","    Args:\n","        train_data (np.ndarray): Training data of shape (T_train, num_stations, num_features).\n","        val_data (np.ndarray): Validation data of shape (T_val, num_stations, num_features).\n","        scaler_dir (str): Directory path to save the scalers.\n","\n","    Returns:\n","        train_data_scaled (np.ndarray): Scaled training data of shape (T_train, num_stations, num_features).\n","        val_data_scaled (np.ndarray): Scaled validation data of shape (T_val, num_stations, num_features).\n","        scalers (list of StandardScaler): List containing a scaler for each station.\n","    \"\"\"\n","    if not os.path.exists(scaler_dir):\n","        os.makedirs(scaler_dir)\n","\n","    T_train, num_stations, num_features = train_data.shape\n","    T_val = val_data.shape[0]\n","\n","    # Initialize arrays to hold scaled data\n","    train_data_scaled = np.zeros_like(train_data)\n","    val_data_scaled = np.zeros_like(val_data)\n","\n","    scalers = []\n","\n","    for station_idx in range(num_stations):\n","        scaler = StandardScaler()\n","\n","        # Extract training data for the current station\n","        train_station_data = train_data[:, station_idx, :]  # Shape: (T_train, num_features)\n","\n","        # Fit the scaler on training data\n","        scaler.fit(train_station_data)\n","        scalers.append(scaler)\n","\n","        # Transform training and validation data for the current station\n","        train_data_scaled[:, station_idx, :] = scaler.transform(train_station_data)\n","        val_data_scaled[:, station_idx, :] = scaler.transform(val_data[:, station_idx, :])\n","\n","        # Save the scaler for the current station\n","        scaler_path = os.path.join(scaler_dir, f'scaler_station_{station_idx}.pkl')\n","        with open(scaler_path, 'wb') as f:\n","            pickle.dump(scaler, f)\n","        print(f\"Scaler for Station {station_idx} saved to {scaler_path}\")\n","\n","    return train_data_scaled, val_data_scaled, scalers\n","\n","def normalize_data_collective(train_data, val_data, scaler_path='./scaler.pkl'):\n","    \"\"\"\n","    Fit a single StandardScaler across all stations and features.\n","\n","    Args:\n","        train_data (np.ndarray): Training data of shape (T_train, num_stations, num_features).\n","        val_data (np.ndarray): Validation data of shape (T_val, num_stations, num_features).\n","        scaler_path (str): Path to save the scaler.\n","\n","    Returns:\n","        train_scaled (np.ndarray), val_scaled (np.ndarray), scaler (StandardScaler)\n","    \"\"\"\n","    T_train, num_stations, num_features = train_data.shape\n","    T_val = val_data.shape[0]\n","\n","    # Reshape to (T_train*num_stations, num_features)\n","    train_reshaped = train_data.reshape(-1, num_features)\n","    val_reshaped = val_data.reshape(-1, num_features)\n","\n","    scaler = StandardScaler()\n","    scaler.fit(train_reshaped)\n","\n","    train_scaled = scaler.transform(train_reshaped).reshape(train_data.shape)\n","    val_scaled = scaler.transform(val_reshaped).reshape(val_data.shape)\n","\n","    # Save the scaler\n","    with open(scaler_path, 'wb') as f:\n","        pickle.dump(scaler, f)\n","    print(f\"Scaler saved to {scaler_path}\")\n","\n","    return train_scaled, val_scaled, scaler\n","\n","def load_pkl_file(station_name):\n","    current_path = os.path.dirname(__file__)\n","    file_path = f\"{current_path}\\\\..\\\\..\\\\..\\\\data\\\\{station_name}.pkl\"\n","    try:\n","        with open(file_path, 'rb') as file:\n","            data = pickle.load(file)\n","        print(f\"data succsesfuly loaded from {file_path}\")\n","        return data\n","    except Exception as e:\n","        print(f\"Failed to load file:\\n{e}\")\n","        return None\n","\n","def openJsonFile():\n","    current_path = os.path.dirname(__file__)\n","    file_path = f\"{current_path}\\\\..\\\\..\\\\data code files\\\\stations_details_updated.json\"\n","    with open(file_path) as file:\n","        stations = json.load(file)\n","    return stations\n","\n","def loadCoordinatesNewIsraelData(stations_details, station_name):\n","    for station_id, station_details in stations_details.items():\n","        if station_details[\"name\"] == station_name:\n","            return station_details[\"coordinates_in_a_new_israe\"][\"east\"], station_details[\"coordinates_in_a_new_israe\"][\"north\"]\n","\n","def loadData(station_names):\n","    stations_data = {}\n","    stations_details = openJsonFile()\n","    for station in station_names:\n","        stations_csv = load_pkl_file(station)\n","        station_coordinates = loadCoordinatesNewIsraelData(stations_details, station)\n","        stations_data[station] = stations_csv, station_coordinates\n","    return stations_data\n","\"\"\"\n","# example of use for this file\n","if __name__ == \"__main__\":\n","    # Load the data\n","    stations_data = loadData([\"Afeq\",\"Harashim\"])\n","    if \"Afeq\" in stations_data:\n","        print(\"Data of Afeq:\")\n","        print(stations_data[\"Afeq\"][0].head())\n","\n","        print(\"Coordinate of Afeq:\")\n","        print(stations_data[\"Afeq\"][1])\n","\n","        print(\"First coordinate of Afeq:\")\n","        print(stations_data[\"Afeq\"][1][0])\n","\n","        print(\"Second coordinate of Afeq:\")\n","        print(stations_data[\"Afeq\"][1][1])\n","    else:\n","        print(\"Afeq data not found\")\n","\n","    print(\"yey\")\n","\n","\"\"\""]},{"cell_type":"markdown","id":"738ee5d4","metadata":{"id":"738ee5d4"},"source":["### `window_generator_multiple_stations.py`"]},{"cell_type":"code","execution_count":4,"id":"47fd4d1d","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":123},"id":"47fd4d1d","executionInfo":{"status":"ok","timestamp":1738430991960,"user_tz":-120,"elapsed":19,"user":{"displayName":"יובל רוזנר","userId":"07921919419559702173"}},"outputId":"21ca30dd-3524-4c25-ac0f-98f2fda952cc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nif __name__ == \"__main__\":\\n    import numpy as np\\n    import torch\\n\\n    # Number of time steps, stations, and features\\n    T = 1000\\n    num_stations = 5\\n    num_features = 10\\n\\n    # Generate random data\\n    np.random.seed(42)\\n    data = np.random.randn(T, num_stations, num_features).astype(np.float32)\\n\\n    # Define target station index\\n    target_station_idx = 0  # Change as needed\\n\\n    # Create WindowGenerator instance\\n    window_size = 24\\n    label_size = 1\\n    shift = 1\\n\\n    window_gen = WindowGeneratorMultipleStations(\\n        data=data,\\n        input_width=window_size,\\n        label_width=label_size,\\n        shift=shift,\\n        label_columns=[2],  # Set to specific feature indices if needed\\n        target_station_idx=target_station_idx\\n    )\\n\\n    # Create DataLoader\\n    batch_size = 32\\n    train_loader = DataLoader(window_gen, batch_size=batch_size, shuffle=True)\\n\\n    # Iterate through one batch\\n    for batch_idx, (x, y) in enumerate(train_loader):\\n        print(f\"Batch {batch_idx + 1}\")\\n        print(f\"x shape: {x.shape}\")  # Expected: [batch_size, input_width, num_stations, num_features]\\n        print(f\"y shape: {y.shape}\")  # Expected: [batch_size, label_width]\\n        break  # Only show the first batch\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}],"source":["import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","class WindowGeneratorMultipleStations(Dataset):\n","    \"\"\"\n","    Creates sliding windows from a multi-station dataset.\n","    data: shape (T, num_stations, num_features)\n","    input_width (int): window size for input\n","    label_width (int): how many time steps to predict\n","    shift (int): how far ahead the prediction starts after the input\n","    label_columns (list[int] or None): indices of the columns used as labels\n","    target_station_idx (int): index of the target station\n","    \"\"\"\n","    def __init__(\n","        self,\n","        data,\n","        input_width,\n","        label_width,\n","        shift,\n","        label_columns=None,\n","        target_station_idx=0,\n","    ):\n","        super().__init__()\n","\n","        if not isinstance(data, torch.Tensor):\n","            data = torch.tensor(data, dtype=torch.float32)\n","        self.data = data  # shape (T, num_stations, num_features)\n","\n","        self.input_width = input_width\n","        self.label_width = label_width\n","        self.shift = shift\n","\n","        # total window = input plus how far to shift plus label_width\n","        self.total_window_size = input_width + shift - 1 + label_width\n","\n","        # label columns\n","        self.label_columns = label_columns\n","        # If None, we'll just return all features as labels\n","\n","        if label_columns is not None:\n","            self.num_label_features = len(label_columns)\n","        else:\n","            self.num_label_features = self.data.shape[-1]\n","\n","        # Corrected number of samples\n","        self.num_samples = len(self.data) - self.total_window_size + 1\n","\n","        if self.num_samples < 1:\n","            raise ValueError(\"Not enough data to create windows with these parameters.\")\n","\n","        self.target_station_idx = target_station_idx\n","\n","    def __len__(self):\n","        return self.num_samples\n","\n","    def __getitem__(self, idx):\n","        # input window range\n","        x_start = idx\n","        x_end = x_start + self.input_width  # not inclusive\n","\n","        # label window range\n","        y_start = x_start + self.input_width + self.shift - 1\n","        y_end = y_start + self.label_width\n","\n","        # slice input\n","        x = self.data[x_start:x_end]  # shape (input_width, num_stations, num_features)\n","\n","        # slice label\n","        x = x.permute(1, 0, 2)\n","        if self.label_columns is not None:\n","            y = self.data[y_start:y_end, self.target_station_idx, self.label_columns]  # [label_width, num_label_features]\n","        else:\n","            y = self.data[y_start:y_end, self.target_station_idx, :]  # [label_width, num_features]\n","        # Return x and y\n","        return x, y\n","\n","    def __repr__(self):\n","        return '\\n'.join([\n","            f'Total window size: {self.total_window_size}',\n","            f'Input window size: {self.input_width}',\n","            f'Label window size: {self.label_width}',\n","            f'Shift: {self.shift}',\n","            f'Label columns indices: {self.label_columns}',\n","            f'Target station index: {self.target_station_idx}'\n","        ])\n","\n","\"\"\"\n","if __name__ == \"__main__\":\n","    import numpy as np\n","    import torch\n","\n","    # Number of time steps, stations, and features\n","    T = 1000\n","    num_stations = 5\n","    num_features = 10\n","\n","    # Generate random data\n","    np.random.seed(42)\n","    data = np.random.randn(T, num_stations, num_features).astype(np.float32)\n","\n","    # Define target station index\n","    target_station_idx = 0  # Change as needed\n","\n","    # Create WindowGenerator instance\n","    window_size = 24\n","    label_size = 1\n","    shift = 1\n","\n","    window_gen = WindowGeneratorMultipleStations(\n","        data=data,\n","        input_width=window_size,\n","        label_width=label_size,\n","        shift=shift,\n","        label_columns=[2],  # Set to specific feature indices if needed\n","        target_station_idx=target_station_idx\n","    )\n","\n","    # Create DataLoader\n","    batch_size = 32\n","    train_loader = DataLoader(window_gen, batch_size=batch_size, shuffle=True)\n","\n","    # Iterate through one batch\n","    for batch_idx, (x, y) in enumerate(train_loader):\n","        print(f\"Batch {batch_idx + 1}\")\n","        print(f\"x shape: {x.shape}\")  # Expected: [batch_size, input_width, num_stations, num_features]\n","        print(f\"y shape: {y.shape}\")  # Expected: [batch_size, label_width]\n","        break  # Only show the first batch\n","\"\"\""]},{"cell_type":"markdown","id":"4d9afdae","metadata":{"id":"4d9afdae"},"source":["### `model.py`"]},{"cell_type":"code","execution_count":5,"id":"3bc88cb8","metadata":{"id":"3bc88cb8","executionInfo":{"status":"ok","timestamp":1738430991960,"user_tz":-120,"elapsed":17,"user":{"displayName":"יובל רוזנר","userId":"07921919419559702173"}}},"outputs":[],"source":["# model.py\n","\n","import torch\n","import torch.nn as nn\n","import math\n","\n","class StationCNN(nn.Module):\n","    def __init__(self,\n","                 input_features=15,\n","                 output_per_feature=3,\n","                 kernel_size=3,\n","                 use_batch_norm=False,\n","                 use_residual=False):\n","        \"\"\"\n","        Args:\n","            input_features (int): Number of input features per station.\n","            output_per_feature (int): Number of output channels per feature.\n","            kernel_size (int): Size of the convolutional kernel.\n","            use_batch_norm (bool): Whether to use Batch Normalization.\n","            use_residual (bool): Whether to use residual connections.\n","        \"\"\"\n","        super(StationCNN, self).__init__()\n","        self.output_per_feature = output_per_feature\n","        self.use_batch_norm = use_batch_norm\n","        self.use_residual = use_residual\n","\n","        # Total out_channels = input_features * output_per_feature\n","        self.out_channels = input_features * output_per_feature\n","\n","        # First convolutional layer\n","        self.conv1 = nn.Conv1d(\n","            in_channels=input_features,\n","            out_channels=self.out_channels,\n","            kernel_size=kernel_size,\n","            padding=kernel_size // 2,\n","            groups=input_features  # Depthwise convolution\n","        )\n","        self.relu1 = nn.ReLU()\n","\n","        # Optional Batch Normalization\n","        if self.use_batch_norm:\n","            self.bn1 = nn.BatchNorm1d(self.out_channels)\n","\n","        # Second convolutional layer (optional for deeper CNN)\n","        self.conv2 = nn.Conv1d(\n","            in_channels=self.out_channels,\n","            out_channels=self.out_channels,\n","            kernel_size=kernel_size,\n","            padding=kernel_size // 2,\n","            groups=input_features  # Maintain feature independence\n","        )\n","        self.relu2 = nn.ReLU()\n","\n","        # Optional Batch Normalization\n","        if self.use_batch_norm:\n","            self.bn2 = nn.BatchNorm1d(self.out_channels)\n","\n","        # Optional Residual Connection\n","        if self.use_residual:\n","            self.residual_conv = nn.Conv1d(\n","                in_channels=input_features,\n","                out_channels=self.out_channels,\n","                kernel_size=1,\n","                groups=input_features  # Depthwise 1x1 convolution\n","            )\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x (torch.Tensor): Input tensor of shape [batch_size, input_features, time_steps]\n","\n","        Returns:\n","            torch.Tensor: Output tensor of shape [batch_size, output_per_feature, time_steps, input_features]\n","        \"\"\"\n","        b, f, t = x.shape  # [batch_size, input_features, time_steps]\n","\n","        # First convolution\n","        out = self.conv1(x)  # [batch_size, input_features * output_per_feature, time_steps]\n","        if self.use_batch_norm:\n","            out = self.bn1(out)\n","        out = self.relu1(out)\n","\n","        # Second convolution\n","        out = self.conv2(out)  # [batch_size, input_features * output_per_feature, time_steps]\n","        if self.use_batch_norm:\n","            out = self.bn2(out)\n","        out = self.relu2(out)\n","\n","        # Optional Residual Connection\n","        if self.use_residual:\n","            residual = self.residual_conv(x)  # [batch_size, input_features * output_per_feature, time_steps]\n","            out = out + residual\n","            out = self.relu2(out)\n","\n","        # Reshape to [batch_size, output_per_feature, input_features, time_steps]\n","        out = out.view(b, self.output_per_feature, f, t)\n","        # Permute to [batch_size, output_per_feature, time_steps, input_features]\n","        out = out.permute(0, 1, 3, 2)  # [batch_size, output_per_feature, time_steps, features]\n","\n","        return out  # [batch_size, output_per_feature, time_steps, features]\n","\n","class CoordinatePositionalEncoding(nn.Module):\n","    def __init__(self, d_model):\n","        super(CoordinatePositionalEncoding, self).__init__()\n","        # Assuming two coordinates: X and Y\n","        self.lat_linear = nn.Linear(1, d_model // 2)\n","        self.lon_linear = nn.Linear(1, d_model // 2)\n","        self.activation = nn.ReLU()\n","\n","    def forward(self, lat, lon):\n","        \"\"\"\n","        Args:\n","            lat (torch.Tensor): [num_stations, 1] - Normalized X coordinates\n","            lon (torch.Tensor): [num_stations, 1] - Normalized Y coordinates\n","        Returns:\n","            torch.Tensor: [num_stations, d_model]\n","        \"\"\"\n","        lat_enc = self.lat_linear(lat)  # [num_stations, d_model//2]\n","        lon_enc = self.lon_linear(lon)  # [num_stations, d_model//2]\n","        spatial_emb = self.activation(torch.cat([lat_enc, lon_enc], dim=1))  # [num_stations, d_model]\n","        return spatial_emb\n","\n","class TemporalPositionalEncoding(nn.Module):\n","    def __init__(self, d_model, max_len=5000):\n","        super(TemporalPositionalEncoding, self).__init__()\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x (torch.Tensor): [batch_size, num_stations, time_steps, d_model]\n","        Returns:\n","            torch.Tensor: [batch_size, num_stations, time_steps, d_model]\n","        \"\"\"\n","        x = x + self.pe[:, :x.size(2), :].unsqueeze(1)  # [batch, num_stations, time_steps, d_model]\n","        return x\n","\n","class TargetedWeatherPredictionModel(nn.Module):\n","    def __init__(self, num_stations, time_steps, feature_dim, kernel_size,\n","                 d_model, nhead, num_layers, target_station_idx, label_width=1,\n","                 output_per_feature=3, use_batch_norm=False, use_residual=False):\n","        \"\"\"\n","        Args:\n","            num_stations (int): Number of stations.\n","            time_steps (int): Number of time steps in the sliding window.\n","            feature_dim (int): Number of features per station.\n","            kernel_size (int): Size of the CNN kernel.\n","            d_model (int): Dimension of the model (for Transformer).\n","            nhead (int): Number of attention heads in the Transformer.\n","            num_layers (int): Number of Transformer encoder layers.\n","            target_station_idx (int): Index of the target station.\n","            label_width (int): Number of prediction steps.\n","            output_per_feature (int): Number of output channels per feature in CNN.\n","            use_batch_norm (bool): Whether to use Batch Normalization in CNNs.\n","            use_residual (bool): Whether to use residual connections in CNNs.\n","        \"\"\"\n","        super(TargetedWeatherPredictionModel, self).__init__()\n","        self.num_stations = num_stations\n","        self.time_steps = time_steps\n","        self.target_station_idx = target_station_idx\n","        self.label_width = label_width\n","        self.output_per_feature = output_per_feature\n","\n","        # Initialize separate CNNs for each station\n","        self.station_cnns = nn.ModuleList([\n","            StationCNN(\n","                input_features=feature_dim,\n","                output_per_feature=output_per_feature,\n","                kernel_size=kernel_size,\n","                use_batch_norm=use_batch_norm,\n","                use_residual=use_residual\n","            )\n","            for _ in range(num_stations)\n","        ])\n","\n","        # Coordinate Positional Encoding\n","        self.coord_pos_encoding = CoordinatePositionalEncoding(d_model=d_model)\n","\n","        # Linear layer to map CNN features to d_model\n","        # New feature_dim after CNN: output_per_feature * original feature_dim\n","        self.feature_mapping = nn.Linear(feature_dim * output_per_feature, d_model)\n","\n","        # Temporal Positional Encoding\n","        self.temporal_pos_encoding = TemporalPositionalEncoding(d_model=d_model, max_len=time_steps)\n","\n","        # Transformer Encoder\n","        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n","        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n","\n","        # Final prediction layer\n","        self.fc_out = nn.Linear(d_model, label_width)  # Output label_width predictions\n","\n","    def forward(self, x, lat, lon):\n","        \"\"\"\n","        Args:\n","            x (torch.Tensor): [batch_size, num_stations, time_steps, feature_dim]\n","            lat (torch.Tensor): [num_stations, 1] - Normalized X coordinates\n","            lon (torch.Tensor): [num_stations, 1] - Normalized Y coordinates\n","        Returns:\n","            torch.Tensor: [batch_size, label_width]\n","        \"\"\"\n","        batch_size, num_stations, time_steps, feature_dim = x.size()\n","\n","        # Extract temporal features for each station\n","        # Initialize a list to collect CNN outputs\n","        temporal_features = []\n","        for i in range(num_stations):\n","            station_data = x[:, i, :, :]  # [batch_size, time_steps, feature_dim]\n","            station_data = station_data.permute(0, 2, 1)  # [batch_size, feature_dim, time_steps]\n","            cnn_out = self.station_cnns[i](station_data)  # [batch_size, output_per_feature, time_steps, feature_dim]\n","            temporal_features.append(cnn_out)\n","\n","        # Stack temporal features: [batch_size, num_stations, output_per_feature, time_steps, feature_dim]\n","        temporal_features = torch.stack(temporal_features, dim=1)  # [batch, num_stations, output_per_feature, time_steps, features]\n","\n","        # Reshape to combine output_per_feature and features dimensions\n","        # New shape: [batch_size, num_stations, time_steps, output_per_feature * feature_dim]\n","        temporal_features = temporal_features.view(batch_size, num_stations, self.output_per_feature, time_steps, feature_dim)\n","        temporal_features = temporal_features.permute(0, 1, 3, 2, 4)  # [batch, num_stations, time_steps, output_per_feature, features]\n","        temporal_features = temporal_features.contiguous().view(batch_size, num_stations, time_steps, self.output_per_feature * feature_dim)  # [batch, num_stations, time_steps, output_per_feature * features]\n","\n","        # Spatial positional encoding using coordinates\n","        spatial_emb = self.coord_pos_encoding(lat, lon)  # [num_stations, d_model]\n","        spatial_emb = spatial_emb.unsqueeze(0).unsqueeze(2)  # [1, num_stations, 1, d_model]\n","\n","        # Map temporal features to d_model\n","        temporal_features = self.feature_mapping(temporal_features)  # [batch_size, num_stations, time_steps, d_model]\n","\n","        # Apply temporal positional encoding\n","        temporal_features = self.temporal_pos_encoding(temporal_features)  # [batch, num_stations, time_steps, d_model]\n","\n","        # Combine temporal and spatial features\n","        combined_features = temporal_features + spatial_emb  # [batch, num_stations, time_steps, d_model]\n","\n","        # Reshape for Transformer: [batch_size, num_stations * time_steps, d_model]\n","        combined_features = combined_features.view(batch_size, num_stations * time_steps, -1)\n","\n","        # Transpose for Transformer: [sequence_length, batch_size, d_model]\n","        combined_features = combined_features.permute(1, 0, 2)  # [num_stations * time_steps, batch_size, d_model]\n","\n","        # Transformer expects [sequence_length, batch_size, d_model]\n","        transformer_out = self.transformer_encoder(combined_features)  # [sequence_length, batch_size, d_model]\n","\n","        # Reshape back: [batch_size, num_stations, time_steps, d_model]\n","        transformer_out = transformer_out.permute(1, 0, 2)  # [batch_size, sequence_length, d_model]\n","        transformer_out = transformer_out.view(batch_size, num_stations, time_steps, -1)  # [batch_size, num_stations, time_steps, d_model]\n","\n","        # Select target station's features: [batch_size, time_steps, d_model]\n","        target_features = transformer_out[:, self.target_station_idx, :, :]  # [batch_size, time_steps, d_model]\n","\n","        # Instead of mean pooling, retain temporal information or use other aggregation\n","        # Here, we'll take the last time step's features for simplicity\n","        last_time_step_features = target_features[:, -1, :]  # [batch_size, d_model]\n","\n","        # Final prediction\n","        prediction = self.fc_out(last_time_step_features)  # [batch_size, label_width]\n","\n","        return prediction\n"]},{"cell_type":"markdown","id":"11871581","metadata":{"id":"11871581"},"source":["### `train.py`"]},{"cell_type":"code","execution_count":6,"id":"eb97dec1","metadata":{"id":"eb97dec1","executionInfo":{"status":"ok","timestamp":1738430991961,"user_tz":-120,"elapsed":17,"user":{"displayName":"יובל רוזנר","userId":"07921919419559702173"}}},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","import numpy as np\n","from tqdm import tqdm\n","import logging\n","\n","def setup_logger(log_dir, log_filename=\"training.log\"):\n","\n","    # Check if path exists, if not, return None\n","    if not os.path.exists(log_dir):\n","        print(f\"Error: Directory {log_dir} does not exist.\")\n","        return None\n","\n","    # Define log file path\n","    log_path = os.path.join(log_dir, log_filename)\n","\n","    # Create logger\n","    logger = logging.getLogger(\"TrainingLogger\")\n","    logger.setLevel(logging.INFO)\n","\n","    # Check if the logger already has handlers (to avoid duplicate logs)\n","    if not logger.handlers:\n","        # Create a file handler\n","        file_handler = logging.FileHandler(log_path, mode='a')\n","        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n","\n","        # Add handler to the logger\n","        logger.addHandler(file_handler)\n","\n","    return logger\n","def get_logger():\n","    \"\"\"Retrieve the logger instance.\"\"\"\n","    return logging.getLogger(\"TrainingLogger\")\n","\n","def train_model(\n","        train_dataset,\n","        val_dataset,\n","        model,\n","        coordinates,\n","        epochs=50,  # Increased epochs for better exploration\n","        batch_size=32,\n","        lr=1e-4,    # Updated learning rate as per recommendation\n","        checkpoint_dir='./checkpoints',\n","        resume=False,\n","        device='cuda' if torch.cuda.is_available() else 'cpu',\n","        early_stopping_patience=10,\n","        scheduler_patience=5,\n","        scheduler_factor=0.5,\n","        min_lr=1e-7,\n","        logger_path=None\n","    ):\n","    logger = setup_logger(logger_path)\n","    logger.info(f\"start logger\")\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","    # Move model to device\n","    model = model.to(device)\n","\n","    # Loss and optimizer\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    # Initialize scheduler\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","        optimizer,\n","        mode='min',\n","        factor=scheduler_factor,\n","        patience=scheduler_patience,\n","        verbose=True,\n","        min_lr=min_lr\n","    )\n","\n","    best_val_loss = float('inf')\n","    patience_counter = 0\n","    start_epoch = 0\n","\n","    os.makedirs(checkpoint_dir, exist_ok=True)\n","    latest_ckpt = os.path.join(checkpoint_dir, 'latest_checkpoint.pth')\n","\n","    # Resume training if needed\n","    if resume and os.path.exists(latest_ckpt):\n","        print(\"Resuming training from latest checkpoint...\")\n","        checkpoint = torch.load(latest_ckpt, map_location=device)\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        start_epoch = checkpoint['epoch'] + 1\n","        best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n","        print(f\"Resumed from epoch {start_epoch}, best_val_loss={best_val_loss:.4f}\")\n","        logger.info(f\"Resumed from epoch {start_epoch}, best_val_loss={best_val_loss:.4f}\")\n","\n","    for epoch in range(start_epoch, start_epoch + epochs):\n","        model.train()\n","        epoch_loss = 0.0\n","        for batch_idx, (x_batch, y_batch) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{start_epoch + epochs}\")):\n","            x_batch = x_batch.to(device)  # [batch_size, input_width, num_stations, num_features]\n","            y_batch = y_batch.to(device)  # [batch_size, num_label_features]\n","            coord1 = coordinates[0].to(device)\n","            coord2 = coordinates[1].to(device)\n","            # Forward pass with coordinates\n","            preds = model(x_batch, coord1, coord2)  # [batch_size, 1]\n","\n","            # Flatten y_batch if necessary\n","            y_batch_single = y_batch.squeeze(-1)  # [batch_size]\n","            preds = preds.squeeze(-1)            # [batch_size]\n","\n","            # Compute loss\n","            loss = criterion(preds, y_batch_single)\n","\n","            # Backward pass and optimization\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            # Accumulate loss\n","            epoch_loss += loss.item()\n","\n","        # Compute average loss for the epoch\n","        train_loss = epoch_loss / len(train_loader)\n","\n","        # Validation\n","        model.eval()\n","        val_losses = []\n","        with torch.no_grad():\n","            for x_val, y_val in val_loader:\n","                x_val = x_val.to(device)\n","                y_val = y_val.to(device)\n","                coord1 = coordinates[0].to(device)\n","                coord2 = coordinates[1].to(device)\n","                # Forward pass with coordinates\n","                preds_val = model(x_val, coord1, coord2)  # [batch_size, 1]\n","\n","                # Flatten predictions and labels\n","                preds_val = preds_val.squeeze(-1)          # [batch_size]\n","                y_val_single = y_val.squeeze(-1)          # [batch_size]\n","\n","                # Compute loss\n","                loss_val = criterion(preds_val, y_val_single)\n","                val_losses.append(loss_val.item())\n","\n","        # Compute average validation loss\n","        val_loss = np.mean(val_losses)\n","\n","        print(f\"Epoch {epoch+1}/{start_epoch + epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n","        logger.info(f\"Epoch {epoch+1}/{start_epoch + epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n","        # Step the scheduler based on validation loss\n","        scheduler.step(val_loss)\n","\n","        current_lr = optimizer.param_groups[0]['lr']\n","        print(f\"  -> Current Learning Rate: {current_lr:.6f}\")\n","        logger.info(f\"  -> Current Learning Rate: {current_lr:.6f}\")\n","        # Early Stopping Logic\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            patience_counter = 0  # Reset counter if validation loss improves\n","\n","            # Save the best model\n","            best_ckpt = os.path.join(checkpoint_dir, 'best_checkpoint.pth')\n","            torch.save({\n","                'epoch': epoch,\n","                'model_state_dict': model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict(),\n","                'best_val_loss': best_val_loss,\n","            }, best_ckpt)\n","            print(f\"  -> Best model saved at epoch {epoch+1} (val_loss={val_loss:.4f})\")\n","            logger.info(f\"  -> Best model saved at epoch {epoch+1} (val_loss={val_loss:.4f})\")\n","        else:\n","            patience_counter += 1\n","            print(f\"  -> No improvement in validation loss for {patience_counter} epoch(s)\")\n","            logger.info(f\"  -> No improvement in validation loss for {patience_counter} epoch(s)\")\n","            if patience_counter >= early_stopping_patience:\n","                print(\"Early stopping triggered.\")\n","                logger.info(\"Early stopping triggered.\")\n","                break  # Exit the training loop\n","\n","        # Checkpoint: always save the 'latest'\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'best_val_loss': best_val_loss,\n","        }, latest_ckpt)\n"]},{"cell_type":"markdown","id":"62c6b0a1","metadata":{"id":"62c6b0a1"},"source":["### `parameters.py`"]},{"cell_type":"code","execution_count":7,"id":"d368c494","metadata":{"id":"d368c494","executionInfo":{"status":"ok","timestamp":1738430991961,"user_tz":-120,"elapsed":17,"user":{"displayName":"יובל רוזנר","userId":"07921919419559702173"}}},"outputs":[],"source":["import torch\n","import os\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Determine device\n","\n","# must define these 3 variables below!!!\n","###########################################################################################################################################################\n","# for training - where the output will be saved\n","train_base_path = '/content/drive/MyDrive/hyperparameters/24_36/4' # in general we put the folder of the path that contains the parameters.py file\n","\n","# you need to put int inference_base_path,in it will be folders where each folder has the model files - inference_base_path/model_i/parameters.py, inference_base_path/model_i/scalers, inference_base_path/model_i/checkpoints.\n","inference_base_path = \" \"#os.path.dirname(__file__)\n","models_paths_dir_names = ['model_1'] # for instance for alot of models we want to inference: ['model_1','model_2' ... ] for one : ['model_1']\n","\n","###########################################################################################################################################################\n","\n","\n","\n","output_path = os.path.join(train_base_path, 'output')\n","checkpoints_path = os.path.join(output_path, 'checkpoints')\n","scalers_path = os.path.join(output_path, 'scalers')\n","inference_output_path = os.path.join(output_path, 'inference_output')\n","\n","STATIONS_COORDINATES = {\n","    'Tavor Kadoorie':           (238440, 734540),\n","    'Newe Yaar':                (217010, 734820),\n","    'Yavneel':                  (248110, 733730),\n","    'En Hashofet':              (209310, 723170),\n","    'Eden Farm':                (246190, 708240),\n","    'Eshhar':                   (228530, 754390),\n","    'Afula Nir Haemeq':         (226260, 722410)\n","}\n","\n","STATIONS_COORDINATES_COLAB = {\n","    f'/content/drive/MyDrive/final data/Tavor Kadoorie.pkl':     (238440, 734540),\n","    f'/content/drive/MyDrive/final data/Newe Yaar.pkl':          (217010, 734820),\n","    f'/content/drive/MyDrive/final data/Yavneel.pkl':            (248110, 733730),\n","    f'/content/drive/MyDrive/final data/En Hashofet.pkl':        (209310, 723170),\n","    f'/content/drive/MyDrive/final data/Eden Farm.pkl':          (246190, 708240),\n","    f'/content/drive/MyDrive/final data/Eshhar.pkl':             (228530, 754390),\n","    f'/content/drive/MyDrive/final data/Afula Nir Haemeq.pkl':   (226260, 722410)\n","}\n","\n","STATIONS_LIST = {\n","    \"Tavor Kadoorie\":   \"13\",\n","    \"Newe Yaar\":        \"186\",\n","    \"Yavneel\":          \"11\",\n","    \"En Hashofet\":      \"67\",\n","    \"Eden Farm\":        \"206\",\n","    \"Eshhar\":           \"205\",\n","    \"Afula Nir Haemeq\": \"16\"\n","}\n","\n","PARAMS = {\n","    #'paths_in_colab': [f'/content/Newe Yaar_data_2005_2024.pkl', f'/content/Tavor Kadoorie_data_2005_2024.pkl'],\n","    'paths_in_colab': [\n","        f'/content/drive/MyDrive/final data/Tavor Kadoorie.pkl',\n","        f'/content/drive/MyDrive/final data/Newe Yaar.pkl',\n","        f'/content/drive/MyDrive/final data/Yavneel.pkl',\n","        f'/content/drive/MyDrive/final data/En Hashofet.pkl',\n","        f'/content/drive/MyDrive/final data/Eden Farm.pkl',\n","        f'/content/drive/MyDrive/final data/Eshhar.pkl',\n","        f'/content/drive/MyDrive/final data/Afula Nir Haemeq.pkl'],\n","    'fileNames':        ['Tavor Kadoorie', 'Newe Yaar', 'Yavneel', 'En Hashofet', 'Eden Farm', 'Eshhar', 'Afula Nir Haemeq'],\n","    'target_station':   'Tavor Kadoorie',\n","    'target_station_desplay_name':   'Tavor Kadoorie',\n","    'target_station_id': 0,\n","    'device' :           device,\n","    'in_channels' :      15, # how many features we have\n","    'output_path':       output_path,\n","    'checkpoints_path':  checkpoints_path,\n","    'scalers_path':      scalers_path,\n","    'inference_output_path': inference_output_path\n","}\n","\n","WINDOW_PARAMS = {\n","    'input_width' :     72, # window input size\n","    'label_width' :     12, # how many hours to predict to the future\n","    'shift' :           13,\n","    'label_columns' :   ['TD (degC)'],\n","}\n","\n","\"\"\"\n","WINDOW_PARAMS = {\n","    'input_width' :     72, # window input size\n","    'label_width' :     12, # how many hours to predict to the future\n","    'shift' :           1,\n","    'label_columns' :   ['TD (degC)'],\n","}\n","\"\"\"\n","\"\"\"\n","WINDOW_PARAMS = {\n","    'input_width' :     72, # window input size\n","    'label_width' :     12, # how many hours to predict to the future\n","    'shift' :           13,\n","    'label_columns' :   ['TD (degC)'],\n","}\n","\"\"\"\n","\"\"\"\n","WINDOW_PARAMS = {\n","    'input_width' :     72, # window input size\n","    'label_width' :     12, # how many hours to predict to the future\n","    'shift' :           25,\n","    'label_columns' :   ['TD (degC)'],\n","}\n","\"\"\"\n","\"\"\"\n","WINDOW_PARAMS = {\n","    'input_width' :     72, # window input size\n","    'label_width' :     24, # how many hours to predict to the future\n","    'shift' :           37,\n","    'label_columns' :   ['TD (degC)'],\n","}\n","\"\"\"\n","\n","\"\"\"\n","use_residual false to true\n","\"\"\"\n","TRAIN_PARAMS = {\n","    'epochs' :          50,\n","    'batch_size':       32,\n","    'lr':               1e-5,                                   # 1e-3, 1e-4, 1e-5\n","    'checkpoint_dir' :  PARAMS['checkpoints_path'],\n","    'resume':           False,\n","    'device':           PARAMS['device'],\n","    'early_stopping_patience':10,                               # how many epochs to wait before stopping the training\n","    'scheduler_patience':3,                                     # how many epochs to wait before reducing the learning rate\n","    'scheduler_factor':  0.5,                                   # the factor to reduce the learning rate\n","    'min_lr':            1e-7,\n","    'logger_path':       PARAMS['output_path']\n","}\n","\n","ADVANCED_MODEL_PARAMS = {\n","    'num_stations':         len(PARAMS['fileNames']),\n","    'time_steps':           WINDOW_PARAMS['input_width'],\n","    'feature_dim':          PARAMS['in_channels'],\n","    'kernel_size':          3,  # cnn filter size                       4, 5, 6, 7\n","    'd_model':              64, # input for transformer size            64, 128\n","    'nhead':                8,  # number of heads in the transformer    8, 16\n","    'num_layers':           4,  # number of layers in the transformer - 6 - 12\n","    'target_station_idx':   PARAMS['target_station_id'],\n","    'label_width':          WINDOW_PARAMS['label_width'],\n","    'output_per_feature':   3,                                          # 4 ,5\n","    'use_batch_norm':       False,\n","    'use_residual':         True\n","}\n","\n","models_paths_dir_names_full_paths = [os.path.join(inference_base_path, model_folder_name) for model_folder_name in models_paths_dir_names]\n","\n","INFERENCE_PARAMS = {\n","    'params_path':             [os.path.join(folder, 'parameters.py') for folder in models_paths_dir_names_full_paths],\n","    'weights_paths':           [os.path.join(folder, 'checkpoints', 'best_checkpoint.pth') for folder in models_paths_dir_names_full_paths],\n","    'scaler_folder_path':      PARAMS['scalers_path'],\n","    'inference_output_path_per_model':  models_paths_dir_names_full_paths, # for saving the output of the inference in the model folder for each model\n","    'inference_output_path':  os.path.join(inference_base_path, 'inference_output'), # for saving the output of the inference of all models in one folder (later analyze.py will use it)\n","}"]},{"cell_type":"markdown","id":"7e2c37ac","metadata":{"id":"7e2c37ac"},"source":["### `main.py`"]},{"cell_type":"code","execution_count":8,"id":"9a54f28b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9a54f28b","executionInfo":{"status":"ok","timestamp":1738435023359,"user_tz":-120,"elapsed":4031413,"user":{"displayName":"יובל רוזנר","userId":"07921919419559702173"}},"outputId":"9387e683-0e46-4dc0-b71e-7786f6bcf10d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","size of data:\n","Station 0: (149315, 15)\n","Station 1: (149315, 15)\n","Station 2: (149315, 15)\n","Station 3: (149315, 15)\n","Station 4: (149315, 15)\n","Station 5: (149315, 15)\n","Station 6: (149315, 15)\n","Scaler for Station 0 saved to /content/drive/MyDrive/hyperparameters/24_36/4/output/scalers/scaler_station_0.pkl\n","Scaler for Station 1 saved to /content/drive/MyDrive/hyperparameters/24_36/4/output/scalers/scaler_station_1.pkl\n","Scaler for Station 2 saved to /content/drive/MyDrive/hyperparameters/24_36/4/output/scalers/scaler_station_2.pkl\n","Scaler for Station 3 saved to /content/drive/MyDrive/hyperparameters/24_36/4/output/scalers/scaler_station_3.pkl\n","Scaler for Station 4 saved to /content/drive/MyDrive/hyperparameters/24_36/4/output/scalers/scaler_station_4.pkl\n","Scaler for Station 5 saved to /content/drive/MyDrive/hyperparameters/24_36/4/output/scalers/scaler_station_5.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(\n","INFO:TrainingLogger:start logger\n"]},{"output_type":"stream","name":"stdout","text":["Scaler for Station 6 saved to /content/drive/MyDrive/hyperparameters/24_36/4/output/scalers/scaler_station_6.pkl\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","Epoch 1/50: 100%|██████████| 3730/3730 [02:05<00:00, 29.83it/s]\n","INFO:TrainingLogger:Epoch 1/50 | Train Loss: 0.2032 | Val Loss: 0.1152\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000010\n","INFO:TrainingLogger:  -> Best model saved at epoch 1 (val_loss=0.1152)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/50 | Train Loss: 0.2032 | Val Loss: 0.1152\n","  -> Current Learning Rate: 0.000010\n","  -> Best model saved at epoch 1 (val_loss=0.1152)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/50: 100%|██████████| 3730/3730 [02:03<00:00, 30.26it/s]\n","INFO:TrainingLogger:Epoch 2/50 | Train Loss: 0.1185 | Val Loss: 0.0912\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000010\n","INFO:TrainingLogger:  -> Best model saved at epoch 2 (val_loss=0.0912)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/50 | Train Loss: 0.1185 | Val Loss: 0.0912\n","  -> Current Learning Rate: 0.000010\n","  -> Best model saved at epoch 2 (val_loss=0.0912)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/50: 100%|██████████| 3730/3730 [02:03<00:00, 30.30it/s]\n","INFO:TrainingLogger:Epoch 3/50 | Train Loss: 0.1014 | Val Loss: 0.0824\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000010\n","INFO:TrainingLogger:  -> Best model saved at epoch 3 (val_loss=0.0824)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/50 | Train Loss: 0.1014 | Val Loss: 0.0824\n","  -> Current Learning Rate: 0.000010\n","  -> Best model saved at epoch 3 (val_loss=0.0824)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/50: 100%|██████████| 3730/3730 [02:03<00:00, 30.31it/s]\n","INFO:TrainingLogger:Epoch 4/50 | Train Loss: 0.0933 | Val Loss: 0.0774\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000010\n","INFO:TrainingLogger:  -> Best model saved at epoch 4 (val_loss=0.0774)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4/50 | Train Loss: 0.0933 | Val Loss: 0.0774\n","  -> Current Learning Rate: 0.000010\n","  -> Best model saved at epoch 4 (val_loss=0.0774)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/50: 100%|██████████| 3730/3730 [02:03<00:00, 30.30it/s]\n","INFO:TrainingLogger:Epoch 5/50 | Train Loss: 0.0889 | Val Loss: 0.0761\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000010\n","INFO:TrainingLogger:  -> Best model saved at epoch 5 (val_loss=0.0761)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5/50 | Train Loss: 0.0889 | Val Loss: 0.0761\n","  -> Current Learning Rate: 0.000010\n","  -> Best model saved at epoch 5 (val_loss=0.0761)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/50: 100%|██████████| 3730/3730 [02:03<00:00, 30.30it/s]\n","INFO:TrainingLogger:Epoch 6/50 | Train Loss: 0.0858 | Val Loss: 0.0736\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000010\n","INFO:TrainingLogger:  -> Best model saved at epoch 6 (val_loss=0.0736)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6/50 | Train Loss: 0.0858 | Val Loss: 0.0736\n","  -> Current Learning Rate: 0.000010\n","  -> Best model saved at epoch 6 (val_loss=0.0736)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/50: 100%|██████████| 3730/3730 [02:03<00:00, 30.32it/s]\n","INFO:TrainingLogger:Epoch 7/50 | Train Loss: 0.0836 | Val Loss: 0.0717\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000010\n","INFO:TrainingLogger:  -> Best model saved at epoch 7 (val_loss=0.0717)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7/50 | Train Loss: 0.0836 | Val Loss: 0.0717\n","  -> Current Learning Rate: 0.000010\n","  -> Best model saved at epoch 7 (val_loss=0.0717)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/50: 100%|██████████| 3730/3730 [02:03<00:00, 30.29it/s]\n","INFO:TrainingLogger:Epoch 8/50 | Train Loss: 0.0817 | Val Loss: 0.0715\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000010\n","INFO:TrainingLogger:  -> Best model saved at epoch 8 (val_loss=0.0715)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8/50 | Train Loss: 0.0817 | Val Loss: 0.0715\n","  -> Current Learning Rate: 0.000010\n","  -> Best model saved at epoch 8 (val_loss=0.0715)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/50: 100%|██████████| 3730/3730 [02:03<00:00, 30.31it/s]\n","INFO:TrainingLogger:Epoch 9/50 | Train Loss: 0.0803 | Val Loss: 0.0727\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000010\n","INFO:TrainingLogger:  -> No improvement in validation loss for 1 epoch(s)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9/50 | Train Loss: 0.0803 | Val Loss: 0.0727\n","  -> Current Learning Rate: 0.000010\n","  -> No improvement in validation loss for 1 epoch(s)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/50: 100%|██████████| 3730/3730 [02:02<00:00, 30.33it/s]\n","INFO:TrainingLogger:Epoch 10/50 | Train Loss: 0.0791 | Val Loss: 0.0714\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000010\n","INFO:TrainingLogger:  -> Best model saved at epoch 10 (val_loss=0.0714)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10/50 | Train Loss: 0.0791 | Val Loss: 0.0714\n","  -> Current Learning Rate: 0.000010\n","  -> Best model saved at epoch 10 (val_loss=0.0714)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 11/50: 100%|██████████| 3730/3730 [02:03<00:00, 30.31it/s]\n","INFO:TrainingLogger:Epoch 11/50 | Train Loss: 0.0780 | Val Loss: 0.0704\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000010\n","INFO:TrainingLogger:  -> Best model saved at epoch 11 (val_loss=0.0704)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11/50 | Train Loss: 0.0780 | Val Loss: 0.0704\n","  -> Current Learning Rate: 0.000010\n","  -> Best model saved at epoch 11 (val_loss=0.0704)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 12/50: 100%|██████████| 3730/3730 [02:03<00:00, 30.28it/s]\n","INFO:TrainingLogger:Epoch 12/50 | Train Loss: 0.0770 | Val Loss: 0.0720\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000010\n","INFO:TrainingLogger:  -> No improvement in validation loss for 1 epoch(s)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12/50 | Train Loss: 0.0770 | Val Loss: 0.0720\n","  -> Current Learning Rate: 0.000010\n","  -> No improvement in validation loss for 1 epoch(s)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 13/50: 100%|██████████| 3730/3730 [02:03<00:00, 30.31it/s]\n","INFO:TrainingLogger:Epoch 13/50 | Train Loss: 0.0762 | Val Loss: 0.0691\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000010\n","INFO:TrainingLogger:  -> Best model saved at epoch 13 (val_loss=0.0691)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13/50 | Train Loss: 0.0762 | Val Loss: 0.0691\n","  -> Current Learning Rate: 0.000010\n","  -> Best model saved at epoch 13 (val_loss=0.0691)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 14/50: 100%|██████████| 3730/3730 [02:03<00:00, 30.28it/s]\n","INFO:TrainingLogger:Epoch 14/50 | Train Loss: 0.0754 | Val Loss: 0.0685\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000010\n","INFO:TrainingLogger:  -> Best model saved at epoch 14 (val_loss=0.0685)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14/50 | Train Loss: 0.0754 | Val Loss: 0.0685\n","  -> Current Learning Rate: 0.000010\n","  -> Best model saved at epoch 14 (val_loss=0.0685)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 15/50: 100%|██████████| 3730/3730 [02:03<00:00, 30.25it/s]\n","INFO:TrainingLogger:Epoch 15/50 | Train Loss: 0.0747 | Val Loss: 0.0686\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000010\n","INFO:TrainingLogger:  -> No improvement in validation loss for 1 epoch(s)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15/50 | Train Loss: 0.0747 | Val Loss: 0.0686\n","  -> Current Learning Rate: 0.000010\n","  -> No improvement in validation loss for 1 epoch(s)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 16/50: 100%|██████████| 3730/3730 [02:03<00:00, 30.24it/s]\n","INFO:TrainingLogger:Epoch 16/50 | Train Loss: 0.0741 | Val Loss: 0.0686\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000010\n","INFO:TrainingLogger:  -> No improvement in validation loss for 2 epoch(s)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 16/50 | Train Loss: 0.0741 | Val Loss: 0.0686\n","  -> Current Learning Rate: 0.000010\n","  -> No improvement in validation loss for 2 epoch(s)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 17/50: 100%|██████████| 3730/3730 [02:03<00:00, 30.27it/s]\n","INFO:TrainingLogger:Epoch 17/50 | Train Loss: 0.0735 | Val Loss: 0.0693\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000010\n","INFO:TrainingLogger:  -> No improvement in validation loss for 3 epoch(s)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 17/50 | Train Loss: 0.0735 | Val Loss: 0.0693\n","  -> Current Learning Rate: 0.000010\n","  -> No improvement in validation loss for 3 epoch(s)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 18/50: 100%|██████████| 3730/3730 [02:03<00:00, 30.30it/s]\n","INFO:TrainingLogger:Epoch 18/50 | Train Loss: 0.0728 | Val Loss: 0.0689\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000005\n","INFO:TrainingLogger:  -> No improvement in validation loss for 4 epoch(s)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 18/50 | Train Loss: 0.0728 | Val Loss: 0.0689\n","  -> Current Learning Rate: 0.000005\n","  -> No improvement in validation loss for 4 epoch(s)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 19/50: 100%|██████████| 3730/3730 [02:03<00:00, 30.29it/s]\n","INFO:TrainingLogger:Epoch 19/50 | Train Loss: 0.0717 | Val Loss: 0.0718\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000005\n","INFO:TrainingLogger:  -> No improvement in validation loss for 5 epoch(s)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 19/50 | Train Loss: 0.0717 | Val Loss: 0.0718\n","  -> Current Learning Rate: 0.000005\n","  -> No improvement in validation loss for 5 epoch(s)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 20/50: 100%|██████████| 3730/3730 [02:03<00:00, 30.27it/s]\n","INFO:TrainingLogger:Epoch 20/50 | Train Loss: 0.0716 | Val Loss: 0.0676\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000005\n","INFO:TrainingLogger:  -> Best model saved at epoch 20 (val_loss=0.0676)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 20/50 | Train Loss: 0.0716 | Val Loss: 0.0676\n","  -> Current Learning Rate: 0.000005\n","  -> Best model saved at epoch 20 (val_loss=0.0676)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 21/50: 100%|██████████| 3730/3730 [02:03<00:00, 30.28it/s]\n","INFO:TrainingLogger:Epoch 21/50 | Train Loss: 0.0712 | Val Loss: 0.0727\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000005\n","INFO:TrainingLogger:  -> No improvement in validation loss for 1 epoch(s)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 21/50 | Train Loss: 0.0712 | Val Loss: 0.0727\n","  -> Current Learning Rate: 0.000005\n","  -> No improvement in validation loss for 1 epoch(s)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 22/50: 100%|██████████| 3730/3730 [02:03<00:00, 30.25it/s]\n","INFO:TrainingLogger:Epoch 22/50 | Train Loss: 0.0711 | Val Loss: 0.0695\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000005\n","INFO:TrainingLogger:  -> No improvement in validation loss for 2 epoch(s)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 22/50 | Train Loss: 0.0711 | Val Loss: 0.0695\n","  -> Current Learning Rate: 0.000005\n","  -> No improvement in validation loss for 2 epoch(s)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 23/50: 100%|██████████| 3730/3730 [02:03<00:00, 30.28it/s]\n","INFO:TrainingLogger:Epoch 23/50 | Train Loss: 0.0707 | Val Loss: 0.0689\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000005\n","INFO:TrainingLogger:  -> No improvement in validation loss for 3 epoch(s)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 23/50 | Train Loss: 0.0707 | Val Loss: 0.0689\n","  -> Current Learning Rate: 0.000005\n","  -> No improvement in validation loss for 3 epoch(s)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 24/50: 100%|██████████| 3730/3730 [02:03<00:00, 30.31it/s]\n","INFO:TrainingLogger:Epoch 24/50 | Train Loss: 0.0706 | Val Loss: 0.0677\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000003\n","INFO:TrainingLogger:  -> No improvement in validation loss for 4 epoch(s)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 24/50 | Train Loss: 0.0706 | Val Loss: 0.0677\n","  -> Current Learning Rate: 0.000003\n","  -> No improvement in validation loss for 4 epoch(s)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 25/50: 100%|██████████| 3730/3730 [02:03<00:00, 30.28it/s]\n","INFO:TrainingLogger:Epoch 25/50 | Train Loss: 0.0700 | Val Loss: 0.0677\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000003\n","INFO:TrainingLogger:  -> No improvement in validation loss for 5 epoch(s)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 25/50 | Train Loss: 0.0700 | Val Loss: 0.0677\n","  -> Current Learning Rate: 0.000003\n","  -> No improvement in validation loss for 5 epoch(s)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 26/50: 100%|██████████| 3730/3730 [02:03<00:00, 30.29it/s]\n","INFO:TrainingLogger:Epoch 26/50 | Train Loss: 0.0699 | Val Loss: 0.0695\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000003\n","INFO:TrainingLogger:  -> No improvement in validation loss for 6 epoch(s)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 26/50 | Train Loss: 0.0699 | Val Loss: 0.0695\n","  -> Current Learning Rate: 0.000003\n","  -> No improvement in validation loss for 6 epoch(s)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 27/50: 100%|██████████| 3730/3730 [02:03<00:00, 30.30it/s]\n","INFO:TrainingLogger:Epoch 27/50 | Train Loss: 0.0696 | Val Loss: 0.0676\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000003\n","INFO:TrainingLogger:  -> No improvement in validation loss for 7 epoch(s)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 27/50 | Train Loss: 0.0696 | Val Loss: 0.0676\n","  -> Current Learning Rate: 0.000003\n","  -> No improvement in validation loss for 7 epoch(s)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 28/50: 100%|██████████| 3730/3730 [02:03<00:00, 30.30it/s]\n","INFO:TrainingLogger:Epoch 28/50 | Train Loss: 0.0695 | Val Loss: 0.0683\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000001\n","INFO:TrainingLogger:  -> No improvement in validation loss for 8 epoch(s)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 28/50 | Train Loss: 0.0695 | Val Loss: 0.0683\n","  -> Current Learning Rate: 0.000001\n","  -> No improvement in validation loss for 8 epoch(s)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 29/50: 100%|██████████| 3730/3730 [02:03<00:00, 30.26it/s]\n","INFO:TrainingLogger:Epoch 29/50 | Train Loss: 0.0692 | Val Loss: 0.0679\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000001\n","INFO:TrainingLogger:  -> No improvement in validation loss for 9 epoch(s)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 29/50 | Train Loss: 0.0692 | Val Loss: 0.0679\n","  -> Current Learning Rate: 0.000001\n","  -> No improvement in validation loss for 9 epoch(s)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 30/50: 100%|██████████| 3730/3730 [02:02<00:00, 30.33it/s]\n","INFO:TrainingLogger:Epoch 30/50 | Train Loss: 0.0692 | Val Loss: 0.0681\n","INFO:TrainingLogger:  -> Current Learning Rate: 0.000001\n","INFO:TrainingLogger:  -> No improvement in validation loss for 10 epoch(s)\n","INFO:TrainingLogger:Early stopping triggered.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 30/50 | Train Loss: 0.0692 | Val Loss: 0.0681\n","  -> Current Learning Rate: 0.000001\n","  -> No improvement in validation loss for 10 epoch(s)\n","Early stopping triggered.\n"]}],"source":["# main.py\n","import os\n","import numpy as np\n","\n","def main():\n","    if os.path.exists(PARAMS['output_path']) and TRAIN_PARAMS['resume'] is False:\n","        print(f\"Error: Directory {PARAMS['output_path']} already exists. Please remove it or set resume=True.\")\n","        return\n","\n","    print(f\"Using device: {PARAMS['device']}\")\n","    east = []\n","    north = []\n","    filenames = PARAMS['paths_in_colab']\n","    dfs = []\n","    for filename in filenames:\n","        df = pd.read_pickle(filename)\n","        dfs.append(df)\n","        east.append(STATIONS_COORDINATES_COLAB[filename][0])\n","        north.append(STATIONS_COORDINATES_COLAB[filename][1])\n","    east = np.array(east)\n","    north = np.array(north)\n","    east_normalized, north_normalized = normalize_coordinates(east, north)\n","\n","    print(\"size of data:\")\n","    for i, df in enumerate(dfs):\n","        print(f\"Station {i}: {df.shape}\")\n","\n","    list_of_values = [df.values for df in dfs]    # Extract Feature Values\n","\n","    # Train/Validation Split per Station\n","    train_size = int(0.8 * len(list_of_values[0]))\n","    list_of_train_data = []\n","    list_of_val_data = []\n","    for values in list_of_values:\n","        train_data = values[:train_size]\n","        val_data = values[train_size:]\n","        list_of_train_data.append(train_data)\n","        list_of_val_data.append(val_data)\n","\n","    # Combine Data into 3D Arrays\n","    combined_train_data = np.stack(list_of_train_data, axis=1)  # (T_train, num_stations, num_features)\n","    combined_val_data = np.stack(list_of_val_data, axis=1)      # (T_val, num_stations, num_features)\n","\n","    # Normalize Data Independently per Station\n","    train_data_scaled, val_data_scaled, scalers = normalize_data_independent(\n","        train_data=combined_train_data,\n","        val_data=combined_val_data,\n","        scaler_dir=PARAMS['scalers_path']\n","    )\n","    representative_df = dfs[0]\n","    column_indices = {name: i for i, name in enumerate(representative_df.columns)}\n","    label_columns = [column_indices[WINDOW_PARAMS['label_columns'][0]]]\n","\n","    # Instantiate Datasets\n","    train_dataset = WindowGeneratorMultipleStations(\n","        data=train_data_scaled,\n","        input_width=WINDOW_PARAMS['input_width'],\n","        label_width=WINDOW_PARAMS['label_width'],\n","        shift=      WINDOW_PARAMS['shift'],\n","        label_columns=label_columns,\n","        target_station_idx=PARAMS['target_station_id']\n","    )\n","\n","    val_dataset = WindowGeneratorMultipleStations(\n","        data=val_data_scaled,\n","        input_width=WINDOW_PARAMS['input_width'],\n","        label_width=WINDOW_PARAMS['label_width'],\n","        shift=      WINDOW_PARAMS['shift'],\n","        label_columns=label_columns,\n","        target_station_idx=PARAMS['target_station_id']\n","    )\n","\n","    model = TargetedWeatherPredictionModel(**ADVANCED_MODEL_PARAMS.copy())\n","\n","    train_model(\n","        train_dataset=train_dataset,\n","        val_dataset=val_dataset,\n","        model=      model,\n","        coordinates=[east_normalized, north_normalized],  # Ensure these are on the correct device\n","        epochs=     TRAIN_PARAMS['epochs'],\n","        batch_size= TRAIN_PARAMS['batch_size'],\n","        lr=         TRAIN_PARAMS['lr'],\n","        checkpoint_dir=TRAIN_PARAMS['checkpoint_dir'],\n","        resume=     TRAIN_PARAMS['resume'],\n","        device=     TRAIN_PARAMS['device'],\n","        early_stopping_patience= TRAIN_PARAMS['early_stopping_patience'],\n","        scheduler_patience=TRAIN_PARAMS['scheduler_patience'],\n","        scheduler_factor=TRAIN_PARAMS['scheduler_factor'],\n","        min_lr=TRAIN_PARAMS['min_lr'],\n","        logger_path = TRAIN_PARAMS['logger_path']\n","    )\n","\n","main()"]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"accelerator":"GPU","kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}