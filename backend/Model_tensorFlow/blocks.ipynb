{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window generator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "  def __init__(self, input_width, label_width, shift,train_df, val_df, test_df,label_columns=None):\n",
    "    # Store the raw data.\n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "    self.test_df = test_df\n",
    "\n",
    "    # Work out the label column indices.\n",
    "    self.label_columns = label_columns\n",
    "    if label_columns is not None:\n",
    "      self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}\n",
    "    self.column_indices = {name: i for i, name in enumerate(train_df.columns)}\n",
    "\n",
    "    # Work out the window parameters.\n",
    "    self.input_width = input_width\n",
    "    self.label_width = label_width\n",
    "    self.shift = shift\n",
    "\n",
    "    self.total_window_size = input_width + shift\n",
    "\n",
    "    self.input_slice = slice(0, input_width)\n",
    "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "    self.label_start = self.total_window_size - self.label_width\n",
    "    self.labels_slice = slice(self.label_start, None)\n",
    "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "  def __repr__(self): #return the string representation of the object\n",
    "    return '\\n'.join([\n",
    "        f'Total window size: {self.total_window_size}',\n",
    "        f'Input indices: {self.input_indices}',\n",
    "        f'Label indices: {self.label_indices}',\n",
    "        f'Label column name(s): {self.label_columns}'])\n",
    "  \n",
    "  def split_window(self, features):\n",
    "    inputs = features[:, self.input_slice, :]\n",
    "    labels = features[:, self.labels_slice, :]\n",
    "    if self.label_columns is not None:\n",
    "        labels = tf.stack(\n",
    "            [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "            axis=-1)\n",
    "\n",
    "    # Slicing doesn't preserve static shape information, so set the shapes\n",
    "    # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
    "    inputs.set_shape([None, self.input_width, None])\n",
    "    labels.set_shape([None, self.label_width, None])\n",
    "    return inputs, labels\n",
    "  \n",
    "  def plot(self, model=None, plot_col='TD (degC)', max_subplots=3):\n",
    "    inputs, labels = self.example\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plot_col_index = self.column_indices[plot_col]\n",
    "    max_n = min(max_subplots, len(inputs))\n",
    "    for n in range(max_n):\n",
    "      plt.subplot(max_n, 1, n+1)\n",
    "      plt.ylabel(f'{plot_col} [normed]')\n",
    "      plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
    "              label='Inputs', marker='.', zorder=-10)\n",
    "\n",
    "      if self.label_columns:\n",
    "        label_col_index = self.label_columns_indices.get(plot_col, None)\n",
    "      else:\n",
    "        label_col_index = plot_col_index\n",
    "\n",
    "      if label_col_index is None:\n",
    "        continue\n",
    "\n",
    "      plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
    "                  edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "      if model is not None:\n",
    "        predictions = model(inputs)\n",
    "        plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
    "                    marker='X', edgecolors='k', label='Predictions',\n",
    "                    c='#ff7f0e', s=64)\n",
    "\n",
    "      if n == 0:\n",
    "        plt.legend()\n",
    "      \n",
    "      plt.xlabel('Time [h]')\n",
    "\n",
    "\n",
    "  def make_dataset(self, data):\n",
    "    data = np.array(data, dtype=np.float32)\n",
    "    ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "        data=data,\n",
    "        targets=None,\n",
    "        sequence_length=self.total_window_size,\n",
    "        sequence_stride=1,\n",
    "        shuffle=True,\n",
    "        batch_size=32,)\n",
    "\n",
    "    ds = ds.map(self.split_window)\n",
    "\n",
    "    return ds\n",
    "  \n",
    "  @property #decorator to make a method behave like an attribute of the class\n",
    "  def train(self): \n",
    "    return self.make_dataset(self.train_df)\n",
    "\n",
    "  @property\n",
    "  def val(self):\n",
    "    return self.make_dataset(self.val_df)\n",
    "\n",
    "  @property\n",
    "  def test(self):\n",
    "    return self.make_dataset(self.test_df)\n",
    "\n",
    "  @property\n",
    "  def example(self):\n",
    "    \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
    "    result = getattr(self, '_example', None)\n",
    "    if result is None:\n",
    "      # No example batch was found, so get one from the `.train` dataset\n",
    "      result = next(iter(self.train))\n",
    "      # And cache it for next time\n",
    "      self._example = result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding step by step tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - working only with window generator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total window size: 25\n",
      "Input indices: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n",
      "Label indices: [24]\n",
      "Label column name(s): ['TD (degC)']\n",
      "Example input batch shape: (32, 24, 3)\n",
      "first: The batch size, second: The length of the input sequence for each example, third: The number of features (columns) in your data.\n",
      "Example label batch shape: (32, 1, 1)\n",
      "First sample input: tf.Tensor(\n",
      "[[4.0168700e-01 5.0459886e-01 8.2421970e-01]\n",
      " [3.9037362e-01 7.5998867e-01 4.9404573e-02]\n",
      " [2.4878137e-01 1.8084264e-01 2.7639589e-01]\n",
      " [5.3209162e-01 2.3457150e-01 3.4356412e-01]\n",
      " [9.7648728e-01 4.8141509e-01 7.9466629e-01]\n",
      " [7.7820696e-02 2.1349562e-02 6.5592892e-02]\n",
      " [5.3392988e-01 5.8770972e-01 4.5982581e-01]\n",
      " [2.3215547e-01 5.8226188e-04 8.0550975e-01]\n",
      " [3.9349687e-01 5.3515112e-01 3.6645806e-01]\n",
      " [7.4941236e-01 4.0955618e-01 8.3430678e-01]\n",
      " [9.8743916e-01 9.9407321e-01 8.1653047e-01]\n",
      " [8.7993968e-01 3.0999726e-01 2.2300580e-01]\n",
      " [8.6759456e-02 4.9507588e-01 7.8161466e-01]\n",
      " [8.4284568e-01 7.2219330e-01 9.5563489e-01]\n",
      " [2.7618530e-01 4.8997876e-01 7.3666161e-01]\n",
      " [1.0276988e-01 4.4520551e-01 1.6229808e-02]\n",
      " [6.3255265e-02 9.8322302e-01 3.5423237e-01]\n",
      " [1.2624909e-03 7.0994228e-01 3.3329162e-01]\n",
      " [6.2581378e-01 9.8283815e-01 3.8436851e-01]\n",
      " [3.4638542e-01 1.5781020e-01 8.6465120e-01]\n",
      " [6.2136471e-01 2.5219801e-01 5.4188740e-01]\n",
      " [7.2023559e-01 7.4566966e-01 9.9565703e-01]\n",
      " [3.5051608e-01 1.0167793e-02 8.9714028e-02]\n",
      " [2.9886234e-01 8.6985135e-01 8.8332021e-01]], shape=(24, 3), dtype=float32)\n",
      "First sample label: tf.Tensor([[0.21628268]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def main():\n",
    "    # 1) Create some dummy DataFrames as if they are your train/val/test sets.\n",
    "    #    We'll create random data with columns: [\"Rain\", \"TD (degC)\", \"RH\"]\n",
    "    num_samples = 1000\n",
    "    columns = [\"Rain\", \"TD (degC)\", \"RH\"]\n",
    "\n",
    "    # Make random data for each column\n",
    "    data = np.random.rand(num_samples, len(columns)).astype(np.float32)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    full_df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    # Let's split into train/val/test\n",
    "    train_df = full_df.iloc[:700]\n",
    "    val_df   = full_df.iloc[700:850]\n",
    "    test_df  = full_df.iloc[850:]\n",
    "\n",
    "    # 2) Choose window parameters\n",
    "    input_width = 24  # e.g. last 24 timesteps for input\n",
    "    label_width = 1   # e.g. predict 1 step ahead\n",
    "    shift = 1         # that next step is 1 step after the input\n",
    "\n",
    "    # We want to predict the column \"TD (degC)\" only\n",
    "    label_columns = [\"TD (degC)\"]\n",
    "\n",
    "    # 3) Instantiate the WindowGenerator\n",
    "    w = WindowGenerator(input_width=input_width,\n",
    "                        label_width=label_width,\n",
    "                        shift=shift,\n",
    "                        train_df=train_df,\n",
    "                        val_df=val_df,\n",
    "                        test_df=test_df,\n",
    "                        label_columns=label_columns)\n",
    "\n",
    "    # 4) Print the window configuration\n",
    "    print(w)\n",
    "\n",
    "    # 5) Access a batch from the training dataset\n",
    "    example_inputs, example_labels = next(iter(w.train))\n",
    "    print(\"Example input batch shape:\", example_inputs.shape)\n",
    "    print(\"first: The batch size, second: The length of the input sequence for each example, third: The number of features (columns) in your data.\")\n",
    "\n",
    "    print(\"Example label batch shape:\", example_labels.shape)\n",
    "\n",
    "    # 6) (Optional) If you want to see how the first sample in that batch looks:\n",
    "    print(\"First sample input:\", example_inputs[0])\n",
    "    print(\"First sample label:\", example_labels[0])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 2 - adding cnn per station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\dorsh\\Documents\\GitHub\\WeatherNet\\venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:216: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The name \"station_cnn\" is used 5 times in the model. All operation names should be unique.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 85\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnn_individual_station_model.keras\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 68\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m num_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m      \u001b[38;5;66;03m# features per station (e.g., \"Rain\", \"TD (degC)\", \"RH\")\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Build the model.\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_individual_cnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_stations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Generate some dummy data:\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Create 1000 samples; each sample shape = (24, 5, 3)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 57\u001b[0m, in \u001b[0;36mbuild_individual_cnn_model\u001b[1;34m(input_width, num_stations, num_features)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Optional: add some fully connected layers here if desired.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m outputs \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforecast\u001b[39m\u001b[38;5;124m\"\u001b[39m)(combined)\n\u001b[1;32m---> 57\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIndividualCNNPerStation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\dorsh\\Documents\\GitHub\\WeatherNet\\venv\\Lib\\site-packages\\keras\\src\\utils\\tracking.py:26\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m DotNotTrackScope():\n\u001b[1;32m---> 26\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dorsh\\Documents\\GitHub\\WeatherNet\\venv\\Lib\\site-packages\\keras\\src\\models\\functional.py:135\u001b[0m, in \u001b[0;36mFunctional.__init__\u001b[1;34m(self, inputs, outputs, name, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(is_input_keras_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m flat_inputs):\n\u001b[0;32m    133\u001b[0m     inputs, outputs \u001b[38;5;241m=\u001b[39m clone_graph_nodes(inputs, outputs)\n\u001b[1;32m--> 135\u001b[0m \u001b[43mFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m trainable\n",
      "File \u001b[1;32mc:\\Users\\dorsh\\Documents\\GitHub\\WeatherNet\\venv\\Lib\\site-packages\\keras\\src\\ops\\function.py:77\u001b[0m, in \u001b[0;36mFunction.__init__\u001b[1;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m _self_setattr_tracking\n\u001b[1;32m---> 77\u001b[0m (nodes, nodes_by_depth, operations, operations_by_depth) \u001b[38;5;241m=\u001b[39m \u001b[43mmap_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_outputs\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nodes \u001b[38;5;241m=\u001b[39m nodes\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_nodes_by_depth \u001b[38;5;241m=\u001b[39m nodes_by_depth\n",
      "File \u001b[1;32mc:\\Users\\dorsh\\Documents\\GitHub\\WeatherNet\\venv\\Lib\\site-packages\\keras\\src\\ops\\function.py:329\u001b[0m, in \u001b[0;36mmap_graph\u001b[1;34m(inputs, outputs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m all_names:\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m all_names\u001b[38;5;241m.\u001b[39mcount(name) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 329\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    330\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe name \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is used \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mall_names\u001b[38;5;241m.\u001b[39mcount(name)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimes in the model. All operation names should be unique.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    332\u001b[0m         )\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m network_nodes, nodes_by_depth, operations, operations_by_depth\n",
      "\u001b[1;31mValueError\u001b[0m: The name \"station_cnn\" is used 5 times in the model. All operation names should be unique."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# train_individual_cnn.py\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "def build_cnn_for_station(input_width, num_features, station_id=None):\n",
    "    \"\"\"\n",
    "    Build a CNN branch for a single station.\n",
    "    This branch accepts input of shape (input_width, num_features)\n",
    "    and outputs an embedding vector (here, of length 32).\n",
    "\n",
    "    The branch's layers will have unique names if station_id is provided.\n",
    "    \"\"\"\n",
    "    name_suffix = f\"_{station_id}\" if station_id is not None else \"\"\n",
    "    inp = Input(shape=(input_width, num_features), name=f\"station_input{name_suffix}\")\n",
    "    # A simple 1D CNN: a Conv1D with 'causal' padding followed by global pooling.\n",
    "    x = layers.Conv1D(filters=32, kernel_size=3, activation='relu', padding='causal',\n",
    "                      name=f\"conv1d{name_suffix}\")(inp)\n",
    "    x = layers.GlobalAveragePooling1D(name=f\"global_avg_pool{name_suffix}\")(x)\n",
    "    return Model(inputs=inp, outputs=x, name=f\"station_cnn{name_suffix}\")\n",
    "\n",
    "def build_individual_cnn_model(input_width, num_stations, num_features):\n",
    "    \"\"\"\n",
    "    Build a model that:\n",
    "      - Accepts input with shape (input_width, num_stations, num_features)\n",
    "      - For each station:\n",
    "          * Slices the input for that station (resulting shape: (input_width, num_features))\n",
    "          * Applies an individual (not shared) CNN branch (with unique weights)\n",
    "      - Concatenates the station-specific embeddings and predicts a single output.\n",
    "    \"\"\"\n",
    "    # Define the overall input.\n",
    "    inputs = Input(shape=(input_width, num_stations, num_features), name=\"input_window\")\n",
    "    \n",
    "    # List to hold the outputs from each station's CNN branch.\n",
    "    station_outputs = []\n",
    "    \n",
    "    # Loop over the station indices and build an independent branch for each.\n",
    "    for i in range(num_stations):\n",
    "        # Use a Lambda layer to extract station i (slicing along the station axis).\n",
    "        station_i = layers.Lambda(lambda x, idx=i: x[:, :, idx, :],\n",
    "                                  name=f\"slice_station_{i}\")(inputs)\n",
    "        # Now, station_i has shape (batch, input_width, num_features)\n",
    "        \n",
    "        # Build a separate CNN branch for station i with a unique name.\n",
    "        cnn_branch = build_cnn_for_station(input_width, num_features, station_id=i)\n",
    "        # Get the CNN output (embedding) for this station.\n",
    "        branch_output = cnn_branch(station_i)\n",
    "        station_outputs.append(branch_output)\n",
    "    \n",
    "    # Combine (concatenate) all station embeddings into one vector.\n",
    "    combined = layers.concatenate(station_outputs, axis=-1)  # shape: (batch, num_stations * 32)\n",
    "    \n",
    "    # Optional: add some fully connected layers here if desired.\n",
    "    outputs = layers.Dense(1, name=\"forecast\")(combined)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"IndividualCNNPerStation\")\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # Set model parameters:\n",
    "    input_width = 24      # e.g., 24 timesteps per window\n",
    "    num_stations = 5      # e.g., 5 stations\n",
    "    num_features = 3      # features per station (e.g., \"Rain\", \"TD (degC)\", \"RH\")\n",
    "    \n",
    "    # Build the model.\n",
    "    model = build_individual_cnn_model(input_width, num_stations, num_features)\n",
    "    model.summary()\n",
    "    \n",
    "    # Generate some dummy data:\n",
    "    # Create 1000 samples; each sample shape = (24, 5, 3)\n",
    "    num_samples = 1000\n",
    "    X = np.random.rand(num_samples, input_width, num_stations, num_features).astype(np.float32)\n",
    "    y = np.random.rand(num_samples, 1).astype(np.float32)\n",
    "    \n",
    "    # Train the model.\n",
    "    model.fit(X, y, epochs=3, batch_size=32, validation_split=0.1)\n",
    "    \n",
    "    # Save the model (using the native Keras format).\n",
    "    model.save(\"cnn_individual_station_model.keras\")\n",
    "    print(\"Model saved to 'cnn_individual_station_model.keras'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
